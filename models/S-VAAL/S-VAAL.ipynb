{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Variational Adversarial Active Learning (S-VAAL)\n",
    "@author: Tyler Bikaun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook intends to flesh out an initial proof concept of the coupling of S-VAE (Bowman <i>et al.</i> 2016; https://arxiv.org/abs/1511.06349) and VAAL (Sinha <i>et al.</i> 2019; https://arxiv.org/abs/1904.00370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Application:</b> Named Entity Task (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "<i>Model architectures</i><br>\n",
    "<b>SVAE</b> - RNN<br>\n",
    "<b>Discriminator</b> - FC NN<br>\n",
    "<b>Task Learner</b> - RNN<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVAE(nn.Module):\n",
    "    \"\"\" Sentence Variational Autoencoder (Bowman et al. 2016)\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SVAE, self).__init__()\n",
    "        \n",
    "        # TODO: fix dodgy vocab_size issue... this will be cleared up when utils implemented properly\n",
    "        \n",
    "        self.tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "        \n",
    "        self.max_sequence_length = 40    # arg\n",
    "        self.pad_idx = 0\n",
    "        self.eos_idx = vocab_size + 1\n",
    "        self.sos_idx = vocab_size + 2\n",
    "        self.unk_idx = vocab_size + 3\n",
    "        \n",
    "        self.vocab_size = vocab_size + 4\n",
    "        \n",
    "        self.z_dim = 8\n",
    "        \n",
    "        self.rnn_type = 'gru'\n",
    "        self.bidirectional = False\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 128\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_size)\n",
    "        self.word_dropout_rate = 0.1\n",
    "        self.embedding_dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # set rnn type\n",
    "        if self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        \n",
    "        # init encoder-decoder RNNs (models are identical)\n",
    "        self.encoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "        self.decoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "\n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.num_layers\n",
    "        \n",
    "        # Initialisation of FC layers\n",
    "        # These go from encoder to latent (z) space\n",
    "        self.hidden2mean = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.hidden2logv = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.z2hidden = nn.Linear(self.z_dim, self.hidden_size * self.hidden_factor)\n",
    "        self.outputs2vocab = nn.Linear(self.hidden_size * (2 if self.bidirectional else 1), self.vocab_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequence, length):\n",
    "        \"\"\" Forward pass through VAE \"\"\"\n",
    "        \n",
    "        batch_size = input_sequence.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(length, descending=True)   # trick for packed padding\n",
    "        input_sequence = input_sequence[sorted_idx]\n",
    "        \n",
    "        # ENCODER\n",
    "        input_embedding = self.embedding(input_sequence)\n",
    "#         print(input_embedding.shape)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        _, hidden = self.encoder_rnn(packed_input)\n",
    "        \n",
    "        if self.bidirectional or 1 < self.num_layers:\n",
    "            # flatten hidden state\n",
    "            hidden = hidden.view(batch_size, self.hidden_size * self.hidden_factor)\n",
    "        else:\n",
    "            # .squeeze() -> Returns a tensor with all the dimensions of input of size 1 removed.\n",
    "            print(f'hidden shape before squeeze {hidden.shape}')\n",
    "#             hidden = hidden.squeeze()\n",
    "            pass\n",
    "            print(f'hidden shape after squeeze {hidden.shape}')\n",
    "\n",
    "        \n",
    "        # Reparameterisation trick!\n",
    "        z, mean, logv, std = self.reparameterise(hidden, batch_size)\n",
    "        \n",
    "        # DECODER\n",
    "        if 0 < self.word_dropout_rate:\n",
    "            prob = torch.rand(input_sequence.size())\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                prob = prob.cuda()\n",
    "\n",
    "            prob[(input_sequence.data - self.sos_idx) * (input_sequence.data - self.pad_idx) == 0] = 1\n",
    "\n",
    "            decoder_input_sequence = input_sequence.clone()\n",
    "            \n",
    "#             print(vocab_size)\n",
    "#             print(self.unk_idx)\n",
    "            decoder_input_sequence[prob < self.word_dropout_rate] = self.unk_idx\n",
    "\n",
    "#             print(decoder_input_sequence)\n",
    "            input_embedding = self.embedding(decoder_input_sequence)\n",
    "\n",
    "        input_embedding = self.embedding_dropout(input_embedding)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        \n",
    "        outputs, _ = self.decoder_rnn(packed_input, hidden)\n",
    "        \n",
    "        # process outputs\n",
    "        # Process outputs\n",
    "        # Unpack padded sequence\n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(outputs, batch_first=True)[0]\n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        _, reversed_idx = torch.sort(sorted_idx)\n",
    "        padded_outputs = padded_outputs[reversed_idx]\n",
    "        b, s, _ = padded_outputs.size()\n",
    "\n",
    "        # Project outputs to vocab\n",
    "        # e.g. project hidden state into label space...\n",
    "        logp = nn.functional.log_softmax(self.outputs2vocab(padded_outputs.view(-1, padded_outputs.size(2))), dim=-1)\n",
    "#         print(f'logp before view {logp.shape}\\n')\n",
    "#         print(f'b {b} s {s} no emb {self.embedding.num_embeddings}')\n",
    "        logp = logp.view(b, s, self.embedding.num_embeddings)\n",
    "\n",
    "        # logp - log posterior over label space; mean - tensor Gaussian mean, logv - tensor Gaussian variance, z - VAE latent space \n",
    "        return logp, mean, logv, z\n",
    "    \n",
    "    def to_var(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return x\n",
    "    \n",
    "    def reparameterise(self, hidden, batch_size):\n",
    "        \"\"\" Implement reparameterisation trick (Kingma and Welling 2014) \"\"\"\n",
    "        \n",
    "        mean = self.hidden2mean(hidden)\n",
    "        logv = self.hidden2logv(hidden)\n",
    "        std = torch.exp(0.5 * logv)\n",
    "        \n",
    "        z = self.to_var(torch.randn([batch_size, self.z_dim]))\n",
    "        return z * std + mean, mean, logv, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "vocab_size = 100\n",
    "hidden_size = 128\n",
    "SVAE = SVAE(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([10, 40]) - lengths torch.Size([10])\n",
      "hidden shape before squeeze torch.Size([1, 10, 128])\n",
      "hidden shape after squeeze torch.Size([1, 10, 128])\n",
      "(tensor([[[-4.6944, -4.5930, -4.4858,  ..., -4.8528, -4.4396, -5.0879],\n",
      "         [-4.5565, -4.8121, -4.3012,  ..., -4.5629, -4.3834, -4.8766],\n",
      "         [-4.4574, -4.7374, -4.6422,  ..., -4.7135, -4.4288, -4.8301],\n",
      "         ...,\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548]],\n",
      "\n",
      "        [[-4.3756, -4.4705, -4.8215,  ..., -4.9709, -4.7022, -4.6703],\n",
      "         [-4.5016, -4.6237, -4.9081,  ..., -4.6778, -4.5357, -4.5366],\n",
      "         [-4.5677, -4.8624, -4.7604,  ..., -4.3396, -4.5037, -4.5329],\n",
      "         ...,\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548]],\n",
      "\n",
      "        [[-4.8454, -4.6693, -4.7580,  ..., -4.5225, -4.6548, -4.4956],\n",
      "         [-4.7081, -4.7523, -4.6614,  ..., -4.5665, -4.7787, -4.4012],\n",
      "         [-4.7260, -4.7130, -4.5133,  ..., -4.7065, -4.5786, -4.6345],\n",
      "         ...,\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.5099, -4.6857, -4.5845,  ..., -4.4851, -4.8157, -4.5349],\n",
      "         [-4.5465, -5.0163, -4.5117,  ..., -4.3635, -4.8347, -4.7225],\n",
      "         [-4.5525, -4.9056, -4.5830,  ..., -4.7921, -4.5957, -4.8748],\n",
      "         ...,\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548]],\n",
      "\n",
      "        [[-4.3597, -4.5833, -4.8191,  ..., -4.6759, -4.2440, -4.8536],\n",
      "         [-4.5035, -4.6544, -4.9678,  ..., -4.8332, -4.3862, -4.6878],\n",
      "         [-4.6560, -4.7884, -4.7650,  ..., -4.8522, -4.3858, -4.8883],\n",
      "         ...,\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548]],\n",
      "\n",
      "        [[-4.4271, -4.8283, -4.9044,  ..., -4.5965, -4.8028, -4.9339],\n",
      "         [-4.5583, -4.8754, -4.6519,  ..., -4.7715, -4.7005, -4.8851],\n",
      "         [-4.5701, -4.4922, -4.7706,  ..., -4.7660, -4.6369, -4.7948],\n",
      "         ...,\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548],\n",
      "         [-4.6116, -4.6598, -4.6994,  ..., -4.6540, -4.6051, -4.6548]]],\n",
      "       grad_fn=<ViewBackward>), tensor([[[ 2.3237e-01,  3.9430e-01, -2.1220e-01,  3.6220e-02, -1.4093e-01,\n",
      "           2.3015e-01,  3.1911e-01, -5.0382e-03],\n",
      "         [-1.0703e-01,  2.0199e-01, -8.3254e-02,  5.9058e-02, -6.4852e-02,\n",
      "           5.7448e-02,  1.5648e-01,  4.8937e-02],\n",
      "         [ 2.4715e-01,  1.5595e-01,  4.8307e-02,  1.2371e-01,  3.2984e-02,\n",
      "           9.6763e-02,  1.6276e-01, -1.7656e-02],\n",
      "         [ 1.2086e-01, -1.9997e-01, -3.5131e-02,  3.8539e-03, -1.2859e-01,\n",
      "          -1.6642e-01,  9.6230e-02,  1.1957e-01],\n",
      "         [ 4.0361e-01,  2.9753e-01,  1.2307e-01, -5.4554e-02, -6.6794e-02,\n",
      "          -1.0129e-01, -1.7575e-01,  2.2994e-01],\n",
      "         [ 1.8919e-01,  4.1764e-01, -3.9795e-02,  8.6796e-02,  1.2885e-01,\n",
      "           1.3141e-01,  3.0846e-01,  3.4485e-02],\n",
      "         [ 3.0676e-01,  1.8013e-01,  2.4515e-01, -8.4243e-02,  4.3597e-02,\n",
      "           1.9607e-01,  1.1046e-01,  1.3257e-01],\n",
      "         [ 1.3100e-01,  5.1612e-01,  4.5014e-02,  2.8429e-01,  1.0363e-01,\n",
      "           1.0479e-01,  2.8691e-01,  7.7748e-02],\n",
      "         [ 2.5813e-01,  1.8289e-02,  6.4113e-02, -9.4916e-02,  3.0008e-01,\n",
      "          -3.7957e-01, -1.0518e-04,  1.8890e-01],\n",
      "         [ 1.2031e-01,  6.2871e-03,  2.3863e-02,  1.1792e-02, -1.7428e-04,\n",
      "           3.3879e-02,  2.6502e-01,  3.3836e-01]]], grad_fn=<AddBackward0>), tensor([[[-0.0569,  0.1109, -0.0776,  0.0877,  0.1403,  0.0567, -0.2364,\n",
      "           0.2749],\n",
      "         [ 0.0120,  0.2737,  0.1097, -0.1495,  0.0871, -0.1935, -0.1966,\n",
      "          -0.2162],\n",
      "         [-0.0112,  0.0429,  0.1337,  0.1531, -0.2269,  0.1810,  0.0279,\n",
      "           0.3225],\n",
      "         [ 0.0328, -0.0753,  0.0473, -0.1109,  0.0957,  0.3195,  0.0490,\n",
      "           0.2695],\n",
      "         [ 0.0986, -0.1021,  0.1173,  0.0609, -0.1705,  0.1040,  0.0917,\n",
      "          -0.1584],\n",
      "         [ 0.0238, -0.1170, -0.0942,  0.0091,  0.1112,  0.2341, -0.0881,\n",
      "          -0.1273],\n",
      "         [-0.0620,  0.0201,  0.1154, -0.1915,  0.1004, -0.4478, -0.0933,\n",
      "           0.2573],\n",
      "         [ 0.0246,  0.0382,  0.0682, -0.0816,  0.1005,  0.3486, -0.1247,\n",
      "           0.0522],\n",
      "         [ 0.1981, -0.0105,  0.0906, -0.0103, -0.2687,  0.0049,  0.2422,\n",
      "          -0.0429],\n",
      "         [-0.0835,  0.1235,  0.2503, -0.2411, -0.1139, -0.1997, -0.0291,\n",
      "           0.2195]]], grad_fn=<AddBackward0>), tensor([[[ 0.8219,  0.2146,  1.5769,  0.1568,  0.6283, -1.4751, -0.6942,\n",
      "           0.4623],\n",
      "         [ 1.1195,  0.1920,  2.2011,  0.7428,  0.3296,  0.0647,  0.3520,\n",
      "          -0.9448],\n",
      "         [ 0.6007,  0.5228, -0.6814,  0.4485,  1.0794, -0.9062, -0.2092,\n",
      "          -1.3355],\n",
      "         [ 0.5369, -2.2755,  1.4149, -0.4977, -0.3136,  2.2078,  1.2401,\n",
      "          -1.8808],\n",
      "         [ 1.6420,  0.4297,  1.5122,  0.6893, -1.5495,  2.5319, -0.3165,\n",
      "          -0.6399],\n",
      "         [-1.7320, -0.8758, -0.0110, -1.4165,  0.1790, -1.8066,  0.2167,\n",
      "          -0.4485],\n",
      "         [-0.2417, -0.1775,  0.5994,  1.0015, -0.4215,  2.3585,  0.2622,\n",
      "           0.9634],\n",
      "         [ 0.0292,  1.2327, -0.1802,  0.8076,  1.0143, -0.8991,  0.7014,\n",
      "          -0.9958],\n",
      "         [ 1.0017,  1.0018,  1.0123, -0.9597, -0.3479,  1.1002,  1.6497,\n",
      "          -1.8731],\n",
      "         [ 0.5876, -0.6031, -1.2566,  0.4570,  0.4540, -0.3626,  1.6170,\n",
      "           1.5197]]], grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass of SVAE\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# generate some fake sequences with padding\n",
    "seqs = list()\n",
    "for i in range(10):\n",
    "    seq = np.random.randint(low=1, high=100, size=(random.randint(1,40),))\n",
    "    # add padding\n",
    "    seq = np.concatenate((seq, np.zeros(shape=(40 - len(seq)))), axis=None)\n",
    "    seqs.append(seq)\n",
    "sequences = torch.LongTensor(seqs)\n",
    "# get lengths of sequences (not including padding)\n",
    "lengths = torch.tensor([len(seq[seq != 0]) for seq in sequences])\n",
    "\n",
    "print(f'Shapes - seq {sequences.shape} - lengths {lengths.shape}')\n",
    "# print(lengths)\n",
    "\n",
    "# Pass sequences and lengths into SVAE forward method\n",
    "print(SVAE.forward(sequences, lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: GRU(128, 128, batch_first=True)\n",
      "Decoder: GRU(128, 128, batch_first=True)\n",
      "hidden2mean: Linear(in_features=128, out_features=8, bias=True)\n",
      "hidden2logv: Linear(in_features=128, out_features=8, bias=True)\n",
      "z2hidden: Linear(in_features=8, out_features=128, bias=True)\n",
      "outputs2vocab: Linear(in_features=128, out_features=104, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Reviewing components\n",
    "print(f'Encoder: {SVAE.encoder_rnn}')\n",
    "print(f'Decoder: {SVAE.decoder_rnn}')\n",
    "print(f'hidden2mean: {SVAE.hidden2mean}')\n",
    "print(f'hidden2logv: {SVAE.hidden2logv}')\n",
    "print(f'z2hidden: {SVAE.z2hidden}')\n",
    "print(f'outputs2vocab: {SVAE.outputs2vocab}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskLearner(nn.Module):\n",
    "    \"\"\" Task learner for NER \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TaskLearner, self).__init__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaskLearner()"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing functionality\n",
    "TaskLearner()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" Adversary architecture for discriminator module \"\"\"\n",
    "    \n",
    "    def __init__(self, z_dim=8):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim    # latent space dimension\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                nn.Linear(z_dim, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128,1),\n",
    "                                nn.Sigmoid()\n",
    "                                )\n",
    "        \n",
    "        self.weight_init()\n",
    "        \n",
    "    def weight_init(self):\n",
    "        \"\"\" Weight initialisation\n",
    "        \n",
    "        Using Xavier uniform initialisation rather than Kaiming (I think that is more focused for CV? TODO: investigate)\n",
    "        See: https://pytorch.org/cppdocs/api/function_namespacetorch_1_1nn_1_1init_1ace282f75916a862c9678343dfd4d5ffe.html\n",
    "        \"\"\"\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                if type(m) == nn.Linear:\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\" Forward pass through discriminator\n",
    "        \n",
    "        Arguments\n",
    "        --------\n",
    "            z : tensor\n",
    "                Tensor derived from SVAE latent space\n",
    "        \"\"\"\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([ 0.0312,  1.0590,  0.3862, -0.1136, -1.1375,  1.2425, -1.3496, -0.7329])\n",
      "Output: tensor([0.5398], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Testing functionality\n",
    "dsc = Discriminator()\n",
    "\n",
    "# Pass random tensor through forward pass of discriminator\n",
    "rand_tensor = torch.randn((8,))\n",
    "print(f'Input: {rand_tensor}')\n",
    "print(f'Output: {dsc.forward(rand_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine\n",
    "<i> Pseudo code</i>\n",
    "```python\n",
    "for epoch in max_epochs:\n",
    "        train(task learner)\n",
    "            get preds\n",
    "            calc loss\n",
    "            zero grads\n",
    "            backpropigate loss\n",
    "            update model parameters\n",
    "        for step in max_steps:\n",
    "            train(SVAE)\n",
    "        for step in max_steps:\n",
    "            train(discriminator)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
