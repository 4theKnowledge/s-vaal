{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Variational Adversarial Active Learning (S-VAAL)\n",
    "@author: Tyler Bikaun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook intends to flesh out an initial proof concept of the coupling of S-VAE (Bowman <i>et al.</i> 2016; https://arxiv.org/abs/1511.06349) and VAAL (Sinha <i>et al.</i> 2019; https://arxiv.org/abs/1904.00370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Application:</b> Named Entity Task (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Build data preprocessor\n",
    "- [ ] Build data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "<i>Model architectures</i><br>\n",
    "<b>SVAE</b> - RNN<br>\n",
    "<b>Discriminator</b> - FC NN<br>\n",
    "<b>Task Learner</b> - RNN<br>\n",
    "- [ ] SVAE\n",
    "- [ ] Discriminator\n",
    "- [ ] Task Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVAE\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVAE(nn.Module):\n",
    "    \"\"\" Sentence Variational Autoencoder (Bowman et al. 2016)\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SVAE, self).__init__()\n",
    "        \n",
    "        # TODO: fix dodgy vocab_size issue... this will be cleared up when utils implemented properly\n",
    "        \n",
    "        self.tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "        \n",
    "        self.max_sequence_length = 40    # arg\n",
    "        self.pad_idx = 0\n",
    "        self.eos_idx = vocab_size + 1\n",
    "        self.sos_idx = vocab_size + 2\n",
    "        self.unk_idx = vocab_size + 3\n",
    "        \n",
    "        self.vocab_size = vocab_size + 4\n",
    "        \n",
    "        self.z_dim = 8\n",
    "        \n",
    "        self.rnn_type = 'gru'\n",
    "        self.bidirectional = False\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 128\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_size)\n",
    "        self.word_dropout_rate = 0.1\n",
    "        self.embedding_dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # set rnn type\n",
    "        if self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        \n",
    "        # init encoder-decoder RNNs (models are identical)\n",
    "        self.encoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "        self.decoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "\n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.num_layers\n",
    "        \n",
    "        # Initialisation of FC layers\n",
    "        # These go from encoder to latent (z) space\n",
    "        self.hidden2mean = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.hidden2logv = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.z2hidden = nn.Linear(self.z_dim, self.hidden_size * self.hidden_factor)\n",
    "        self.outputs2vocab = nn.Linear(self.hidden_size * (2 if self.bidirectional else 1), self.vocab_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequence, length):\n",
    "        \"\"\" Forward pass through VAE \"\"\"\n",
    "        \n",
    "        batch_size = input_sequence.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(length, descending=True)   # trick for packed padding\n",
    "        input_sequence = input_sequence[sorted_idx]\n",
    "        \n",
    "        # ENCODER\n",
    "        input_embedding = self.embedding(input_sequence)\n",
    "#         print(input_embedding.shape)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        _, hidden = self._encode(packed_input)\n",
    "        \n",
    "        if self.bidirectional or 1 < self.num_layers:\n",
    "            # flatten hidden state\n",
    "            hidden = hidden.view(batch_size, self.hidden_size * self.hidden_factor)\n",
    "        else:\n",
    "            # .squeeze() -> Returns a tensor with all the dimensions of input of size 1 removed.\n",
    "            print(f'hidden shape before squeeze {hidden.shape}')\n",
    "#             hidden = hidden.squeeze()   # doesn't work? gives wrong dimension down stream...\n",
    "            pass\n",
    "            print(f'hidden shape after squeeze {hidden.shape}')\n",
    "\n",
    "        \n",
    "        # Reparameterisation trick!\n",
    "        z, mean, logv, std = self.reparameterise(hidden, batch_size)\n",
    "        \n",
    "        # DECODER\n",
    "        if 0 < self.word_dropout_rate:\n",
    "            prob = torch.rand(input_sequence.size())\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                prob = prob.cuda()\n",
    "\n",
    "            prob[(input_sequence.data - self.sos_idx) * (input_sequence.data - self.pad_idx) == 0] = 1\n",
    "\n",
    "            decoder_input_sequence = input_sequence.clone()\n",
    "            \n",
    "#             print(vocab_size)\n",
    "#             print(self.unk_idx)\n",
    "            decoder_input_sequence[prob < self.word_dropout_rate] = self.unk_idx\n",
    "\n",
    "#             print(decoder_input_sequence)\n",
    "            input_embedding = self.embedding(decoder_input_sequence)\n",
    "\n",
    "        input_embedding = self.embedding_dropout(input_embedding)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        \n",
    "        outputs, _ = self._decode(packed_input, hidden)\n",
    "        \n",
    "        # process outputs\n",
    "        # Process outputs\n",
    "        # Unpack padded sequence\n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(outputs, batch_first=True)[0]\n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        _, reversed_idx = torch.sort(sorted_idx)\n",
    "        padded_outputs = padded_outputs[reversed_idx]\n",
    "        b, s, _ = padded_outputs.size()\n",
    "\n",
    "        # Project outputs to vocab\n",
    "        # e.g. project hidden state into label space...\n",
    "        logp = nn.functional.log_softmax(self.outputs2vocab(padded_outputs.view(-1, padded_outputs.size(2))), dim=-1)\n",
    "#         print(f'logp before view {logp.shape}\\n')\n",
    "#         print(f'b {b} s {s} no emb {self.embedding.num_embeddings}')\n",
    "        logp = logp.view(b, s, self.embedding.num_embeddings)\n",
    "\n",
    "        # logp - log posterior over label space; mean - tensor Gaussian mean, logv - tensor Gaussian variance, z - VAE latent space \n",
    "        return logp, mean, logv, z\n",
    "    \n",
    "    def to_var(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return x\n",
    "    \n",
    "    def reparameterise(self, hidden, batch_size):\n",
    "        \"\"\" Implement reparameterisation trick (Kingma and Welling 2014) \"\"\"\n",
    "        \n",
    "        mean = self.hidden2mean(hidden)\n",
    "        logv = self.hidden2logv(hidden)\n",
    "        std = torch.exp(0.5 * logv) \n",
    "        \n",
    "        z = self.to_var(torch.randn([batch_size, self.z_dim]))\n",
    "        return z * std + mean, mean, logv, std\n",
    "    \n",
    "    def _encode(self, x):\n",
    "        \"\"\" x - pack padded sequence \"\"\"\n",
    "        return self.encoder_rnn(x)\n",
    "    \n",
    "    def _decode(self, x, hidden):\n",
    "        \"\"\" x - pack padded sequence\n",
    "            hidden - latent tensor\"\"\"\n",
    "        return self.decoder_rnn(x, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "vocab_size = 100\n",
    "hidden_size = 128\n",
    "SVAE = SVAE(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([10, 40]) - lengths torch.Size([10])\n",
      "hidden shape before squeeze torch.Size([1, 10, 128])\n",
      "hidden shape after squeeze torch.Size([1, 10, 128])\n",
      "(tensor([[[-4.7107, -4.2747, -5.0126,  ..., -4.7662, -4.6241, -4.4391],\n",
      "         [-4.7077, -4.2380, -4.5522,  ..., -4.8695, -4.3438, -4.6424],\n",
      "         [-4.7907, -4.4818, -4.7881,  ..., -4.7656, -4.8037, -4.6855],\n",
      "         ...,\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610]],\n",
      "\n",
      "        [[-4.6742, -4.5768, -4.4621,  ..., -4.7236, -4.4771, -4.3558],\n",
      "         [-4.6727, -4.6019, -4.4895,  ..., -4.8585, -4.6749, -4.6464],\n",
      "         [-4.8492, -4.4185, -4.6465,  ..., -4.9295, -4.5632, -4.5648],\n",
      "         ...,\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610]],\n",
      "\n",
      "        [[-4.8454, -4.4828, -4.4837,  ..., -4.6546, -4.6630, -4.8880],\n",
      "         [-4.9671, -4.4851, -4.3494,  ..., -4.4893, -4.7132, -4.7908],\n",
      "         [-4.4267, -4.6929, -4.5217,  ..., -4.6206, -4.7577, -4.7425],\n",
      "         ...,\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.5271, -4.1459, -4.7978,  ..., -4.5689, -4.3972, -4.5354],\n",
      "         [-4.2519, -4.3342, -4.5425,  ..., -4.4451, -4.4829, -4.8814],\n",
      "         [-4.4156, -4.6323, -4.4587,  ..., -4.4357, -4.5962, -4.9708],\n",
      "         ...,\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610]],\n",
      "\n",
      "        [[-4.5958, -4.5819, -4.6898,  ..., -4.9723, -4.7660, -4.7942],\n",
      "         [-4.6048, -4.7875, -4.6474,  ..., -4.5937, -4.6228, -4.7207],\n",
      "         [-4.4847, -4.5778, -4.8539,  ..., -5.0329, -4.7494, -4.6249],\n",
      "         ...,\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610]],\n",
      "\n",
      "        [[-4.7106, -4.4463, -4.7458,  ..., -4.5504, -4.7763, -4.6300],\n",
      "         [-4.9811, -4.7781, -4.5348,  ..., -4.2518, -4.6311, -4.6642],\n",
      "         [-4.8076, -4.8967, -4.5492,  ..., -4.3868, -4.8100, -4.5268],\n",
      "         ...,\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610]]],\n",
      "       grad_fn=<ViewBackward>), tensor([[[-0.0617, -0.0473,  0.4632,  0.0229,  0.3618, -0.3012,  0.0325,\n",
      "          -0.2425],\n",
      "         [-0.1254,  0.1309, -0.0425, -0.1092, -0.1578,  0.0189, -0.2261,\n",
      "           0.1312],\n",
      "         [ 0.2424, -0.1085, -0.2788, -0.3789, -0.0899, -0.0446,  0.1885,\n",
      "          -0.0192],\n",
      "         [ 0.0323,  0.2474, -0.1293,  0.1752,  0.1303,  0.0154,  0.0461,\n",
      "           0.1495],\n",
      "         [-0.0031, -0.0575, -0.1751, -0.1911,  0.2001, -0.0508,  0.3093,\n",
      "           0.0731],\n",
      "         [-0.1858, -0.0062, -0.2086, -0.0502,  0.0142,  0.1403,  0.0696,\n",
      "          -0.0520],\n",
      "         [-0.0842,  0.1282, -0.0700,  0.1492,  0.0352,  0.0087,  0.0077,\n",
      "           0.1586],\n",
      "         [-0.2298, -0.0991,  0.2041,  0.0897,  0.0059, -0.0357, -0.1774,\n",
      "          -0.0192],\n",
      "         [-0.1673, -0.0860, -0.0893, -0.0208, -0.3003,  0.0634,  0.1039,\n",
      "           0.0067],\n",
      "         [-0.2401,  0.0749,  0.1955, -0.0175,  0.0962,  0.0874,  0.0722,\n",
      "          -0.1330]]], grad_fn=<AddBackward0>), tensor([[[-0.2217, -0.0612, -0.0462,  0.1029, -0.1426,  0.0876, -0.0802,\n",
      "           0.4023],\n",
      "         [ 0.0929, -0.0168, -0.1591, -0.1009, -0.2436, -0.1824, -0.1311,\n",
      "          -0.2446],\n",
      "         [ 0.3743,  0.0012,  0.0817,  0.0645,  0.0232,  0.1371,  0.0648,\n",
      "          -0.1521],\n",
      "         [-0.2208,  0.0371,  0.3617,  0.0572,  0.0969, -0.1828, -0.0578,\n",
      "           0.0802],\n",
      "         [ 0.1144,  0.0890,  0.0767, -0.0192,  0.1181,  0.0439,  0.1655,\n",
      "           0.3148],\n",
      "         [-0.3238,  0.3118,  0.0561, -0.0059,  0.0737, -0.1966, -0.3250,\n",
      "          -0.1396],\n",
      "         [ 0.0695,  0.0706,  0.2301, -0.1955, -0.5529,  0.2191,  0.2718,\n",
      "           0.3094],\n",
      "         [ 0.1623, -0.1319, -0.0369,  0.0611, -0.0224, -0.0546, -0.1419,\n",
      "          -0.0493],\n",
      "         [-0.1735, -0.1350,  0.2949, -0.2301,  0.0574, -0.3119,  0.0497,\n",
      "           0.2441],\n",
      "         [-0.0333, -0.2310,  0.1502,  0.0415,  0.0902, -0.0877, -0.3072,\n",
      "           0.2477]]], grad_fn=<AddBackward0>), tensor([[[ 1.5229,  0.5031, -0.1481,  1.2152,  0.9819,  0.2425,  0.1000,\n",
      "          -1.2920],\n",
      "         [ 0.6392,  1.8822, -0.5329,  0.9143,  1.0128, -0.0503, -0.8752,\n",
      "          -0.1305],\n",
      "         [ 0.8976, -1.0352,  1.6863,  0.0155, -0.7478,  0.7714,  1.9406,\n",
      "          -2.0906],\n",
      "         [-1.4066,  1.0898,  1.5704,  0.0453,  0.5378,  0.1345, -0.0853,\n",
      "           1.2217],\n",
      "         [ 0.0605,  0.1535,  0.4701, -2.2092, -0.4565, -0.8892,  2.3550,\n",
      "           0.0283],\n",
      "         [ 1.6530, -0.1615,  1.0311, -1.8535, -0.2999,  0.3577,  0.2625,\n",
      "          -0.2468],\n",
      "         [ 0.5799,  3.0354,  0.3002, -0.0747, -0.0057, -2.4402, -0.4318,\n",
      "           1.1839],\n",
      "         [-0.3440,  0.2759, -0.1587, -0.1836, -0.5821,  1.3744,  0.1732,\n",
      "           0.2517],\n",
      "         [ 0.6687, -0.6110,  1.1402, -0.6162, -0.2710, -1.3491,  0.4065,\n",
      "           1.2198],\n",
      "         [-1.0328, -0.2804,  0.1124, -1.2020, -0.9008, -1.0454, -0.9868,\n",
      "           0.8587]]], grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass of SVAE\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# generate some fake sequences with padding\n",
    "seqs = list()\n",
    "for i in range(10):\n",
    "    seq = np.random.randint(low=1, high=100, size=(random.randint(1,40),))\n",
    "    # add padding\n",
    "    seq = np.concatenate((seq, np.zeros(shape=(40 - len(seq)))), axis=None)\n",
    "    seqs.append(seq)\n",
    "sequences = torch.LongTensor(seqs)\n",
    "# get lengths of sequences (not including padding)\n",
    "lengths = torch.tensor([len(seq[seq != 0]) for seq in sequences])\n",
    "\n",
    "print(f'Shapes - seq {sequences.shape} - lengths {lengths.shape}')\n",
    "# print(lengths)\n",
    "\n",
    "# Pass sequences and lengths into SVAE forward method\n",
    "print(SVAE.forward(sequences, lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: GRU(128, 128, batch_first=True)\n",
      "Decoder: GRU(128, 128, batch_first=True)\n",
      "hidden2mean: Linear(in_features=128, out_features=8, bias=True)\n",
      "hidden2logv: Linear(in_features=128, out_features=8, bias=True)\n",
      "z2hidden: Linear(in_features=8, out_features=128, bias=True)\n",
      "outputs2vocab: Linear(in_features=128, out_features=104, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Reviewing components\n",
    "print(f'Encoder: {SVAE.encoder_rnn}')\n",
    "print(f'Decoder: {SVAE.decoder_rnn}')\n",
    "print(f'hidden2mean: {SVAE.hidden2mean}')\n",
    "print(f'hidden2logv: {SVAE.hidden2logv}')\n",
    "print(f'z2hidden: {SVAE.z2hidden}')\n",
    "print(f'outputs2vocab: {SVAE.outputs2vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Learner\n",
    "Using PyTorch tutorial implementation (https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) for rapid development (will implement SoTA in the future)<br><br>\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskLearner(nn.Module):\n",
    "    \"\"\" Task learner for NER \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(TaskLearner, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "tasklearner = TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=104, tagset_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.3646, -1.4012, -1.3667, -1.4135],\n",
       "        [-1.3912, -1.3452, -1.3913, -1.4189],\n",
       "        [-1.3578, -1.4526, -1.3384, -1.4003],\n",
       "        [-1.4360, -1.3843, -1.3061, -1.4240],\n",
       "        [-1.4032, -1.3590, -1.3353, -1.4517],\n",
       "        [-1.4705, -1.3100, -1.3063, -1.4715],\n",
       "        [-1.4036, -1.3782, -1.2292, -1.5619],\n",
       "        [-1.2345, -1.5217, -1.3853, -1.4253],\n",
       "        [-1.3498, -1.4740, -1.3688, -1.3574],\n",
       "        [-1.3601, -1.4565, -1.4957, -1.2510]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing forward functionality\n",
    "tasklearner.forward(torch.randint(0,100,size=(10,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" Adversary architecture for discriminator module \"\"\"\n",
    "    \n",
    "    def __init__(self, z_dim=8):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim    # latent space dimension\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                nn.Linear(z_dim, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128,1),\n",
    "                                nn.Sigmoid()\n",
    "                                )\n",
    "        # Exe\n",
    "        self.weight_init()\n",
    "        \n",
    "    def weight_init(self):\n",
    "        \"\"\" Weight initialisation\n",
    "        \n",
    "        Using Xavier uniform initialisation rather than Kaiming (I think that is more focused for CV? TODO: investigate)\n",
    "        See: https://pytorch.org/cppdocs/api/function_namespacetorch_1_1nn_1_1init_1ace282f75916a862c9678343dfd4d5ffe.html\n",
    "        \"\"\"\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                if type(m) == nn.Linear:\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\" Forward pass through discriminator\n",
    "        \n",
    "        Arguments\n",
    "        --------\n",
    "            z : tensor\n",
    "                Tensor derived from SVAE latent space\n",
    "        \"\"\"\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([ 0.0312,  1.0590,  0.3862, -0.1136, -1.1375,  1.2425, -1.3496, -0.7329])\n",
      "Output: tensor([0.5398], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Testing functionality\n",
    "dsc = Discriminator()\n",
    "\n",
    "# Pass random tensor through forward pass of discriminator\n",
    "rand_tensor = torch.randn((8,))\n",
    "print(f'Input: {rand_tensor}')\n",
    "print(f'Output: {dsc.forward(rand_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler Routine\n",
    "Active learning based sample selection for task learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code copied from VAAL - TODO: investigate applicability; it may not work for sequence based samples.\n",
    "class Sampler:\n",
    "    \"\"\" Adversary sampler \"\"\"\n",
    "    def __init__(self, budget):\n",
    "        self.budget = budget\n",
    "        \n",
    "    def sample(self, vae, discriminator, data, cuda):\n",
    "        \"\"\" Selective sampling algorithm\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            vae : torch model\n",
    "                VAE model\n",
    "            discriminator : torch model\n",
    "                discriminator model\n",
    "            data : tensor\n",
    "                Image data\n",
    "            cuda : boolean\n",
    "                GPU flag\n",
    "        Returns\n",
    "        -------\n",
    "            querry_pool_indices: int, list\n",
    "                List of indices corresponding to sorted (top-K) samples to be sampled from\n",
    "        \"\"\"\n",
    "        all_preds = []\n",
    "        all_indices = []\n",
    "\n",
    "        for images, _, indices in data:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, _, mu, _ = vae(images)\n",
    "                preds = discriminator(mu)\n",
    "\n",
    "            preds = preds.cpu().data\n",
    "            all_preds.extend(preds)\n",
    "            all_indices.extend(indices)\n",
    "\n",
    "        all_preds = torch.stack(all_preds)\n",
    "        all_preds = all_preds.view(-1)\n",
    "        # need to multiply by -1 to be able to use torch.topk \n",
    "        all_preds *= -1\n",
    "\n",
    "        # select the points which the discriminator things are the most likely to be unlabeled\n",
    "        _, querry_indices = torch.topk(all_preds, int(self.budget))\n",
    "        querry_pool_indices = np.asarray(all_indices)[querry_indices]\n",
    "\n",
    "        return querry_pool_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine\n",
    "<i> Pseudo code</i>\n",
    "```python\n",
    "for epoch in max_epochs:\n",
    "        train(task learner)\n",
    "            get preds\n",
    "            calc loss\n",
    "            zero grads\n",
    "            backpropigate loss\n",
    "            update model parameters\n",
    "        for step in max_steps:\n",
    "            train(SVAE)\n",
    "        for step in max_steps:\n",
    "            train(discriminator)\n",
    "```<br><br>\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.vae_steps = 10\n",
    "        self.discriminator_steps = 10\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\" Performs model training \"\"\"\n",
    "    \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'\\nEpoch: {epoch}')\n",
    "            \n",
    "            print('Train task learner')\n",
    "            \n",
    "            print('Train VAE')\n",
    "            for step in range(self.vae_steps):\n",
    "                print(f'VAE Step: {step}')\n",
    "            \n",
    "            print('Train Discriminator')\n",
    "            for step in range(self.discriminator_steps):\n",
    "                print(f'Discriminator Step: {step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 1\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 2\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 3\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 4\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 5\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 6\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 7\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 8\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 9\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n"
     ]
    }
   ],
   "source": [
    "slvr = Solver()\n",
    "slvr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Routine\n",
    "To do:\n",
    " - [ ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Routine\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
