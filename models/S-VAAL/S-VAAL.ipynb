{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Variational Adversarial Active Learning (S-VAAL)\n",
    "@author: Tyler Bikaun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook intends to flesh out an initial proof concept of the coupling of S-VAE (Bowman <i>et al.</i> 2016; https://arxiv.org/abs/1511.06349) and VAAL (Sinha <i>et al.</i> 2019; https://arxiv.org/abs/1904.00370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Application:</b> Named Entity Task (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Architecture Diagram:</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13ef33edc78>"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Functions\n",
    "These functions are used for spot testing code whilst developing. For example, building random sequences of tensors.\n",
    "- [x] Build artificial sequence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_sequences(self, batch_size: int, max_seq_len: int) -> int:\n",
    "        \"\"\"\n",
    "        Builds tensor of specified size containing variable length, padded, sequences of integers\n",
    "            \n",
    "        Arguments\n",
    "        ---------\n",
    "            batch_size : int\n",
    "                Number of sequences to generate\n",
    "            max_seq_len : int\n",
    "                Maximum length of sequences\n",
    "        Returns\n",
    "        -------\n",
    "            sequences : tensor\n",
    "                Tensor of generated sequences\n",
    "            lengths : tensor\n",
    "                Tensor of sequence lengths\n",
    "        \"\"\"\n",
    "        seqs = list()\n",
    "        for i in range(batch_size):\n",
    "            # Generate random integer sequences\n",
    "            seq = np.random.randint(low=1, high=100, size=(random.randint(1, max_seq_len),))\n",
    "            # Add padding\n",
    "            seq = np.concatenate((seq, np.zeros(shape=(max_seq_len - len(seq)))), axis=None)\n",
    "            seqs.append(seq)\n",
    "        sequences = torch.LongTensor(seqs)\n",
    "        lengths = torch.tensor([len(seq[seq != 0]) for seq in sequences])\n",
    "        \n",
    "        print(f'Shapes - seq {sequences.shape} - lengths {lengths.shape}')\n",
    "        \n",
    "        return sequences, lengths\n",
    "    \n",
    "    def build_sequence_tags(self, sequences, label_space_size: int):\n",
    "        \"\"\"\n",
    "        Given a set of sequences, generates ground truth labels\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            sequences : tensor\n",
    "                Tensor of generated sequences\n",
    "            label_space_size : int\n",
    "                Size of label space\n",
    "        Returns\n",
    "        -------\n",
    "            X,y : list of tuples\n",
    "                Artificial ground truth dataset\n",
    "                    X dim : (seq batch size)\n",
    "                    y dim : (seq batch size, label space size)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = len(sequences)\n",
    "        \n",
    "        dataset = list()\n",
    "        for sequence in sequences:\n",
    "            # each 'token' in the sequence has a label mapping\n",
    "            label_set = list()\n",
    "            for token in sequence:\n",
    "                one_hot_array = np.zeros(shape=(label_space_size))\n",
    "                one_hot_array[random.randint(0,label_space_size-1)]=1\n",
    "                labels = torch.IntTensor(one_hot_array)\n",
    "                label_set.append(labels)\n",
    "            dataset.append((sequence, torch.stack(label_set)))   # stack list of labels into tensors\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([10, 10]) - lengths torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Test functionality\n",
    "tester = Tester()\n",
    "sequences, lengths = tester.build_sequences(batch_size=10, max_seq_len=10)\n",
    "dataset = tester.build_sequence_tags(sequences=sequences, label_space_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([10]) - tensor([39,  9, 25, 56, 80,  0,  0,  0,  0,  0])\n",
      "\n",
      "y torch.Size([10, 4]) - tensor([[1, 0, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 1, 0, 0],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [1, 0, 0, 0],\n",
      "        [0, 0, 0, 1],\n",
      "        [0, 1, 0, 0],\n",
      "        [0, 0, 0, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataset:\n",
    "    print(f'X {X.shape} - {X}\\n')\n",
    "    print(f'y {y.shape} - {y}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Build data preprocessor\n",
    "- [ ] Build data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for model building, training, evaluation process. This will be converted into yaml.\n",
    "config = {'': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "<i>Model architectures</i><br>\n",
    "<b>SVAE</b> - RNN<br>\n",
    "<b>Discriminator</b> - FC NN<br>\n",
    "<b>Task Learner</b> - RNN<br>\n",
    "- [ ] SVAE\n",
    "- [ ] Discriminator\n",
    "- [ ] Task Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVAE\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVAE(nn.Module):\n",
    "    \"\"\" Sentence Variational Autoencoder (Bowman et al. 2016)\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SVAE, self).__init__()\n",
    "        \n",
    "        # TODO: fix dodgy vocab_size issue... this will be cleared up when utils implemented properly\n",
    "        \n",
    "        self.tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "        \n",
    "        self.max_sequence_length = 40    # arg\n",
    "        self.pad_idx = 0\n",
    "        self.eos_idx = vocab_size + 1\n",
    "        self.sos_idx = vocab_size + 2\n",
    "        self.unk_idx = vocab_size + 3\n",
    "        \n",
    "        self.vocab_size = vocab_size + 4\n",
    "        \n",
    "        self.z_dim = 8\n",
    "        \n",
    "        self.rnn_type = 'gru'\n",
    "        self.bidirectional = False\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 128\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_size)\n",
    "        self.word_dropout_rate = 0.1\n",
    "        self.embedding_dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # set rnn type\n",
    "        if self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        \n",
    "        # init encoder-decoder RNNs (models are identical)\n",
    "        self.encoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "        self.decoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "\n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.num_layers\n",
    "        \n",
    "        # Initialisation of FC layers\n",
    "        # These go from encoder to latent (z) space\n",
    "        self.hidden2mean = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.hidden2logv = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.z2hidden = nn.Linear(self.z_dim, self.hidden_size * self.hidden_factor)\n",
    "        self.outputs2vocab = nn.Linear(self.hidden_size * (2 if self.bidirectional else 1), self.vocab_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequence, length):\n",
    "        \"\"\" Forward pass through VAE \"\"\"\n",
    "        \n",
    "        batch_size = input_sequence.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(length, descending=True)   # trick for packed padding\n",
    "        input_sequence = input_sequence[sorted_idx]\n",
    "        \n",
    "        # ENCODER\n",
    "        input_embedding = self.embedding(input_sequence)\n",
    "#         print(input_embedding.shape)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        _, hidden = self._encode(packed_input)\n",
    "        \n",
    "        if self.bidirectional or 1 < self.num_layers:\n",
    "            # flatten hidden state\n",
    "            hidden = hidden.view(batch_size, self.hidden_size * self.hidden_factor)\n",
    "        else:\n",
    "            # .squeeze() -> Returns a tensor with all the dimensions of input of size 1 removed.\n",
    "            print(f'hidden shape before squeeze {hidden.shape}')\n",
    "#             hidden = hidden.squeeze()   # doesn't work? gives wrong dimension down stream...\n",
    "            pass\n",
    "            print(f'hidden shape after squeeze {hidden.shape}')\n",
    "\n",
    "        \n",
    "        # Reparameterisation trick!\n",
    "        z, mean, logv, std = self.reparameterise(hidden, batch_size)\n",
    "        \n",
    "        # DECODER\n",
    "        if 0 < self.word_dropout_rate:\n",
    "            prob = torch.rand(input_sequence.size())\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                prob = prob.cuda()\n",
    "\n",
    "            prob[(input_sequence.data - self.sos_idx) * (input_sequence.data - self.pad_idx) == 0] = 1\n",
    "\n",
    "            decoder_input_sequence = input_sequence.clone()\n",
    "            \n",
    "#             print(vocab_size)\n",
    "#             print(self.unk_idx)\n",
    "            decoder_input_sequence[prob < self.word_dropout_rate] = self.unk_idx\n",
    "\n",
    "#             print(decoder_input_sequence)\n",
    "            input_embedding = self.embedding(decoder_input_sequence)\n",
    "\n",
    "        input_embedding = self.embedding_dropout(input_embedding)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        \n",
    "        outputs, _ = self._decode(packed_input, hidden)\n",
    "        \n",
    "        # process outputs\n",
    "        # Process outputs\n",
    "        # Unpack padded sequence\n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(outputs, batch_first=True)[0]\n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        _, reversed_idx = torch.sort(sorted_idx)\n",
    "        padded_outputs = padded_outputs[reversed_idx]\n",
    "        b, s, _ = padded_outputs.size()\n",
    "\n",
    "        # Project outputs to vocab\n",
    "        # e.g. project hidden state into label space...\n",
    "        logp = nn.functional.log_softmax(self.outputs2vocab(padded_outputs.view(-1, padded_outputs.size(2))), dim=-1)\n",
    "#         print(f'logp before view {logp.shape}\\n')\n",
    "#         print(f'b {b} s {s} no emb {self.embedding.num_embeddings}')\n",
    "        logp = logp.view(b, s, self.embedding.num_embeddings)\n",
    "\n",
    "        # logp - log posterior over label space; mean - tensor Gaussian mean, logv - tensor Gaussian variance, z - VAE latent space \n",
    "        return logp, mean, logv, z\n",
    "    \n",
    "    def to_var(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return x\n",
    "    \n",
    "    def reparameterise(self, hidden, batch_size):\n",
    "        \"\"\" Implement reparameterisation trick (Kingma and Welling 2014) \"\"\"\n",
    "        \n",
    "        mean = self.hidden2mean(hidden)\n",
    "        logv = self.hidden2logv(hidden)\n",
    "        std = torch.exp(0.5 * logv) \n",
    "        \n",
    "        z = self.to_var(torch.randn([batch_size, self.z_dim]))\n",
    "        return z * std + mean, mean, logv, std\n",
    "    \n",
    "    def _encode(self, x):\n",
    "        \"\"\" x - pack padded sequence \"\"\"\n",
    "        return self.encoder_rnn(x)\n",
    "    \n",
    "    def _decode(self, x, hidden):\n",
    "        \"\"\" x - pack padded sequence\n",
    "            hidden - latent tensor\"\"\"\n",
    "        return self.decoder_rnn(x, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "vocab_size = 100\n",
    "hidden_size = 128\n",
    "SVAE = SVAE(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([10, 40]) - lengths torch.Size([10])\n",
      "hidden shape before squeeze torch.Size([1, 10, 128])\n",
      "hidden shape after squeeze torch.Size([1, 10, 128])\n",
      "(tensor([[[-4.4606, -4.4063, -4.6647,  ..., -4.3373, -4.7585, -4.6186],\n",
      "         [-4.5217, -4.4854, -4.7991,  ..., -4.3907, -4.6902, -4.5589],\n",
      "         [-4.4746, -4.7599, -4.8052,  ..., -4.3605, -4.7632, -4.4939],\n",
      "         ...,\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610]],\n",
      "\n",
      "        [[-4.2812, -4.5231, -4.7630,  ..., -4.6618, -4.7277, -4.4953],\n",
      "         [-4.5380, -4.6145, -4.7663,  ..., -4.6118, -4.7529, -4.4695],\n",
      "         [-4.5125, -4.6953, -4.4509,  ..., -4.6962, -4.8859, -4.5477],\n",
      "         ...,\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610]],\n",
      "\n",
      "        [[-4.7321, -4.6323, -4.8519,  ..., -4.2463, -4.8279, -4.7326],\n",
      "         [-4.5880, -4.8469, -5.0441,  ..., -4.2087, -4.8922, -4.5803],\n",
      "         [-4.6279, -4.7173, -4.9215,  ..., -4.4694, -4.6832, -4.6060],\n",
      "         ...,\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.3343, -4.2630, -4.7194,  ..., -4.6294, -4.8345, -4.6393],\n",
      "         [-4.3185, -4.6570, -4.6074,  ..., -4.8053, -4.4670, -4.5601],\n",
      "         [-4.4565, -4.7218, -4.7690,  ..., -4.1620, -4.6855, -4.4443],\n",
      "         ...,\n",
      "         [-4.6887, -4.4457, -4.7870,  ..., -4.8366, -4.3741, -4.7145],\n",
      "         [-4.7200, -4.3996, -4.5327,  ..., -4.5387, -4.5913, -4.8730],\n",
      "         [-4.7420, -4.5491, -4.4192,  ..., -4.5172, -4.3786, -4.5123]],\n",
      "\n",
      "        [[-4.4016, -4.6981, -4.6309,  ..., -4.7290, -4.8809, -4.5592],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         ...,\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610]],\n",
      "\n",
      "        [[-4.5844, -4.3905, -4.5600,  ..., -4.4176, -4.8148, -4.6315],\n",
      "         [-4.4106, -4.6694, -4.8136,  ..., -4.3993, -4.9695, -4.4038],\n",
      "         [-4.5206, -5.0369, -4.5607,  ..., -4.2192, -4.9912, -4.2290],\n",
      "         ...,\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610],\n",
      "         [-4.5724, -4.5642, -4.5951,  ..., -4.6142, -4.6185, -4.6610]]],\n",
      "       grad_fn=<ViewBackward>), tensor([[[-0.0046,  0.2001,  0.0637,  0.2751, -0.0423,  0.1182, -0.2391,\n",
      "          -0.1254],\n",
      "         [ 0.1691, -0.0276, -0.0794, -0.1429,  0.1680, -0.0422, -0.1275,\n",
      "          -0.0368],\n",
      "         [-0.2201,  0.3566, -0.0692,  0.2764,  0.1470,  0.0190, -0.2092,\n",
      "           0.0690],\n",
      "         [ 0.1277,  0.0764,  0.1539, -0.3848, -0.1965, -0.2073,  0.2672,\n",
      "           0.0295],\n",
      "         [-0.2203, -0.0246, -0.0483,  0.0318, -0.0597,  0.1421,  0.1583,\n",
      "          -0.1077],\n",
      "         [-0.1929,  0.2521, -0.2493, -0.0111,  0.0973,  0.0784,  0.3606,\n",
      "          -0.2037],\n",
      "         [-0.3740,  0.2909, -0.1012,  0.0885, -0.2337,  0.0686, -0.2430,\n",
      "           0.1403],\n",
      "         [ 0.0430, -0.0800, -0.3722,  0.0971, -0.0161, -0.0056,  0.5858,\n",
      "           0.0764],\n",
      "         [-0.3544,  0.3155, -0.0577, -0.0909,  0.1876, -0.0373, -0.0963,\n",
      "          -0.0416],\n",
      "         [-0.0344, -0.1070, -0.2685,  0.0251,  0.1477,  0.1458,  0.1963,\n",
      "           0.0072]]], grad_fn=<AddBackward0>), tensor([[[-0.3197,  0.1414,  0.2818,  0.2055, -0.1233, -0.0938, -0.2721,\n",
      "          -0.3298],\n",
      "         [-0.0286,  0.3275, -0.2648,  0.3828, -0.3261,  0.0711, -0.0237,\n",
      "          -0.1546],\n",
      "         [ 0.0955,  0.1281,  0.3039,  0.0208, -0.2636, -0.0352, -0.1313,\n",
      "           0.0059],\n",
      "         [ 0.0061,  0.1262, -0.0211,  0.0592,  0.1030, -0.0149,  0.1959,\n",
      "           0.0956],\n",
      "         [-0.1031,  0.1770,  0.4469,  0.1075, -0.2559,  0.1876, -0.3031,\n",
      "           0.2094],\n",
      "         [-0.1360,  0.0258,  0.2281,  0.1600,  0.0956, -0.1035,  0.0299,\n",
      "           0.1917],\n",
      "         [ 0.0616,  0.1035,  0.2710, -0.0392, -0.2174, -0.2540, -0.3617,\n",
      "          -0.1186],\n",
      "         [-0.0855,  0.2326,  0.4002, -0.0993,  0.0393, -0.0148, -0.0229,\n",
      "           0.1155],\n",
      "         [-0.2674,  0.5640,  0.0733,  0.0525, -0.2159, -0.0038, -0.3024,\n",
      "           0.0153],\n",
      "         [ 0.1653, -0.0070,  0.0929,  0.1217,  0.2008,  0.2326,  0.0996,\n",
      "          -0.0350]]], grad_fn=<AddBackward0>), tensor([[[-0.3661, -1.8391,  1.6023,  0.9435,  0.0155,  0.3169, -0.2652,\n",
      "          -0.3394],\n",
      "         [ 0.8804, -0.2695,  0.1616, -0.4534, -0.8520, -1.0171, -0.8628,\n",
      "          -1.6537],\n",
      "         [-0.2479, -0.9262, -1.0062, -0.6602, -0.0491, -1.6190, -0.9171,\n",
      "           0.5624],\n",
      "         [ 1.8724,  2.1072, -1.0142, -2.9783,  1.2261, -0.6997,  0.5437,\n",
      "          -0.3639],\n",
      "         [-0.4250,  1.6001, -0.7133, -0.6367,  0.5539,  0.2525,  2.1020,\n",
      "          -0.4618],\n",
      "         [ 0.8544,  0.0258, -1.3330, -1.1672,  1.1631, -1.5072, -1.5431,\n",
      "           0.4583],\n",
      "         [-1.8344, -0.5804, -0.7205, -1.0782, -0.6139,  1.0218, -1.0924,\n",
      "          -0.3484],\n",
      "         [-1.1681, -0.1023, -4.0296,  1.8166, -0.5070,  0.9259, -0.2191,\n",
      "           0.6887],\n",
      "         [-0.9166,  3.3513, -0.4025,  0.2107,  0.6270, -2.4788,  0.2402,\n",
      "          -1.6122],\n",
      "         [ 1.0516, -0.1037,  0.2424, -0.7096,  0.5170, -0.3195, -1.1020,\n",
      "          -0.5426]]], grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass of SVAE\n",
    "svae_ff_seqs, svae_ff_lengths = Tester().build_sequences(batch_size=10, max_seq_len=40)\n",
    "\n",
    "# Pass sequences and lengths into SVAE forward method\n",
    "print(SVAE.forward(svae_ff_seqs, svae_ff_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: GRU(128, 128, batch_first=True)\n",
      "Decoder: GRU(128, 128, batch_first=True)\n",
      "hidden2mean: Linear(in_features=128, out_features=8, bias=True)\n",
      "hidden2logv: Linear(in_features=128, out_features=8, bias=True)\n",
      "z2hidden: Linear(in_features=8, out_features=128, bias=True)\n",
      "outputs2vocab: Linear(in_features=128, out_features=104, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Reviewing components\n",
    "print(f'Encoder: {SVAE.encoder_rnn}')\n",
    "print(f'Decoder: {SVAE.decoder_rnn}')\n",
    "print(f'hidden2mean: {SVAE.hidden2mean}')\n",
    "print(f'hidden2logv: {SVAE.hidden2logv}')\n",
    "print(f'z2hidden: {SVAE.z2hidden}')\n",
    "print(f'outputs2vocab: {SVAE.outputs2vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Learner\n",
    "Using PyTorch tutorial implementation (https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) for rapid development (will implement SoTA in the future)<br><br>\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskLearner(nn.Module):\n",
    "    \"\"\" Task learner for NER \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(TaskLearner, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "tasklearner = TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=104, tagset_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sequence shape: torch.Size([10])\n",
      "Forward output:\n",
      "tensor([[-1.3398, -1.3176, -1.3322, -1.5778],\n",
      "        [-1.3969, -1.3763, -1.3059, -1.4732],\n",
      "        [-1.3410, -1.3915, -1.3968, -1.4176],\n",
      "        [-1.4056, -1.3032, -1.4402, -1.4016],\n",
      "        [-1.4455, -1.3493, -1.3432, -1.4108],\n",
      "        [-1.3431, -1.3183, -1.3382, -1.5651],\n",
      "        [-1.3362, -1.3592, -1.3775, -1.4781],\n",
      "        [-1.3841, -1.3881, -1.3320, -1.4441],\n",
      "        [-1.2373, -1.5060, -1.3808, -1.4411],\n",
      "        [-1.2520, -1.3633, -1.3697, -1.5894]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Testing forward functionality\n",
    "test_seq = torch.randint(0,100,size=(10,))\n",
    "print(f'Test Sequence shape: {test_seq.shape}')\n",
    "print(f'Forward output:\\n{tasklearner.forward(test_seq)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([10, 10]) - lengths torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Testing basic training cycle\n",
    "seqs, lens = tester.build_sequences(batch_size=10, max_seq_len=10)   # Tester initialised at start of notebook\n",
    "dataset = tester.build_sequence_tags(sequences=seqs, label_space_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([41, 65, 87, 11, 62, 13, 79, 45, 58,  0]),\n",
       "  [tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32)]),\n",
       " (tensor([87,  9, 90,  2, 43, 36, 19, 14,  0,  0]),\n",
       "  [tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32)]),\n",
       " (tensor([ 3, 91,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       "  [tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32)]),\n",
       " (tensor([10,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       "  [tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32)]),\n",
       " (tensor([28,  0,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       "  [tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32)]),\n",
       " (tensor([13, 64,  0,  0,  0,  0,  0,  0,  0,  0]),\n",
       "  [tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32)]),\n",
       " (tensor([54, 61, 88, 89, 97, 67, 41,  0,  0,  0]),\n",
       "  [tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32)]),\n",
       " (tensor([17, 79, 23, 91,  0,  0,  0,  0,  0,  0]),\n",
       "  [tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32)]),\n",
       " (tensor([16, 81, 21, 40, 65, 55, 20, 15,  0,  0]),\n",
       "  [tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32)]),\n",
       " (tensor([28, 25, 87, 45, 90, 38, 58, 89, 46,  0]),\n",
       "  [tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 0, 1], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32),\n",
       "   tensor([0, 0, 1, 0], dtype=torch.int32),\n",
       "   tensor([0, 1, 0, 0], dtype=torch.int32),\n",
       "   tensor([1, 0, 0, 0], dtype=torch.int32)])]"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 3, 9, 10, 11, 13, 14, 15, 16, 17, 19, 20, 21, 23, 25, 28, 36, 38, 40, 41, 43, 45, 46, 54, 55, 58, 61, 62, 64, 65, 67, 79, 81, 87, 88, 89, 90, 91, 97]\n"
     ]
    }
   ],
   "source": [
    "# Generate vocab off of generated sequences\n",
    "vocab = list()\n",
    "for seq in seqs:\n",
    "      vocab.extend(seq.tolist())\n",
    "vocab = list(set(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training routine\n",
    "# vocab_size is max int in vocab ints + 1 as 0 is included...\n",
    "model = TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=max(vocab)+1, tagset_size=4)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4188, -1.4053, -1.3069, -1.4186],\n",
      "        [-1.3717, -1.4399, -1.4022, -1.3344],\n",
      "        [-1.3142, -1.4062, -1.4767, -1.3554],\n",
      "        [-1.3093, -1.3846, -1.4793, -1.3793],\n",
      "        [-1.4229, -1.3184, -1.4278, -1.3800],\n",
      "        [-1.3893, -1.3466, -1.3953, -1.4153],\n",
      "        [-1.3304, -1.3925, -1.4246, -1.4000],\n",
      "        [-1.4343, -1.3493, -1.2767, -1.4991],\n",
      "        [-1.3122, -1.4141, -1.3859, -1.4374],\n",
      "        [-1.4113, -1.4421, -1.3698, -1.3258]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = seqs[0]\n",
    "    # forward pass to get output scores\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-497-4996d2fad19c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mtag_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq_in\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2212\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected 2 or more dimensions (got {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2214\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2215\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[0;32m   2216\u001b[0m                          .format(input.size(0), target.size(0)))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for seq, tags in dataset:\n",
    "#         print(seq,tags)\n",
    "\n",
    "        model.zero_grad()\n",
    "        seq_in = seq\n",
    "        targets = tags\n",
    "        \n",
    "        tag_scores = model(seq_in)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" Adversary architecture for discriminator module \"\"\"\n",
    "    \n",
    "    def __init__(self, z_dim=8):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim    # latent space dimension\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                nn.Linear(z_dim, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128,1),\n",
    "                                nn.Sigmoid()\n",
    "                                )\n",
    "        # Exe\n",
    "        self.weight_init()\n",
    "        \n",
    "    def weight_init(self):\n",
    "        \"\"\" Weight initialisation\n",
    "        \n",
    "        Using Xavier uniform initialisation rather than Kaiming (I think that is more focused for CV? TODO: investigate)\n",
    "        See: https://pytorch.org/cppdocs/api/function_namespacetorch_1_1nn_1_1init_1ace282f75916a862c9678343dfd4d5ffe.html\n",
    "        \"\"\"\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                if type(m) == nn.Linear:\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\" Forward pass through discriminator\n",
    "        \n",
    "        Arguments\n",
    "        --------\n",
    "            z : tensor\n",
    "                Tensor derived from SVAE latent space\n",
    "        \"\"\"\n",
    "        return self.net(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([ 0.0312,  1.0590,  0.3862, -0.1136, -1.1375,  1.2425, -1.3496, -0.7329])\n",
      "Output: tensor([0.5398], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Testing functionality\n",
    "dsc = Discriminator()\n",
    "\n",
    "# Pass random tensor through forward pass of discriminator\n",
    "rand_tensor = torch.randn((8,))\n",
    "print(f'Input: {rand_tensor}')\n",
    "print(f'Output: {dsc.forward(rand_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler Routine\n",
    "Active learning based sample selection for task learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code copied from VAAL and modified for sequence data\n",
    "class Sampler:\n",
    "    \"\"\" Adversary sampler \"\"\"\n",
    "    def __init__(self, budget):\n",
    "        self.budget = budget\n",
    "        \n",
    "    def sample(self, vae, discriminator, data, cuda):\n",
    "        \"\"\" Selective sampling algorithm\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            vae : torch model\n",
    "                VAE model\n",
    "            discriminator : torch model\n",
    "                discriminator model\n",
    "            data : tensor\n",
    "                Image data\n",
    "            cuda : boolean\n",
    "                GPU flag\n",
    "        Returns\n",
    "        -------\n",
    "            querry_pool_indices: int, list\n",
    "                List of indices corresponding to sorted (top-K) samples to be sampled from\n",
    "        \"\"\"\n",
    "        all_preds = []\n",
    "        all_indices = []\n",
    "\n",
    "        for images, _, indices in data:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, _, mu, _ = vae(images)\n",
    "                preds = discriminator(mu)\n",
    "\n",
    "            preds = preds.cpu().data\n",
    "            all_preds.extend(preds)\n",
    "            all_indices.extend(indices)\n",
    "\n",
    "        all_preds = torch.stack(all_preds)\n",
    "        all_preds = all_preds.view(-1)\n",
    "        # need to multiply by -1 to be able to use torch.topk \n",
    "        all_preds *= -1\n",
    "\n",
    "        # select the points which the discriminator things are the most likely to be unlabeled\n",
    "        _, querry_indices = torch.topk(all_preds, int(self.budget))\n",
    "        querry_pool_indices = np.asarray(all_indices)[querry_indices]\n",
    "\n",
    "        return querry_pool_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "sampler = Sampler(budget=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine\n",
    "<i> Pseudo code</i>\n",
    "```python\n",
    "for epoch in max_epochs:\n",
    "        train(task learner)\n",
    "            get preds\n",
    "            calc loss\n",
    "            zero grads\n",
    "            backpropigate loss\n",
    "            update model parameters\n",
    "        for step in max_steps:\n",
    "            train(SVAE)\n",
    "        for step in max_steps:\n",
    "            train(discriminator)\n",
    "```<br><br>\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.vae_steps = 10\n",
    "        self.discriminator_steps = 10\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\" Performs model training \"\"\"\n",
    "    \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'\\nEpoch: {epoch}')\n",
    "            \n",
    "            print('Train task learner')\n",
    "            \n",
    "            print('Train VAE')\n",
    "            for step in range(self.vae_steps):\n",
    "                print(f'VAE Step: {step}')\n",
    "            \n",
    "            print('Train Discriminator')\n",
    "            for step in range(self.discriminator_steps):\n",
    "                print(f'Discriminator Step: {step}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 1\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 2\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 3\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 4\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 5\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 6\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 7\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 8\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n",
      "\n",
      "Epoch: 9\n",
      "Train task learner\n",
      "Train VAE\n",
      "VAE Step: 0\n",
      "VAE Step: 1\n",
      "VAE Step: 2\n",
      "VAE Step: 3\n",
      "VAE Step: 4\n",
      "VAE Step: 5\n",
      "VAE Step: 6\n",
      "VAE Step: 7\n",
      "VAE Step: 8\n",
      "VAE Step: 9\n",
      "Train Discriminator\n",
      "Discriminator Step: 0\n",
      "Discriminator Step: 1\n",
      "Discriminator Step: 2\n",
      "Discriminator Step: 3\n",
      "Discriminator Step: 4\n",
      "Discriminator Step: 5\n",
      "Discriminator Step: 6\n",
      "Discriminator Step: 7\n",
      "Discriminator Step: 8\n",
      "Discriminator Step: 9\n"
     ]
    }
   ],
   "source": [
    "slvr = Solver()\n",
    "slvr.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Routine\n",
    "To do:\n",
    " - [ ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Routine\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
