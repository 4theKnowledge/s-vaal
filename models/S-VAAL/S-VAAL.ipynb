{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Variational Adversarial Active Learning (S-VAAL)\n",
    "@author: Tyler Bikaun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook intends to flesh out an initial proof concept of the coupling of S-VAE (Bowman <i>et al.</i> 2016; https://arxiv.org/abs/1511.06349) and VAAL (Sinha <i>et al.</i> 2019; https://arxiv.org/abs/1904.00370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Application:</b> Named Entity Task (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Architecture Diagram:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23122c275e8>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.optim as optim\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Functions\n",
    "These functions are used for spot testing code whilst developing. For example, building random sequences of tensors.\n",
    "- [x] Build artificial sequence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_sequences(self, batch_size: int, max_seq_len: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Builds tensor of specified size containing variable length, padded, sequences of integers\n",
    "            \n",
    "        Arguments\n",
    "        ---------\n",
    "            batch_size : int\n",
    "                Number of sequences to generate\n",
    "            max_seq_len : int\n",
    "                Maximum length of sequences\n",
    "        Returns\n",
    "        -------\n",
    "            sequences : tensor\n",
    "                Tensor of generated sequences\n",
    "            lengths : tensor\n",
    "                Tensor of sequence lengths\n",
    "        \"\"\"\n",
    "        seqs = list()\n",
    "        for i in range(batch_size):\n",
    "            # Generate random integer sequences\n",
    "            seq = np.random.randint(low=1, high=100, size=(random.randint(1, max_seq_len),))\n",
    "            # Add padding\n",
    "            seq = np.concatenate((seq, np.zeros(shape=(max_seq_len - len(seq)))), axis=None)\n",
    "            seqs.append(seq)\n",
    "        sequences = torch.LongTensor(seqs)\n",
    "        lengths = torch.tensor([len(seq[seq != 0]) for seq in sequences])\n",
    "        \n",
    "        print(f'Shapes - seq {sequences.shape} - lengths {lengths.shape}')\n",
    "        \n",
    "        return sequences, lengths\n",
    "    \n",
    "    def build_sequence_tags(self, sequences: Tensor, lengths: Tensor, label_space_size: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given a set of sequences, generates ground truth labels\n",
    "        \n",
    "        Labels need to be non-zero (otherwise get confused with special characters; currnetly only concerned about 0 = PAD)\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            sequences : tensor\n",
    "                Tensor of generated sequences\n",
    "            label_space_size : int\n",
    "                Size of label space\n",
    "        Returns\n",
    "        -------\n",
    "            X, lengths, y : list of tuples\n",
    "                Artificial ground truth dataset\n",
    "                    X dim : (seq len, batch size )\n",
    "                    lengths dim : (batch size)\n",
    "                    y dim : (batch size, 1)\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        dataset = list()    # stores batch of data (X, lens, y)\n",
    "        \n",
    "        global_label_list = list()\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            # Each 'token' in the sequence has a label mapping\n",
    "            label_list = list()\n",
    "            for token in sequence:\n",
    "                if token != 0:   # don't give a label to any padding...\n",
    "                    label_list.append(random.randint(1,label_space_size+1))\n",
    "                else:\n",
    "                    label_list.append(0)\n",
    "            \n",
    "            global_label_list.append(torch.LongTensor(label_list))\n",
    "        \n",
    "        global_label_tensor = torch.stack(global_label_list)\n",
    "        dataset.append((sequences, lengths, global_label_tensor))   # stack list of labels into tensors\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([10, 10]) - lengths torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Test functionality\n",
    "tester = Tester()\n",
    "sequences, lengths = tester.build_sequences(batch_size=10, max_seq_len=10)\n",
    "dataset = tester.build_sequence_tags(sequences=sequences, lengths=lengths, label_space_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([10, 10]) (torch.int64) - tensor([[94, 75, 19, 60, 12, 55, 57, 52, 99,  0],\n",
      "        [77,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [63,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [18, 97, 70, 17, 96, 56, 90, 18, 84,  0],\n",
      "        [92, 87, 73, 57, 76,  9,  0,  0,  0,  0],\n",
      "        [87, 59, 74, 25, 44, 86, 82,  0,  0,  0],\n",
      "        [79, 57, 84, 62, 83, 15,  0,  0,  0,  0],\n",
      "        [29, 56, 74, 97, 12,  0,  0,  0,  0,  0],\n",
      "        [50, 78, 91, 19, 31, 20, 72, 55,  0,  0],\n",
      "        [54, 45, 25, 49,  7, 18, 42,  0,  0,  0]])\n",
      "\n",
      "y torch.Size([10, 10]) (torch.int64) - tensor([[3, 5, 1, 3, 5, 5, 3, 2, 3, 0],\n",
      "        [3, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [2, 5, 2, 5, 1, 5, 1, 4, 1, 0],\n",
      "        [5, 5, 5, 1, 3, 2, 0, 0, 0, 0],\n",
      "        [2, 2, 5, 2, 5, 3, 4, 0, 0, 0],\n",
      "        [1, 4, 3, 4, 4, 1, 0, 0, 0, 0],\n",
      "        [5, 2, 2, 4, 3, 0, 0, 0, 0, 0],\n",
      "        [2, 1, 4, 5, 1, 4, 2, 5, 0, 0],\n",
      "        [5, 3, 5, 4, 2, 5, 1, 0, 0, 0]])\n",
      "lengths torch.Size([10]) (torch.int64) - tensor([9, 1, 1, 9, 6, 7, 6, 5, 8, 7])\n"
     ]
    }
   ],
   "source": [
    "for X, length, y in dataset:\n",
    "    print(f'X {X.shape} ({X.dtype}) - {X}\\n')\n",
    "    print(f'y {y.shape} ({y.dtype}) - {y}')\n",
    "    print(f'lengths {length.shape} ({length.dtype}) - {length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Build data preprocessor\n",
    "- [ ] Build data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for model building, training, evaluation process. This will be converted into yaml.\n",
    "config = {'': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "<i>Model architectures</i><br>\n",
    "<b>SVAE</b> - RNN<br>\n",
    "<b>Discriminator</b> - FC NN<br>\n",
    "<b>Task Learner</b> - RNN<br>\n",
    "- [ ] SVAE\n",
    "- [ ] Discriminator\n",
    "- [ ] Task Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVAE\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVAE(nn.Module):\n",
    "    \"\"\" Sentence Variational Autoencoder (Bowman et al. 2016)\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SVAE, self).__init__()\n",
    "        \n",
    "        # TODO: fix dodgy vocab_size issue... this will be cleared up when utils implemented properly\n",
    "        \n",
    "        self.tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "        \n",
    "        self.max_sequence_length = 40    # arg\n",
    "        self.pad_idx = 0\n",
    "        self.eos_idx = vocab_size + 1\n",
    "        self.sos_idx = vocab_size + 2\n",
    "        self.unk_idx = vocab_size + 3\n",
    "        \n",
    "        self.vocab_size = vocab_size + 4\n",
    "        \n",
    "        self.z_dim = 8\n",
    "        \n",
    "        self.rnn_type = 'gru'\n",
    "        self.bidirectional = False\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 128\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_size)\n",
    "        self.word_dropout_rate = 0.1\n",
    "        self.embedding_dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # set rnn type\n",
    "        if self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        \n",
    "        # init encoder-decoder RNNs (models are identical)\n",
    "        self.encoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "        self.decoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "\n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.num_layers\n",
    "        \n",
    "        # Initialisation of FC layers\n",
    "        # These go from encoder to latent (z) space\n",
    "        self.hidden2mean = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.hidden2logv = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.z2hidden = nn.Linear(self.z_dim, self.hidden_size * self.hidden_factor)\n",
    "        self.outputs2vocab = nn.Linear(self.hidden_size * (2 if self.bidirectional else 1), self.vocab_size)\n",
    "        \n",
    "        # init partial loss function\n",
    "        self.NLL = nn.NLLLoss(ignore_index=self.pad_idx, reduction='sum')   # TODO: REVIEW args\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequence, length):\n",
    "        \"\"\" Forward pass through VAE \"\"\"\n",
    "        \n",
    "        batch_size = input_sequence.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(length, descending=True)   # trick for packed padding\n",
    "        input_sequence = input_sequence[sorted_idx]\n",
    "        \n",
    "        # ENCODER\n",
    "        input_embedding = self.embedding(input_sequence)\n",
    "#         print(input_embedding.shape)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        _, hidden = self._encode(packed_input)\n",
    "        \n",
    "        if self.bidirectional or 1 < self.num_layers:\n",
    "            # flatten hidden state\n",
    "            hidden = hidden.view(batch_size, self.hidden_size * self.hidden_factor)\n",
    "        else:\n",
    "            # .squeeze() -> Returns a tensor with all the dimensions of input of size 1 removed.\n",
    "            print(f'hidden shape before squeeze {hidden.shape}')\n",
    "#             hidden = hidden.squeeze()   # doesn't work? gives wrong dimension down stream...\n",
    "            pass\n",
    "            print(f'hidden shape after squeeze {hidden.shape}')\n",
    "\n",
    "        \n",
    "        # Reparameterisation trick!\n",
    "        z, mean, logv, std = self.reparameterise(hidden, batch_size)\n",
    "        \n",
    "        # DECODER\n",
    "        if 0 < self.word_dropout_rate:\n",
    "            prob = torch.rand(input_sequence.size())\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                prob = prob.cuda()\n",
    "\n",
    "            prob[(input_sequence.data - self.sos_idx) * (input_sequence.data - self.pad_idx) == 0] = 1\n",
    "\n",
    "            decoder_input_sequence = input_sequence.clone()\n",
    "            \n",
    "#             print(vocab_size)\n",
    "#             print(self.unk_idx)\n",
    "            decoder_input_sequence[prob < self.word_dropout_rate] = self.unk_idx\n",
    "\n",
    "#             print(decoder_input_sequence)\n",
    "            input_embedding = self.embedding(decoder_input_sequence)\n",
    "\n",
    "        input_embedding = self.embedding_dropout(input_embedding)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        \n",
    "        outputs, _ = self._decode(packed_input, hidden)\n",
    "        \n",
    "        # process outputs\n",
    "        # Process outputs\n",
    "        # Unpack padded sequence\n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(outputs, batch_first=True)[0]\n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        _, reversed_idx = torch.sort(sorted_idx)\n",
    "        padded_outputs = padded_outputs[reversed_idx]\n",
    "        b, s, _ = padded_outputs.size()\n",
    "\n",
    "        # Project outputs to vocab\n",
    "        # e.g. project hidden state into label space...\n",
    "        logp = nn.functional.log_softmax(self.outputs2vocab(padded_outputs.view(-1, padded_outputs.size(2))), dim=-1)\n",
    "#         print(f'logp before view {logp.shape}\\n')\n",
    "#         print(f'b {b} s {s} no emb {self.embedding.num_embeddings}')\n",
    "        logp = logp.view(b, s, self.embedding.num_embeddings)\n",
    "\n",
    "        # logp - log posterior over label space; mean - tensor Gaussian mean, logv - tensor Gaussian variance, z - VAE latent space \n",
    "        return logp, mean, logv, z\n",
    "    \n",
    "    def to_var(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return x\n",
    "    \n",
    "    def kl_anneal_function(self, anneal_function, step, k, x0):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if anneal_function == 'logistic':\n",
    "            return float(1/(1+np.exp(-k*(step-x0))))\n",
    "        elif anneal_function == 'linear':\n",
    "            return min(1, step/x0)\n",
    "        \n",
    "    def loss_fn(self, logp, target, mean, logv, anneal_function, step, k, x0):\n",
    "        \"\"\"\n",
    "        SVAE loss function\n",
    "        \n",
    "        NLL - Negative Log Likelihood loss between predicted tags and ground truth\n",
    "        KL_Loss - Evidence Lower Bound (ELBO)? - TODO: validate interpretation\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # insert assertion\n",
    "        \n",
    "        # ALL THE CODE BELOW NEEDS REVIEW and piece wise implemented\n",
    "        # Cut-off unnecessary padding from target and flatten\n",
    "        target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
    "        logp = logp.view(-1, logp.size(2))\n",
    "        \n",
    "        # Negative log likelihood\n",
    "        NLL_loss = self.NLL(logp, target)\n",
    "        \n",
    "        # KL Divergence (or ELBO? TODO: determine which)\n",
    "        KL_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp())\n",
    "        KL_weight = self.kl_anneal_function(anneal_function, step, k, x0)\n",
    "        \n",
    "        return NLL_loss, KL_loss, KL_weight\n",
    "    \n",
    "    def reparameterise(self, hidden, batch_size):\n",
    "        \"\"\" Implement reparameterisation trick (Kingma and Welling 2014) \"\"\"\n",
    "        \n",
    "        mean = self.hidden2mean(hidden)\n",
    "        logv = self.hidden2logv(hidden)\n",
    "        std = torch.exp(0.5 * logv) \n",
    "        \n",
    "        z = self.to_var(torch.randn([batch_size, self.z_dim]))\n",
    "        return z * std + mean, mean, logv, std\n",
    "    \n",
    "    def _encode(self, x):\n",
    "        \"\"\" x - pack padded sequence \"\"\"\n",
    "        return self.encoder_rnn(x)\n",
    "    \n",
    "    def _decode(self, x, hidden):\n",
    "        \"\"\" x - pack padded sequence\n",
    "            hidden - latent tensor\"\"\"\n",
    "        return self.decoder_rnn(x, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "vocab_size = 100\n",
    "hidden_size = 128\n",
    "svae = SVAE(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([10, 40]) - lengths torch.Size([10])\n",
      "hidden shape before squeeze torch.Size([1, 10, 128])\n",
      "hidden shape after squeeze torch.Size([1, 10, 128])\n",
      "(tensor([[[-4.8427, -4.6776, -4.5889,  ..., -4.5877, -4.4809, -5.0085],\n",
      "         [-4.7896, -4.5403, -4.8593,  ..., -4.4677, -4.6747, -5.0080],\n",
      "         [-4.5772, -4.7142, -4.6639,  ..., -4.6588, -4.4193, -4.8670],\n",
      "         ...,\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832],\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832],\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832]],\n",
      "\n",
      "        [[-5.1999, -4.7055, -4.6987,  ..., -4.6957, -4.9011, -4.7669],\n",
      "         [-4.9151, -4.7409, -4.3450,  ..., -4.5813, -4.9597, -4.7458],\n",
      "         [-4.4706, -4.5708, -4.5558,  ..., -4.5318, -5.0321, -4.3316],\n",
      "         ...,\n",
      "         [-4.6787, -4.8079, -4.4416,  ..., -5.1192, -4.5961, -4.2864],\n",
      "         [-4.6036, -4.9025, -4.4304,  ..., -4.8440, -4.6667, -4.5289],\n",
      "         [-4.7724, -4.9886, -4.6115,  ..., -4.9312, -4.6180, -4.5664]],\n",
      "\n",
      "        [[-4.5843, -4.6755, -4.6702,  ..., -4.4331, -4.4029, -4.3187],\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832],\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832],\n",
      "         ...,\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832],\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832],\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.9105, -4.6362, -4.8109,  ..., -4.4758, -4.6656, -4.7438],\n",
      "         [-5.0865, -4.7032, -4.9511,  ..., -4.6153, -4.2743, -4.6390],\n",
      "         [-5.2245, -4.7754, -5.1115,  ..., -4.5880, -4.5379, -4.3855],\n",
      "         ...,\n",
      "         [-4.6685, -4.9463, -4.2054,  ..., -4.4145, -4.6852, -4.3935],\n",
      "         [-4.6778, -5.0558, -4.4248,  ..., -4.2623, -4.6702, -4.4376],\n",
      "         [-4.8840, -4.8613, -4.6118,  ..., -4.7251, -4.5811, -4.4187]],\n",
      "\n",
      "        [[-4.5616, -4.7252, -4.6752,  ..., -4.7800, -4.5349, -4.6299],\n",
      "         [-4.5573, -4.7816, -4.6982,  ..., -4.5268, -4.6400, -4.9688],\n",
      "         [-4.3832, -4.3915, -4.8548,  ..., -4.4301, -4.4237, -4.6196],\n",
      "         ...,\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832],\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832],\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832]],\n",
      "\n",
      "        [[-4.5176, -4.9769, -4.7197,  ..., -4.5153, -4.5337, -4.8874],\n",
      "         [-4.2984, -4.6301, -4.6527,  ..., -4.4236, -4.5515, -4.8350],\n",
      "         [-4.7459, -4.5254, -5.0462,  ..., -4.5603, -4.6420, -4.7418],\n",
      "         ...,\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832],\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832],\n",
      "         [-4.6962, -4.6314, -4.6847,  ..., -4.6126, -4.5657, -4.5832]]],\n",
      "       grad_fn=<ViewBackward>), tensor([[[-0.1593, -0.0061, -0.2326, -0.3957, -0.3099, -0.2370,  0.2317,\n",
      "           0.1911],\n",
      "         [ 0.0628, -0.1951, -0.3518, -0.0725, -0.2407, -0.0421,  0.0819,\n",
      "           0.2763],\n",
      "         [-0.1247, -0.0063, -0.3572,  0.0412, -0.1972, -0.3647, -0.0964,\n",
      "           0.0080],\n",
      "         [-0.2017, -0.2461, -0.1574,  0.0647,  0.1289, -0.1280, -0.2292,\n",
      "           0.3079],\n",
      "         [-0.0500,  0.1385,  0.0273, -0.1764, -0.1461, -0.1438,  0.3058,\n",
      "           0.1411],\n",
      "         [-0.0316, -0.1228, -0.0390, -0.0923, -0.0913,  0.0408, -0.0264,\n",
      "          -0.2969],\n",
      "         [-0.1628,  0.1888, -0.4176,  0.1831, -0.0139, -0.1440,  0.0108,\n",
      "          -0.0650],\n",
      "         [ 0.1232, -0.2645, -0.1386,  0.0485,  0.0479, -0.1488, -0.1096,\n",
      "           0.0888],\n",
      "         [ 0.0698, -0.0710, -0.4133, -0.3255, -0.0242, -0.2178, -0.0056,\n",
      "           0.2066],\n",
      "         [ 0.0621,  0.0130, -0.0420,  0.0960,  0.0830, -0.1303,  0.2157,\n",
      "           0.0740]]], grad_fn=<AddBackward0>), tensor([[[ 0.1534, -0.2971, -0.1621,  0.0975, -0.0197, -0.1519, -0.0688,\n",
      "          -0.2909],\n",
      "         [-0.1010,  0.2278, -0.1384,  0.1913,  0.1651, -0.0172, -0.0794,\n",
      "          -0.2351],\n",
      "         [ 0.3364, -0.1975,  0.0137, -0.0468, -0.1596, -0.0999, -0.1932,\n",
      "          -0.1070],\n",
      "         [ 0.1646, -0.1381, -0.0467,  0.1638, -0.1524,  0.2452, -0.1055,\n",
      "           0.0007],\n",
      "         [-0.0769, -0.0773, -0.2258,  0.0060, -0.4517,  0.0363,  0.1131,\n",
      "          -0.1578],\n",
      "         [ 0.0141, -0.1288, -0.2765,  0.0756, -0.0952, -0.0569,  0.2735,\n",
      "           0.0587],\n",
      "         [-0.0262,  0.1602,  0.0962, -0.0488, -0.1539,  0.2185, -0.0441,\n",
      "           0.1515],\n",
      "         [ 0.2981, -0.0218, -0.1308, -0.0009, -0.1876, -0.0341, -0.0065,\n",
      "           0.0083],\n",
      "         [ 0.2432,  0.0868, -0.2244, -0.0321, -0.1214,  0.1987, -0.3421,\n",
      "           0.4179],\n",
      "         [ 0.1449, -0.1719,  0.0139,  0.1383, -0.0173, -0.1792,  0.0198,\n",
      "          -0.0757]]], grad_fn=<AddBackward0>), tensor([[[-0.2059,  0.4173,  0.0972,  0.2670, -0.2256,  1.7163,  0.0885,\n",
      "           0.1156],\n",
      "         [ 0.6912,  2.6348, -1.5209, -1.9754,  0.0887,  0.0221,  0.9291,\n",
      "          -0.3425],\n",
      "         [ 2.0927, -2.0678, -2.0655, -2.1716, -1.3227, -0.0413,  0.6530,\n",
      "           0.4688],\n",
      "         [ 0.9417, -0.9302,  0.8428,  0.4507,  0.1969,  0.7287,  0.8407,\n",
      "          -2.0346],\n",
      "         [-0.2731, -0.7758,  2.2758, -1.6393,  1.3007, -2.4229, -1.4154,\n",
      "           1.3963],\n",
      "         [ 0.7032, -0.1015, -0.6228,  0.8272,  1.4884,  1.2113, -0.4259,\n",
      "          -0.4244],\n",
      "         [ 0.8042, -1.0696, -2.9726, -1.7361,  1.0945, -0.0202,  2.2274,\n",
      "           1.4727],\n",
      "         [-1.9092, -1.0759, -0.5223, -0.1475,  0.6644,  1.4966,  0.2052,\n",
      "           0.0776],\n",
      "         [ 1.2672, -0.2873, -0.2618,  0.9060, -0.1553, -0.5141, -1.7642,\n",
      "          -0.9457],\n",
      "         [-1.3345,  0.9695, -1.1337,  1.9970,  0.6697,  1.7062, -0.3928,\n",
      "          -0.4515]]], grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass of SVAE\n",
    "svae_ff_seqs, svae_ff_lengths = Tester().build_sequences(batch_size=10, max_seq_len=40)\n",
    "\n",
    "# Pass sequences and lengths into SVAE forward method\n",
    "print(svae.forward(svae_ff_seqs, svae_ff_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: GRU(128, 128, batch_first=True)\n",
      "Decoder: GRU(128, 128, batch_first=True)\n",
      "hidden2mean: Linear(in_features=128, out_features=8, bias=True)\n",
      "hidden2logv: Linear(in_features=128, out_features=8, bias=True)\n",
      "z2hidden: Linear(in_features=8, out_features=128, bias=True)\n",
      "outputs2vocab: Linear(in_features=128, out_features=104, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Reviewing components\n",
    "print(f'Encoder: {svae.encoder_rnn}')\n",
    "print(f'Decoder: {svae.decoder_rnn}')\n",
    "print(f'hidden2mean: {svae.hidden2mean}')\n",
    "print(f'hidden2logv: {svae.hidden2logv}')\n",
    "print(f'z2hidden: {svae.z2hidden}')\n",
    "print(f'outputs2vocab: {svae.outputs2vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Learner\n",
    "Using PyTorch tutorial implementation (https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) for rapid development (will implement SoTA in the future)<br><br>\n",
    "To do:\n",
    " - [ ] Make batch based to suit other models rather than training on singletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskLearner(nn.Module):\n",
    "    \"\"\" Task learner for NER \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(TaskLearner, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)    # expects input (batch, seq, feature)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, input_sequences, lengths):\n",
    "        \"\"\"\n",
    "        Pack padded sequence and sorted lengths used to optimise forward pass through RNN\n",
    "        \n",
    "        \n",
    "        input_sequence : tensor\n",
    "            (batch, seq, feature)\n",
    "        \n",
    "        \"\"\"\n",
    "#         print(input_sequences)\n",
    "    \n",
    "        batch_size = input_sequences.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(lengths, descending=True)\n",
    "        input_sequences = input_sequences[sorted_idx]\n",
    "#         print(input_sequences)\n",
    "        \n",
    "        input_embeddings = self.word_embeddings(input_sequences)\n",
    "#         print(input_embeddings)\n",
    "        \n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embeddings, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(packed_input)\n",
    "        \n",
    "        # Unpack padded sequence\n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(lstm_out, batch_first=True)[0]\n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        _, reversed_idx = torch.sort(sorted_idx)\n",
    "        padded_outputs = padded_outputs[reversed_idx]\n",
    "        b, s, _ = padded_outputs.size()\n",
    "        \n",
    "        # project into label space\n",
    "        tag_space = self.hidden2tag(padded_outputs.view(-1, padded_outputs.size(2)))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "        \n",
    "#         embeds = self.word_embeddings(sentence)\n",
    "#         lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "#         tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "#         tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "#         return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([1, 10]) - lengths torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Testing functionality\n",
    "tasklearner = TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=104, tagset_size=4)\n",
    "# Generating data for testing\n",
    "seqs, lens = tester.build_sequences(batch_size=1, max_seq_len=10)   # Tester initialised at start of notebook\n",
    "dataset = tester.build_sequence_tags(sequences=seqs, lengths=lens, label_space_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass output shape: torch.Size([7, 4])\n"
     ]
    }
   ],
   "source": [
    "# Testing forward pass functionality\n",
    "for X, lens, y in dataset:\n",
    "#     print(X, lens, y)\n",
    "    forward_out = tasklearner.forward(input_sequences=X, lengths=lens)\n",
    "    print(f'Forward pass output shape: {forward_out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 65, 34, 38, 53, 85, 54, 63]\n"
     ]
    }
   ],
   "source": [
    "# Generate vocab off of generated sequences\n",
    "vocab = list()\n",
    "for seq in seqs:\n",
    "      vocab.extend(seq.tolist())\n",
    "vocab = list(set(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training routine\n",
    "# vocab_size is max int in vocab ints + 1 as 0 is included...\n",
    "model = TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=max(vocab)+1, tagset_size=4)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "torch.Size([7, 4])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(seqs.shape)\n",
    "    # forward pass to get output scores\n",
    "    tag_scores = model(seqs, lens)\n",
    "    print(tag_scores.shape)\n",
    "#     print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Shapes | Seq: torch.Size([1, 10]) Lengths: torch.Size([1]) Tags: torch.Size([10, 1])\n",
      "tag scores: tensor([[-1.4664, -1.3899, -1.4179, -1.2804],\n",
      "        [-1.4406, -1.3350, -1.4387, -1.3363],\n",
      "        [-1.4851, -1.2373, -1.5764, -1.2851],\n",
      "        [-1.4839, -1.2943, -1.4764, -1.3068],\n",
      "        [-1.4404, -1.3593, -1.3852, -1.3624],\n",
      "        [-1.5102, -1.3866, -1.2913, -1.3693],\n",
      "        [-1.5538, -1.4244, -1.2121, -1.3850]], grad_fn=<LogSoftmaxBackward>) - Shape: torch.Size([7, 4])\n",
      "batch tags: tensor([1, 2, 4, 3, 2, 4, 1, 1, 2, 1]) - Shape: torch.Size([10])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (7) to match target batch_size (10).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-290-b38dea634551>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'batch tags: {batch_tags} - Shape: {batch_tags.shape}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_tags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2214\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2215\u001b[0m         raise ValueError('Expected input batch_size ({}) to match target batch_size ({}).'\n\u001b[1;32m-> 2216\u001b[1;33m                          .format(input.size(0), target.size(0)))\n\u001b[0m\u001b[0;32m   2217\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2218\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (7) to match target batch_size (10)."
     ]
    }
   ],
   "source": [
    "# need to fix to be batch based rather than individual sequence\n",
    "for epoch in range(5):\n",
    "    \n",
    "    # \n",
    "    for batch_seqs, batch_lens, batch_tags in dataset:\n",
    "        print(f'Epoch: {epoch}')\n",
    "        print(f'Shapes | Seq: {batch_seqs.shape} Lengths: {batch_lens.shape} Tags: {batch_tags.shape}')\n",
    "\n",
    "        model.zero_grad()\n",
    "        batch_tags = batch_tags.view(-1)\n",
    "        \n",
    "        tag_scores = model(batch_seqs, batch_lens)\n",
    "        print(f'tag scores: {tag_scores} - Shape: {tag_scores.shape}')\n",
    "        print(f'batch tags: {batch_tags} - Shape: {batch_tags.shape}')\n",
    "        \n",
    "        loss = loss_function(tag_scores, batch_tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 128, got 1280",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-77bbedfd0b09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtag_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-ea5510dff7ba>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mtag_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden2tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mtag_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtag_space\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    572\u001b[0m             \u001b[0mhx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 574\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    575\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[0mexpected_hidden_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mcheck_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    177\u001b[0m             raise RuntimeError(\n\u001b[0;32m    178\u001b[0m                 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n\u001b[1;32m--> 179\u001b[1;33m                     self.input_size, input.size(-1)))\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_expected_hidden_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 128, got 1280"
     ]
    }
   ],
   "source": [
    "# need to fix to be batch based rather than singleton\n",
    "with torch.no_grad():\n",
    "    inputs, _, _ = dataset[0]\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" Adversary architecture for discriminator module \n",
    "    \n",
    "    Tensor needs to be FloatTensor; sequences and tag sets are LongTensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, z_dim=8):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim    # latent space dimension (will be the same shape as the encoder output from VAE)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                nn.Linear(z_dim, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128,1),\n",
    "                                nn.Sigmoid()\n",
    "                                )\n",
    "        # Exe\n",
    "        self.weight_init()\n",
    "        \n",
    "    def weight_init(self):\n",
    "        \"\"\" Weight initialisation\n",
    "        \n",
    "        Using Xavier uniform initialisation rather than Kaiming (I think that is more focused for CV? TODO: investigate)\n",
    "        See: https://pytorch.org/cppdocs/api/function_namespacetorch_1_1nn_1_1init_1ace282f75916a862c9678343dfd4d5ffe.html\n",
    "        \"\"\"\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                if type(m) == nn.Linear:\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\" Forward pass through discriminator\n",
    "        \n",
    "        Arguments\n",
    "        --------\n",
    "            z : tensor\n",
    "                Tensor derived from SVAE latent space\n",
    "        \"\"\"\n",
    "        return self.net(z.type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([85, 21, 26, 30, 77, 66, 35, 16])\n",
      "Output: tensor([0.0033], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Testing functionality\n",
    "z_dim = 8\n",
    "discriminator = Discriminator(z_dim=z_dim)\n",
    "\n",
    "# Pass random integer tensor through forward pass of discriminator\n",
    "rand_tensor = torch.randint(low=0, high=100,size=(z_dim,))\n",
    "print(f'Input: {rand_tensor}')\n",
    "print(f'Output: {discriminator.forward(rand_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.7053, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Testing training functionality for discriminator\n",
    "dsc_bce_loss = nn.BCELoss()\n",
    "dsc_optim = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "discriminator.train()\n",
    "\n",
    "preds = discriminator(rand_tensor)\n",
    "real_labels = torch.ones(preds.size(0))\n",
    "loss = dsc_bce_loss(preds, real_labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler Routine\n",
    "Active learning based sample selection for task learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code copied from VAAL and modified for sequence data\n",
    "class Sampler:\n",
    "    \"\"\" Adversary sampler \"\"\"\n",
    "    def __init__(self, budget):\n",
    "        self.budget = budget\n",
    "        \n",
    "    def sample(self, vae, discriminator, data, cuda):\n",
    "        \"\"\" Selective sampling algorithm\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            vae : torch model\n",
    "                VAE model\n",
    "            discriminator : torch model\n",
    "                discriminator model\n",
    "            data : tensor\n",
    "                Image data\n",
    "            cuda : boolean\n",
    "                GPU flag\n",
    "        Returns\n",
    "        -------\n",
    "            querry_pool_indices: int, list\n",
    "                List of indices corresponding to sorted (top-K) samples to be sampled from\n",
    "        \"\"\"\n",
    "        all_preds = []\n",
    "        all_indices = []\n",
    "\n",
    "        for images, _, indices in data:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, _, mu, _ = vae(images)\n",
    "                preds = discriminator(mu)\n",
    "\n",
    "            preds = preds.cpu().data\n",
    "            all_preds.extend(preds)\n",
    "            all_indices.extend(indices)\n",
    "\n",
    "        all_preds = torch.stack(all_preds)\n",
    "        all_preds = all_preds.view(-1)\n",
    "        # need to multiply by -1 to be able to use torch.topk \n",
    "        all_preds *= -1\n",
    "\n",
    "        # select the points which the discriminator things are the most likely to be unlabeled\n",
    "        _, querry_indices = torch.topk(all_preds, int(self.budget))\n",
    "        querry_pool_indices = np.asarray(all_indices)[querry_indices]\n",
    "\n",
    "        return querry_pool_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "sampler = Sampler(budget=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine\n",
    "<i> Pseudo code</i>\n",
    "\n",
    "```python\n",
    "for epoch in max_epochs:\n",
    "        train(task learner)\n",
    "            get preds\n",
    "            calc loss\n",
    "            zero grads\n",
    "            backpropigate loss\n",
    "            update model parameters\n",
    "        for step in max_steps:\n",
    "            train(SVAE)\n",
    "        for step in max_steps:\n",
    "            train(discriminator)\n",
    "```\n",
    "To do:\n",
    " - [ ] Implement training cycle for task learn\n",
    " - [ ] Implement training cycle for VAE\n",
    " - [ ] Implement training cycle for discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, task_learner, vae, discriminator):\n",
    "        self.epochs = 2\n",
    "        self.svae_steps = 2\n",
    "        self.discriminator_steps = 2\n",
    "        \n",
    "        # Get models\n",
    "        # init else where?\n",
    "        self.task_learner =  TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=max(vocab)+1, tagset_size=4)\n",
    "        self.svae = SVAE(vocab_size=max(vocab)+1, embedding_size=128)\n",
    "        self.discriminator = Discriminator(z_dim=10)   # z_dim will be the latent output of the vae encoder... currently just the seq length\n",
    "        \n",
    "        # Loss functions\n",
    "        self.nll_loss = nn.NLLLoss()   # used in: TL\n",
    "        self.bce_loss = nn.BCELoss()   # used in: Discriminator\n",
    "        self.xxx_loss = 0    # used in: SVAE\n",
    "\n",
    "        # optimiser\n",
    "        self.optim_lr = 0.1\n",
    "        self.tl_optim = optim.SGD(model.parameters(), lr=self.optim_lr)   # todo: revisit learning rate\n",
    "        self.svae_optim = None\n",
    "        self.disc_optim = optim.Adam(self.discriminator.parameters(), lr=0.01)   # todo: revisit learning rate\n",
    "        \n",
    "        # Will need to implement this in the future, need to build generator function rather than rely on Torch implementationss?\n",
    "        self.data_loader = 'generator'\n",
    "        self.seqs, self.lens = tester.build_sequences(batch_size=10, max_seq_len=10)   # Tester initialised at start of notebook\n",
    "        self.dataset = tester.build_sequence_tags(sequences=self.seqs, lengths=self.lens, label_space_size=4)\n",
    "        \n",
    "        # Will need to implement this in the future, inherit the Sampler class?\n",
    "        self.sampler = 'sampler'\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\" Performs model training \"\"\"\n",
    "        \n",
    "        # turn on .train() mode\n",
    "        self.svae.train()\n",
    "        self.task_learner.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'\\nEpoch: {epoch}')\n",
    "            \n",
    "            # these will be batches in the future, not single sequence/tag sets...\n",
    "            # build generator to wrap around sequence/tag sets and then just call next() on them at each epoch to get a batch...\n",
    "            # need to build that in the future to partition based on the unlabelled/labelled splits for AL experiments\n",
    "            for seqs, lens, tags in self.dataset:\n",
    "                print(f'Dataset shapes\\nSeqs:{seqs.shape}\\tLens:{lens.shape}\\tTags:{tags.shape}')\n",
    "            \n",
    "                # Train Task Learner\n",
    "                self.tl_train_step(seqs, tags)\n",
    "            \n",
    "            \n",
    "                # Train VAE\n",
    "                for step in range(self.svae_steps):\n",
    "                    print(f'SVAE Step: {step}')\n",
    "                    self.svae_train_step(seqs, lens, tags)\n",
    "\n",
    "                # Train Discriminator\n",
    "                for step in range(self.discriminator_steps):\n",
    "                    print(f'Discriminator Step: {step}')\n",
    "\n",
    "                    self.disc_train_step(seqs)\n",
    "            clear_output(wait=True)\n",
    "                \n",
    "    def tl_train_step(self, seqs, tags):\n",
    "        \"\"\"\n",
    "        Perform training step of task learner \n",
    "        \n",
    "        Tensor dtype is LongTensor\n",
    "        \"\"\"\n",
    "#         print(seq.dtype)\n",
    "#         assert seq.dtype == 'torch.int64'\n",
    "#         assert tags.dtype == 'torch.int64'\n",
    "        \n",
    "        # zero grad as they accumulate in PyTorch\n",
    "        self.task_learner.zero_grad()\n",
    "        preds = self.task_learner(seqs)\n",
    "        tl_loss = self.nll_loss(preds, tags.view(-1).type(torch.LongTensor))   # flatten tags as they are a 1D tensor rather than 2D\n",
    "        tl_loss.backward()\n",
    "        self.tl_optim.step()\n",
    "        \n",
    "        print(f'Task learning loss: {loss.data:0.4f}')\n",
    "    \n",
    "    def svae_train_step(self, seqs, lengths, tags):\n",
    "        \"\"\"\n",
    "        Perform training step of sentence variational autoencoder\n",
    "        \n",
    "        Tensor type is LongTensor\n",
    "        \"\"\"\n",
    "#         print(self.svae)\n",
    "        \n",
    "        # Forward pass\n",
    "        logp, mean, logv, z = self.svae(seq, lengths)\n",
    "        \n",
    "        # Loss calculation\n",
    "        NLL_loss, KL_loss, KL_weight = loss_fn(logp, tags, lengths, mean, logv, anneal_function, step, k, x0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def disc_train_step(self, seqs):\n",
    "        \"\"\" Perform training step of adversarial discriminatory \n",
    "        \n",
    "        Note: The loss function will be an aggregate across labelled and unlabelled samples (currently just labelled)\n",
    "        \n",
    "        Tensor dtype is FloatTensor for discriminator\n",
    "        \"\"\"\n",
    "        \n",
    "#         assert seq.dtype == 'torch.float64'\n",
    "        \n",
    "#         print(seq)\n",
    "#         print(seq.dtype)\n",
    "        \n",
    "        preds = self.discriminator(seqs)\n",
    "#         print(preds)\n",
    "        real_labels = torch.ones(preds.size(0))\n",
    "        disc_loss = self.bce_loss(preds, real_labels)\n",
    "        disc_loss.backward()\n",
    "        self.disc_optim.step()\n",
    "        \n",
    "        print(f'Discriminator loss: {loss.data}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([10, 10]) - lengths torch.Size([10])\n",
      "\n",
      "Epoch: 0\n",
      "Dataset shapes\n",
      "Seqs:torch.Size([10, 10])\tLens:torch.Size([10])\tTags:torch.Size([10, 1])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-136-ccbb7286f639>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Test training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mslvr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtask_learner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'tl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'vae'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'disc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mslvr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-135-549b073dead8>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;31m# Train Task Learner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtl_train_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-135-549b073dead8>\u001b[0m in \u001b[0;36mtl_train_step\u001b[1;34m(self, seqs, tags)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[1;31m# zero grad as they accumulate in PyTorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_learner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtask_learner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseqs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mtl_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# flatten tags as they are a 1D tensor rather than 2D\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mtl_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-ea5510dff7ba>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, sentence)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0membeds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mlstm_out\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membeds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mtag_space\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden2tag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m         return F.embedding(\n\u001b[0;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tyler\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1812\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1814\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1816\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "# Test training \n",
    "slvr = Solver(task_learner='tl', vae='vae', discriminator='disc')\n",
    "slvr.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Routine\n",
    "To do:\n",
    " - [ ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Routine\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
