{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Variational Adversarial Active Learning (S-VAAL)\n",
    "@author: Tyler Bikaun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook intends to flesh out an initial proof concept of the coupling of S-VAE (Bowman <i>et al.</i> 2016; https://arxiv.org/abs/1511.06349) and VAAL (Sinha <i>et al.</i> 2019; https://arxiv.org/abs/1904.00370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Application:</b> Named Entity Task (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Architecture Diagram:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2337fec75e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Functions\n",
    "These functions are used for spot testing code whilst developing. For example, building random sequences of tensors.\n",
    "- [x] Build artificial sequence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def build_sequences(self, batch_size: int, max_seq_len: int) -> int:\n",
    "        \"\"\"\n",
    "        Builds tensor of specified size containing variable length, padded, sequences of integers\n",
    "            \n",
    "        Arguments\n",
    "        ---------\n",
    "            batch_size : int\n",
    "                Number of sequences to generate\n",
    "            max_seq_len : int\n",
    "                Maximum length of sequences\n",
    "        Returns\n",
    "        -------\n",
    "            sequences : tensor\n",
    "                Tensor of generated sequences\n",
    "            lengths : tensor\n",
    "                Tensor of sequence lengths\n",
    "        \"\"\"\n",
    "        seqs = list()\n",
    "        for i in range(batch_size):\n",
    "            # Generate random integer sequences\n",
    "            seq = np.random.randint(low=1, high=100, size=(random.randint(1, max_seq_len),))\n",
    "            # Add padding\n",
    "            seq = np.concatenate((seq, np.zeros(shape=(max_seq_len - len(seq)))), axis=None)\n",
    "            seqs.append(seq)\n",
    "        sequences = torch.LongTensor(seqs)\n",
    "        lengths = torch.tensor([len(seq[seq != 0]) for seq in sequences])\n",
    "        \n",
    "        print(f'Shapes - seq {sequences.shape} - lengths {lengths.shape}')\n",
    "        \n",
    "        return sequences, lengths\n",
    "    \n",
    "    def build_sequence_tags(self, sequences, label_space_size: int):\n",
    "        \"\"\"\n",
    "        Given a set of sequences, generates ground truth labels\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            sequences : tensor\n",
    "                Tensor of generated sequences\n",
    "            label_space_size : int\n",
    "                Size of label space\n",
    "        Returns\n",
    "        -------\n",
    "            X,y : list of tuples\n",
    "                Artificial ground truth dataset\n",
    "                    X dim : (seq batch size)\n",
    "                    y dim : (seq batch size, label space size)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = len(sequences)\n",
    "        \n",
    "        dataset = list()\n",
    "        for sequence in sequences:\n",
    "            # each 'token' in the sequence has a label mapping\n",
    "            label_set = list()\n",
    "            for token in sequence:\n",
    "                # ONE HOT NOT REQUIRED; LABEL SPACE IS A SINGLE INTEGER VALUE\n",
    "#                 one_hot_array = np.zeros(shape=(label_space_size))\n",
    "#                 one_hot_array[random.randint(0,label_space_size-1)]=1\n",
    "#                 labels = torch.IntTensor(one_hot_array)\n",
    "                label_set.append(torch.randint(low=0,high=label_space_size, size=(1,), dtype=torch.long))\n",
    "            dataset.append((sequence, torch.stack(label_set)))   # stack list of labels into tensors\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([10, 10]) - lengths torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Test functionality\n",
    "tester = Tester()\n",
    "sequences, lengths = tester.build_sequences(batch_size=10, max_seq_len=10)\n",
    "dataset = tester.build_sequence_tags(sequences=sequences, label_space_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([10]) (torch.int64) - tensor([58, 78, 50,  0,  0,  0,  0,  0,  0,  0])\n",
      "\n",
      "y torch.Size([10, 1]) (torch.int64) - tensor([[3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [2],\n",
      "        [1],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "for X, y in dataset:\n",
    "    print(f'X {X.shape} ({X.dtype}) - {X}\\n')\n",
    "    print(f'y {y.shape} ({y.dtype}) - {y}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Build data preprocessor\n",
    "- [ ] Build data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for model building, training, evaluation process. This will be converted into yaml.\n",
    "config = {'': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "<i>Model architectures</i><br>\n",
    "<b>SVAE</b> - RNN<br>\n",
    "<b>Discriminator</b> - FC NN<br>\n",
    "<b>Task Learner</b> - RNN<br>\n",
    "- [ ] SVAE\n",
    "- [ ] Discriminator\n",
    "- [ ] Task Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVAE\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVAE(nn.Module):\n",
    "    \"\"\" Sentence Variational Autoencoder (Bowman et al. 2016)\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SVAE, self).__init__()\n",
    "        \n",
    "        # TODO: fix dodgy vocab_size issue... this will be cleared up when utils implemented properly\n",
    "        \n",
    "        self.tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "        \n",
    "        self.max_sequence_length = 40    # arg\n",
    "        self.pad_idx = 0\n",
    "        self.eos_idx = vocab_size + 1\n",
    "        self.sos_idx = vocab_size + 2\n",
    "        self.unk_idx = vocab_size + 3\n",
    "        \n",
    "        self.vocab_size = vocab_size + 4\n",
    "        \n",
    "        self.z_dim = 8\n",
    "        \n",
    "        self.rnn_type = 'gru'\n",
    "        self.bidirectional = False\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 128\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_size)\n",
    "        self.word_dropout_rate = 0.1\n",
    "        self.embedding_dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # set rnn type\n",
    "        if self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        \n",
    "        # init encoder-decoder RNNs (models are identical)\n",
    "        self.encoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "        self.decoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "\n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.num_layers\n",
    "        \n",
    "        # Initialisation of FC layers\n",
    "        # These go from encoder to latent (z) space\n",
    "        self.hidden2mean = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.hidden2logv = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.z2hidden = nn.Linear(self.z_dim, self.hidden_size * self.hidden_factor)\n",
    "        self.outputs2vocab = nn.Linear(self.hidden_size * (2 if self.bidirectional else 1), self.vocab_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequence, length):\n",
    "        \"\"\" Forward pass through VAE \"\"\"\n",
    "        \n",
    "        batch_size = input_sequence.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(length, descending=True)   # trick for packed padding\n",
    "        input_sequence = input_sequence[sorted_idx]\n",
    "        \n",
    "        # ENCODER\n",
    "        input_embedding = self.embedding(input_sequence)\n",
    "#         print(input_embedding.shape)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        _, hidden = self._encode(packed_input)\n",
    "        \n",
    "        if self.bidirectional or 1 < self.num_layers:\n",
    "            # flatten hidden state\n",
    "            hidden = hidden.view(batch_size, self.hidden_size * self.hidden_factor)\n",
    "        else:\n",
    "            # .squeeze() -> Returns a tensor with all the dimensions of input of size 1 removed.\n",
    "            print(f'hidden shape before squeeze {hidden.shape}')\n",
    "#             hidden = hidden.squeeze()   # doesn't work? gives wrong dimension down stream...\n",
    "            pass\n",
    "            print(f'hidden shape after squeeze {hidden.shape}')\n",
    "\n",
    "        \n",
    "        # Reparameterisation trick!\n",
    "        z, mean, logv, std = self.reparameterise(hidden, batch_size)\n",
    "        \n",
    "        # DECODER\n",
    "        if 0 < self.word_dropout_rate:\n",
    "            prob = torch.rand(input_sequence.size())\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                prob = prob.cuda()\n",
    "\n",
    "            prob[(input_sequence.data - self.sos_idx) * (input_sequence.data - self.pad_idx) == 0] = 1\n",
    "\n",
    "            decoder_input_sequence = input_sequence.clone()\n",
    "            \n",
    "#             print(vocab_size)\n",
    "#             print(self.unk_idx)\n",
    "            decoder_input_sequence[prob < self.word_dropout_rate] = self.unk_idx\n",
    "\n",
    "#             print(decoder_input_sequence)\n",
    "            input_embedding = self.embedding(decoder_input_sequence)\n",
    "\n",
    "        input_embedding = self.embedding_dropout(input_embedding)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        \n",
    "        outputs, _ = self._decode(packed_input, hidden)\n",
    "        \n",
    "        # process outputs\n",
    "        # Process outputs\n",
    "        # Unpack padded sequence\n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(outputs, batch_first=True)[0]\n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        _, reversed_idx = torch.sort(sorted_idx)\n",
    "        padded_outputs = padded_outputs[reversed_idx]\n",
    "        b, s, _ = padded_outputs.size()\n",
    "\n",
    "        # Project outputs to vocab\n",
    "        # e.g. project hidden state into label space...\n",
    "        logp = nn.functional.log_softmax(self.outputs2vocab(padded_outputs.view(-1, padded_outputs.size(2))), dim=-1)\n",
    "#         print(f'logp before view {logp.shape}\\n')\n",
    "#         print(f'b {b} s {s} no emb {self.embedding.num_embeddings}')\n",
    "        logp = logp.view(b, s, self.embedding.num_embeddings)\n",
    "\n",
    "        # logp - log posterior over label space; mean - tensor Gaussian mean, logv - tensor Gaussian variance, z - VAE latent space \n",
    "        return logp, mean, logv, z\n",
    "    \n",
    "    def to_var(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return x\n",
    "    \n",
    "    def reparameterise(self, hidden, batch_size):\n",
    "        \"\"\" Implement reparameterisation trick (Kingma and Welling 2014) \"\"\"\n",
    "        \n",
    "        mean = self.hidden2mean(hidden)\n",
    "        logv = self.hidden2logv(hidden)\n",
    "        std = torch.exp(0.5 * logv) \n",
    "        \n",
    "        z = self.to_var(torch.randn([batch_size, self.z_dim]))\n",
    "        return z * std + mean, mean, logv, std\n",
    "    \n",
    "    def _encode(self, x):\n",
    "        \"\"\" x - pack padded sequence \"\"\"\n",
    "        return self.encoder_rnn(x)\n",
    "    \n",
    "    def _decode(self, x, hidden):\n",
    "        \"\"\" x - pack padded sequence\n",
    "            hidden - latent tensor\"\"\"\n",
    "        return self.decoder_rnn(x, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "vocab_size = 100\n",
    "hidden_size = 128\n",
    "SVAE = SVAE(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([10, 40]) - lengths torch.Size([10])\n",
      "hidden shape before squeeze torch.Size([1, 10, 128])\n",
      "hidden shape after squeeze torch.Size([1, 10, 128])\n",
      "(tensor([[[-4.7531, -4.8106, -4.9538,  ..., -4.6081, -4.5295, -4.5324],\n",
      "         [-4.7728, -4.7408, -4.6469,  ..., -4.6056, -4.4521, -4.6779],\n",
      "         [-4.6151, -4.6546, -4.6274,  ..., -4.2124, -4.4680, -4.6956],\n",
      "         ...,\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948],\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948],\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948]],\n",
      "\n",
      "        [[-4.4980, -4.3337, -4.8377,  ..., -4.6214, -4.5822, -4.5876],\n",
      "         [-4.5180, -4.0787, -4.6752,  ..., -4.5264, -4.4273, -4.4877],\n",
      "         [-4.4887, -4.2805, -4.7759,  ..., -4.2257, -4.3880, -4.4419],\n",
      "         ...,\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948],\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948],\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948]],\n",
      "\n",
      "        [[-4.6297, -4.4382, -4.8879,  ..., -4.5435, -4.5940, -4.6714],\n",
      "         [-4.8426, -4.7348, -4.9092,  ..., -4.5548, -4.5773, -4.8163],\n",
      "         [-4.8882, -4.5890, -4.6846,  ..., -4.6837, -4.2275, -4.4199],\n",
      "         ...,\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948],\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948],\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.8094, -4.4826, -4.7731,  ..., -4.5477, -4.5852, -4.3909],\n",
      "         [-4.6105, -4.7294, -4.8426,  ..., -4.4879, -4.0813, -4.3267],\n",
      "         [-4.9024, -4.2879, -4.8112,  ..., -4.6673, -4.4781, -4.4770],\n",
      "         ...,\n",
      "         [-4.4467, -4.6100, -4.3515,  ..., -5.0480, -4.4592, -4.3181],\n",
      "         [-4.5342, -4.6003, -4.6035,  ..., -5.2723, -4.5631, -4.3898],\n",
      "         [-4.4551, -4.4971, -4.4861,  ..., -5.0370, -4.6766, -4.5196]],\n",
      "\n",
      "        [[-4.7997, -4.7353, -4.8119,  ..., -4.3632, -4.7673, -4.3449],\n",
      "         [-4.6851, -4.3824, -5.1270,  ..., -4.2168, -5.0046, -4.5727],\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948],\n",
      "         ...,\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948],\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948],\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948]],\n",
      "\n",
      "        [[-4.8572, -4.6611, -4.9918,  ..., -4.7571, -4.4729, -4.7000],\n",
      "         [-4.7382, -4.4820, -4.6919,  ..., -4.5654, -4.4996, -4.7592],\n",
      "         [-4.8612, -4.7135, -4.5943,  ..., -4.3636, -4.6044, -4.8296],\n",
      "         ...,\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948],\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948],\n",
      "         [-4.6454, -4.6193, -4.7378,  ..., -4.6265, -4.5784, -4.5948]]],\n",
      "       grad_fn=<ViewBackward>), tensor([[[-0.0052,  0.1381,  0.2490,  0.0568, -0.2103, -0.4033, -0.1490,\n",
      "           0.0563],\n",
      "         [-0.1167,  0.0912, -0.0623,  0.0311, -0.2205, -0.0908, -0.0520,\n",
      "          -0.1231],\n",
      "         [-0.1394, -0.1383,  0.4172,  0.4615, -0.1341, -0.1233,  0.1717,\n",
      "          -0.4117],\n",
      "         [-0.4753,  0.1804,  0.0708, -0.0419, -0.3048, -0.2699, -0.1270,\n",
      "           0.2981],\n",
      "         [-0.3174,  0.0291, -0.0054, -0.2165, -0.3939,  0.0315, -0.1860,\n",
      "           0.2379],\n",
      "         [-0.4696,  0.1204,  0.2490,  0.0639, -0.0155,  0.0187, -0.2228,\n",
      "           0.2327],\n",
      "         [-0.4824,  0.2742, -0.0960, -0.0043,  0.1271,  0.1030, -0.0804,\n",
      "          -0.1360],\n",
      "         [-0.3565,  0.3915,  0.1557, -0.2625, -0.4706, -0.5664, -0.2653,\n",
      "           0.2326],\n",
      "         [-0.0507,  0.0455,  0.1224,  0.1380, -0.1454, -0.2238, -0.0643,\n",
      "           0.1987],\n",
      "         [ 0.0610,  0.0094,  0.1080, -0.0452, -0.3625,  0.1733, -0.0554,\n",
      "          -0.1466]]], grad_fn=<AddBackward0>), tensor([[[-0.1184,  0.0009, -0.0352,  0.1761,  0.2406,  0.1460,  0.1978,\n",
      "          -0.1970],\n",
      "         [-0.0526, -0.0180, -0.0168,  0.0433,  0.1361,  0.1737, -0.4135,\n",
      "          -0.0843],\n",
      "         [-0.0054, -0.2324,  0.0469, -0.0293,  0.1520, -0.2801,  0.0910,\n",
      "           0.0699],\n",
      "         [-0.0499, -0.0278,  0.0696,  0.1664,  0.2403,  0.2284,  0.2006,\n",
      "           0.0247],\n",
      "         [-0.0718, -0.3262, -0.1030, -0.0999, -0.1662, -0.1162,  0.0696,\n",
      "           0.2300],\n",
      "         [-0.0278, -0.2228,  0.3168,  0.0081,  0.3576,  0.3839,  0.1024,\n",
      "           0.2792],\n",
      "         [ 0.1624,  0.3331,  0.0011,  0.4186, -0.2733,  0.0835,  0.3951,\n",
      "          -0.0100],\n",
      "         [-0.1825, -0.0194, -0.2857, -0.0445,  0.2381,  0.0042,  0.0242,\n",
      "          -0.0909],\n",
      "         [-0.0345, -0.1686, -0.1239,  0.0240,  0.2544, -0.0534,  0.1758,\n",
      "          -0.0927],\n",
      "         [ 0.1941,  0.0065,  0.1539,  0.1079, -0.0434,  0.0330,  0.1010,\n",
      "           0.1243]]], grad_fn=<AddBackward0>), tensor([[[ 1.5857e+00,  6.2101e-01, -3.5679e-01, -2.4540e-01, -6.7665e-01,\n",
      "          -8.1762e-01, -5.0615e-01, -5.5724e-01],\n",
      "         [ 1.2809e+00, -2.1412e-01,  1.6028e+00, -1.1784e+00, -3.5276e-01,\n",
      "          -1.8008e+00,  5.6313e-01, -1.4561e+00],\n",
      "         [-5.0505e-01,  1.6541e-01, -3.9344e-01,  3.5480e-01, -1.1468e+00,\n",
      "           1.4410e+00,  1.5526e+00, -1.4021e-01],\n",
      "         [-2.9863e-02, -9.4140e-01, -2.9943e-01,  6.4750e-02,  1.0977e+00,\n",
      "           7.1739e-01,  2.2663e+00,  2.0559e+00],\n",
      "         [-1.2386e+00, -6.5832e-01,  1.7141e-04,  2.5461e-01, -6.6753e-01,\n",
      "          -1.5257e+00, -6.7225e-01,  3.2332e-01],\n",
      "         [-1.1002e+00, -1.7913e-02, -3.2276e+00, -7.2773e-01,  7.6677e-01,\n",
      "           3.4038e-01,  3.5664e-01,  1.4948e+00],\n",
      "         [-1.4972e-01,  1.5700e+00, -1.9383e-01, -5.4208e-01,  1.1076e+00,\n",
      "          -6.0851e-01, -5.4876e-01,  3.6019e-01],\n",
      "         [ 3.9642e-01,  1.5079e-01,  1.4341e+00, -1.6747e+00, -1.4422e+00,\n",
      "          -3.2403e-01, -1.1196e+00,  3.3904e-01],\n",
      "         [ 1.9158e+00, -1.8956e+00,  3.9380e-01,  6.7199e-01,  1.7046e-01,\n",
      "          -8.2691e-01, -1.2492e+00,  2.0755e+00],\n",
      "         [ 7.3905e-01,  6.5898e-01,  1.2132e+00, -1.3947e+00, -6.2872e-01,\n",
      "          -1.7282e+00,  5.7924e-01, -4.1288e-01]]], grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass of SVAE\n",
    "svae_ff_seqs, svae_ff_lengths = Tester().build_sequences(batch_size=10, max_seq_len=40)\n",
    "\n",
    "# Pass sequences and lengths into SVAE forward method\n",
    "print(SVAE.forward(svae_ff_seqs, svae_ff_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: GRU(128, 128, batch_first=True)\n",
      "Decoder: GRU(128, 128, batch_first=True)\n",
      "hidden2mean: Linear(in_features=128, out_features=8, bias=True)\n",
      "hidden2logv: Linear(in_features=128, out_features=8, bias=True)\n",
      "z2hidden: Linear(in_features=8, out_features=128, bias=True)\n",
      "outputs2vocab: Linear(in_features=128, out_features=104, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Reviewing components\n",
    "print(f'Encoder: {SVAE.encoder_rnn}')\n",
    "print(f'Decoder: {SVAE.decoder_rnn}')\n",
    "print(f'hidden2mean: {SVAE.hidden2mean}')\n",
    "print(f'hidden2logv: {SVAE.hidden2logv}')\n",
    "print(f'z2hidden: {SVAE.z2hidden}')\n",
    "print(f'outputs2vocab: {SVAE.outputs2vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Learner\n",
    "Using PyTorch tutorial implementation (https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) for rapid development (will implement SoTA in the future)<br><br>\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskLearner(nn.Module):\n",
    "    \"\"\" Task learner for NER \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(TaskLearner, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "tasklearner = TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=104, tagset_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Sequence shape: torch.Size([10])\n",
      "Forward output:\n",
      "tensor([[-1.4794, -1.2650, -1.3656, -1.4492],\n",
      "        [-1.4865, -1.2843, -1.3209, -1.4692],\n",
      "        [-1.4257, -1.3145, -1.3280, -1.4870],\n",
      "        [-1.3941, -1.3571, -1.3332, -1.4658],\n",
      "        [-1.3769, -1.3420, -1.3499, -1.4826],\n",
      "        [-1.4530, -1.2435, -1.4825, -1.3834],\n",
      "        [-1.5051, -1.2764, -1.4306, -1.3479],\n",
      "        [-1.4349, -1.3009, -1.3679, -1.4485],\n",
      "        [-1.5529, -1.2953, -1.2882, -1.4322],\n",
      "        [-1.5578, -1.2940, -1.3844, -1.3290]], grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Testing forward functionality\n",
    "test_seq = torch.randint(0,100,size=(10,))\n",
    "print(f'Test Sequence shape: {test_seq.shape}')\n",
    "print(f'Forward output:\\n{tasklearner.forward(test_seq)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([10, 10]) - lengths torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "# Testing basic training cycle\n",
    "seqs, lens = tester.build_sequences(batch_size=10, max_seq_len=10)   # Tester initialised at start of notebook\n",
    "dataset = tester.build_sequence_tags(sequences=seqs, label_space_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4, 5, 6, 7, 8, 15, 16, 18, 20, 21, 22, 24, 25, 27, 29, 30, 31, 37, 39, 41, 42, 44, 46, 50, 51, 52, 53, 54, 61, 65, 69, 73, 74, 75, 80, 81, 87, 88, 89, 91, 92, 96]\n"
     ]
    }
   ],
   "source": [
    "# Generate vocab off of generated sequences\n",
    "vocab = list()\n",
    "for seq in seqs:\n",
    "      vocab.extend(seq.tolist())\n",
    "vocab = list(set(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training routine\n",
    "# vocab_size is max int in vocab ints + 1 as 0 is included...\n",
    "model = TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=max(vocab)+1, tagset_size=4)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.4035, -1.4262, -1.4404, -1.2830],\n",
      "        [-1.3603, -1.4606, -1.5082, -1.2378],\n",
      "        [-1.3004, -1.5160, -1.5087, -1.2490],\n",
      "        [-1.2736, -1.5231, -1.5250, -1.2570],\n",
      "        [-1.2643, -1.5132, -1.5407, -1.2618],\n",
      "        [-1.2610, -1.5026, -1.5530, -1.2642],\n",
      "        [-1.2600, -1.4942, -1.5617, -1.2654],\n",
      "        [-1.2601, -1.4881, -1.5675, -1.2658],\n",
      "        [-1.2606, -1.4838, -1.5714, -1.2658],\n",
      "        [-1.2612, -1.4809, -1.5741, -1.2657]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = seqs[0]\n",
    "    # forward pass to get output scores\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "tensor(1.3794)\n",
      "tensor(1.4134)\n",
      "tensor(1.4450)\n",
      "tensor(1.4319)\n",
      "tensor(1.3841)\n",
      "tensor(1.4103)\n",
      "tensor(1.4326)\n",
      "tensor(1.3774)\n",
      "tensor(1.4121)\n",
      "tensor(1.3973)\n",
      "Epoch: 1\n",
      "tensor(1.4099)\n",
      "tensor(1.3550)\n",
      "tensor(1.4328)\n",
      "tensor(1.4307)\n",
      "tensor(1.3450)\n",
      "tensor(1.3413)\n",
      "tensor(1.3767)\n",
      "tensor(1.3472)\n",
      "tensor(1.4051)\n",
      "tensor(1.3582)\n",
      "Epoch: 2\n",
      "tensor(1.4198)\n",
      "tensor(1.3316)\n",
      "tensor(1.4144)\n",
      "tensor(1.4278)\n",
      "tensor(1.3143)\n",
      "tensor(1.3202)\n",
      "tensor(1.3321)\n",
      "tensor(1.3245)\n",
      "tensor(1.3933)\n",
      "tensor(1.3250)\n",
      "Epoch: 3\n",
      "tensor(1.4179)\n",
      "tensor(1.3163)\n",
      "tensor(1.3944)\n",
      "tensor(1.4218)\n",
      "tensor(1.2864)\n",
      "tensor(1.3108)\n",
      "tensor(1.2927)\n",
      "tensor(1.3037)\n",
      "tensor(1.3802)\n",
      "tensor(1.2939)\n",
      "Epoch: 4\n",
      "tensor(1.4133)\n",
      "tensor(1.3031)\n",
      "tensor(1.3742)\n",
      "tensor(1.4149)\n",
      "tensor(1.2595)\n",
      "tensor(1.3048)\n",
      "tensor(1.2559)\n",
      "tensor(1.2830)\n",
      "tensor(1.3665)\n",
      "tensor(1.2634)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    print(f'Epoch: {epoch}')\n",
    "    for seq, tags in dataset:\n",
    "#         print(seq,tags)\n",
    "\n",
    "        model.zero_grad()\n",
    "        seq_in = seq\n",
    "        targets = tags.view(-1)\n",
    "#         print(targets.shape)\n",
    "#         print(targets.view(-1).shape)\n",
    "        \n",
    "        tag_scores = model(seq_in)\n",
    "        \n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(loss.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs, _ = dataset[0]\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" Adversary architecture for discriminator module \n",
    "    \n",
    "    Tensor needs to be FloatTensor; sequences and tag sets are LongTensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, z_dim=8):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim    # latent space dimension (will be the same shape as the encoder output from VAE)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                nn.Linear(z_dim, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128,1),\n",
    "                                nn.Sigmoid()\n",
    "                                )\n",
    "        # Exe\n",
    "        self.weight_init()\n",
    "        \n",
    "    def weight_init(self):\n",
    "        \"\"\" Weight initialisation\n",
    "        \n",
    "        Using Xavier uniform initialisation rather than Kaiming (I think that is more focused for CV? TODO: investigate)\n",
    "        See: https://pytorch.org/cppdocs/api/function_namespacetorch_1_1nn_1_1init_1ace282f75916a862c9678343dfd4d5ffe.html\n",
    "        \"\"\"\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                if type(m) == nn.Linear:\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\" Forward pass through discriminator\n",
    "        \n",
    "        Arguments\n",
    "        --------\n",
    "            z : tensor\n",
    "                Tensor derived from SVAE latent space\n",
    "        \"\"\"\n",
    "        return self.net(z.type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([59, 92,  6,  7, 99, 33, 74, 59])\n",
      "Output: tensor([0.2480], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Testing functionality\n",
    "z_dim = 8\n",
    "discriminator = Discriminator(z_dim=z_dim)\n",
    "\n",
    "# Pass random integer tensor through forward pass of discriminator\n",
    "rand_tensor = torch.randint(low=0, high=100,size=(z_dim,))\n",
    "print(f'Input: {rand_tensor}')\n",
    "print(f'Output: {discriminator.forward(rand_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3945, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Testing training functionality for discriminator\n",
    "dsc_bce_loss = nn.BCELoss()\n",
    "dsc_optim = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "discriminator.train()\n",
    "\n",
    "preds = discriminator(rand_tensor)\n",
    "real_labels = torch.ones(preds.size(0))\n",
    "loss = dsc_bce_loss(preds, real_labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler Routine\n",
    "Active learning based sample selection for task learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code copied from VAAL and modified for sequence data\n",
    "class Sampler:\n",
    "    \"\"\" Adversary sampler \"\"\"\n",
    "    def __init__(self, budget):\n",
    "        self.budget = budget\n",
    "        \n",
    "    def sample(self, vae, discriminator, data, cuda):\n",
    "        \"\"\" Selective sampling algorithm\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            vae : torch model\n",
    "                VAE model\n",
    "            discriminator : torch model\n",
    "                discriminator model\n",
    "            data : tensor\n",
    "                Image data\n",
    "            cuda : boolean\n",
    "                GPU flag\n",
    "        Returns\n",
    "        -------\n",
    "            querry_pool_indices: int, list\n",
    "                List of indices corresponding to sorted (top-K) samples to be sampled from\n",
    "        \"\"\"\n",
    "        all_preds = []\n",
    "        all_indices = []\n",
    "\n",
    "        for images, _, indices in data:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, _, mu, _ = vae(images)\n",
    "                preds = discriminator(mu)\n",
    "\n",
    "            preds = preds.cpu().data\n",
    "            all_preds.extend(preds)\n",
    "            all_indices.extend(indices)\n",
    "\n",
    "        all_preds = torch.stack(all_preds)\n",
    "        all_preds = all_preds.view(-1)\n",
    "        # need to multiply by -1 to be able to use torch.topk \n",
    "        all_preds *= -1\n",
    "\n",
    "        # select the points which the discriminator things are the most likely to be unlabeled\n",
    "        _, querry_indices = torch.topk(all_preds, int(self.budget))\n",
    "        querry_pool_indices = np.asarray(all_indices)[querry_indices]\n",
    "\n",
    "        return querry_pool_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "sampler = Sampler(budget=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine\n",
    "<i> Pseudo code</i>\n",
    "\n",
    "```python\n",
    "for epoch in max_epochs:\n",
    "        train(task learner)\n",
    "            get preds\n",
    "            calc loss\n",
    "            zero grads\n",
    "            backpropigate loss\n",
    "            update model parameters\n",
    "        for step in max_steps:\n",
    "            train(SVAE)\n",
    "        for step in max_steps:\n",
    "            train(discriminator)\n",
    "```\n",
    "To do:\n",
    " - [ ] Implement training cycle for task learn\n",
    " - [ ] Implement training cycle for VAE\n",
    " - [ ] Implement training cycle for discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver:\n",
    "    def __init__(self, task_learner, vae, discriminator):\n",
    "        self.epochs = 2\n",
    "        self.vae_steps = 2\n",
    "        self.discriminator_steps = 2\n",
    "        \n",
    "        # Get models\n",
    "        # init else where?\n",
    "        self.task_learner =  TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=max(vocab)+1, tagset_size=4)\n",
    "        self.svae = SVAE(max(vocab)+1, hidden_size=128)\n",
    "        self.discriminator = Discriminator(z_dim=10)   # z_dim will be the latent output of the vae encoder... currently just the seq length\n",
    "        \n",
    "        # Loss functions\n",
    "        self.nll_loss = nn.NLLLoss()   # used in: TL\n",
    "        self.bce_loss = nn.BCELoss()   # used in: Discriminator\n",
    "        self.xxx_loss = 0    # used in: SVAE\n",
    "\n",
    "        # optimiser\n",
    "        self.optim_lr = 0.1\n",
    "        self.tl_optim = optim.SGD(model.parameters(), lr=self.optim_lr)   # todo: revisit learning rate\n",
    "        self.svae_optim = None\n",
    "        self.disc_optim = optim.Adam(self.discriminator.parameters(), lr=0.01)   # todo: revisit learning rate\n",
    "        \n",
    "        # Will need to implement this in the future, need to build generator function rather than rely on Torch implementationss?\n",
    "        self.data_loader = 'generator'\n",
    "        self.seqs, self.lens = tester.build_sequences(batch_size=10, max_seq_len=10)   # Tester initialised at start of notebook\n",
    "        self.dataset = tester.build_sequence_tags(sequences=seqs, label_space_size=4)\n",
    "        \n",
    "        # Will need to implement this in the future, inherit the Sampler class?\n",
    "        self.sampler = 'sampler'\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\" Performs model training \"\"\"\n",
    "        \n",
    "        # turn on .train() mode\n",
    "        self.svae.train()\n",
    "        self.task_learner.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            print(f'\\nEpoch: {epoch}')\n",
    "            \n",
    "            # these will be batches in the future, not single sequence/tag sets...\n",
    "            # build generator to wrap around sequence/tag sets and then just call next() on them at each epoch to get a batch...\n",
    "            # need to build that in the future to partition based on the unlabelled/labelled splits for AL experiments\n",
    "            for seq, tags in self.dataset:\n",
    "#                 print(seq, tags)\n",
    "            \n",
    "                # Train Task Learner\n",
    "                self.tl_train_step(seq, tags)\n",
    "            \n",
    "            \n",
    "                # Train VAE\n",
    "                for step in range(self.svae_steps):\n",
    "                    print(f'SVAE Step: {step}')\n",
    "                    self.svae_train_step()\n",
    "\n",
    "                # Train Discriminator\n",
    "                for step in range(self.discriminator_steps):\n",
    "                    print(f'Discriminator Step: {step}')\n",
    "\n",
    "                    self.disc_train_step(seq)\n",
    "            clear_output(wait=True)\n",
    "                \n",
    "    def tl_train_step(self, seqs, tags):\n",
    "        \"\"\" Perform training step of task learner \n",
    "        \n",
    "        Tensor dtype is Long for tl\n",
    "        \"\"\"\n",
    "#         print(seq.dtype)\n",
    "#         assert seq.dtype == 'torch.int64'\n",
    "#         assert tags.dtype == 'torch.int64'\n",
    "        \n",
    "        # zero grad as they accumulate in PyTorch\n",
    "        self.task_learner.zero_grad()\n",
    "        preds = self.task_learner(seqs)\n",
    "        tl_loss = self.nll_loss(preds, tags.view(-1).type(torch.LongTensor))   # flatten tags as they are a 1D tensor rather than 2D\n",
    "        tl_loss.backward()\n",
    "        self.tl_optim.step()\n",
    "        \n",
    "        print(f'Task learning loss: {loss.data:0.4f}')\n",
    "    \n",
    "    def svae_train_step(self, seqs, lengths, tags):\n",
    "        \"\"\" Perform training step of sentence variational autoencoder \"\"\"\n",
    "#         print(self.svae)\n",
    "        \n",
    "        # Forward pass\n",
    "        logp, mean, logv, z = self.svae(seq, lengths)\n",
    "        \n",
    "        # Loss calculation\n",
    "        NLL_loss, KL_loss, KL_weight = loss_fn(logp, tags, lengths, mean, logv, anneal_function, step, k, x0)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def disc_train_step(self, seqs):\n",
    "        \"\"\" Perform training step of adversarial discriminatory \n",
    "        \n",
    "        Note: The loss function will be an aggregate across labelled and unlabelled samples (currently just labelled)\n",
    "        \n",
    "        Tensor dtype is FloatTensor for discriminator\n",
    "        \"\"\"\n",
    "        \n",
    "#         assert seq.dtype == 'torch.float64'\n",
    "        \n",
    "#         print(seq)\n",
    "#         print(seq.dtype)\n",
    "        \n",
    "        preds = self.discriminator(seqs)\n",
    "#         print(preds)\n",
    "        real_labels = torch.ones(preds.size(0))\n",
    "        disc_loss = self.bce_loss(preds, real_labels)\n",
    "        disc_loss.backward()\n",
    "        self.disc_optim.step()\n",
    "        \n",
    "        print(f'Discriminator loss: {loss.data}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1\n",
      "Task learning loss: 1.3945\n",
      "VAE Step: 0\n",
      "vae\n",
      "VAE Step: 1\n",
      "vae\n",
      "Discriminator Step: 0\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Discriminator Step: 1\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Task learning loss: 1.3945\n",
      "VAE Step: 0\n",
      "vae\n",
      "VAE Step: 1\n",
      "vae\n",
      "Discriminator Step: 0\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Discriminator Step: 1\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Task learning loss: 1.3945\n",
      "VAE Step: 0\n",
      "vae\n",
      "VAE Step: 1\n",
      "vae\n",
      "Discriminator Step: 0\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Discriminator Step: 1\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Task learning loss: 1.3945\n",
      "VAE Step: 0\n",
      "vae\n",
      "VAE Step: 1\n",
      "vae\n",
      "Discriminator Step: 0\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Discriminator Step: 1\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Task learning loss: 1.3945\n",
      "VAE Step: 0\n",
      "vae\n",
      "VAE Step: 1\n",
      "vae\n",
      "Discriminator Step: 0\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Discriminator Step: 1\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Task learning loss: 1.3945\n",
      "VAE Step: 0\n",
      "vae\n",
      "VAE Step: 1\n",
      "vae\n",
      "Discriminator Step: 0\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Discriminator Step: 1\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Task learning loss: 1.3945\n",
      "VAE Step: 0\n",
      "vae\n",
      "VAE Step: 1\n",
      "vae\n",
      "Discriminator Step: 0\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Discriminator Step: 1\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Task learning loss: 1.3945\n",
      "VAE Step: 0\n",
      "vae\n",
      "VAE Step: 1\n",
      "vae\n",
      "Discriminator Step: 0\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Discriminator Step: 1\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Task learning loss: 1.3945\n",
      "VAE Step: 0\n",
      "vae\n",
      "VAE Step: 1\n",
      "vae\n",
      "Discriminator Step: 0\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Discriminator Step: 1\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Task learning loss: 1.3945\n",
      "VAE Step: 0\n",
      "vae\n",
      "VAE Step: 1\n",
      "vae\n",
      "Discriminator Step: 0\n",
      "Discriminator loss: 1.3944728374481201\n",
      "Discriminator Step: 1\n",
      "Discriminator loss: 1.3944728374481201\n"
     ]
    }
   ],
   "source": [
    "# Test training \n",
    "slvr = Solver(task_learner='tl', vae='vae', discriminator='disc')\n",
    "slvr.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Routine\n",
    "To do:\n",
    " - [ ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Routine\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
