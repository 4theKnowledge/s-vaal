{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Variational Adversarial Active Learning (S-VAAL)\n",
    "@author: Tyler Bikaun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook intends to flesh out an initial proof concept of the coupling of S-VAE (Bowman <i>et al.</i> 2016; https://arxiv.org/abs/1511.06349) and VAAL (Sinha <i>et al.</i> 2019; https://arxiv.org/abs/1904.00370)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Application:</b> Named Entity Task (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Architecture Diagram:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17ad32895e8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.optim as optim\n",
    "\n",
    "Tensor = torch.Tensor\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Functions\n",
    "These functions are used for spot testing code whilst developing. For example, building random sequences of tensors.\n",
    "- [x] Build artificial sequence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tester:\n",
    "    def __init__(self):\n",
    "        self.pad_idx = 0\n",
    "        self.special_chars_list = [self.pad_idx]\n",
    "        self.no_output_classes = 4\n",
    "        self.label_space_size = self.no_output_classes + len(self.special_chars_list)\n",
    "        \n",
    "        print(f'Using label space size of: {self.label_space_size}')\n",
    "\n",
    "        \n",
    "    def build_sequences(self, batch_size: int, max_seq_len: int) -> Tensor:\n",
    "        \"\"\"\n",
    "        Builds tensor of specified size containing variable length, padded, sequences of integers\n",
    "            \n",
    "        Arguments\n",
    "        ---------\n",
    "            batch_size : int\n",
    "                Number of sequences to generate\n",
    "            max_seq_len : int\n",
    "                Maximum length of sequences\n",
    "        Returns\n",
    "        -------\n",
    "            sequences : tensor\n",
    "                Tensor of generated sequences\n",
    "            lengths : tensor\n",
    "                Tensor of sequence lengths\n",
    "        \"\"\"\n",
    "        seqs = list()\n",
    "        for i in range(batch_size):\n",
    "            # Generate random integer sequences\n",
    "            # sequence must be at least 1 token long...\n",
    "            seq = np.random.randint(low=1, high=100, size=(random.randint(1, max_seq_len),))\n",
    "            # Add padding\n",
    "            seq = np.concatenate((seq, np.ones(shape=(max_seq_len - len(seq)))*self.pad_idx), axis=None)\n",
    "            seqs.append(seq)\n",
    "        sequences = torch.LongTensor(seqs)\n",
    "        lengths = torch.tensor([len(seq[seq != self.pad_idx]) for seq in sequences])\n",
    "        \n",
    "        print(f'Shapes - seq {sequences.shape} - lengths {lengths.shape}')\n",
    "        \n",
    "        return sequences, lengths\n",
    "    \n",
    "    def build_sequence_tags(self, sequences: Tensor, lengths: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given a set of sequences, generates ground truth labels\n",
    "        \n",
    "        Labels need to be non-zero (otherwise get confused with special characters; currnetly only concerned about 0 = PAD)\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            sequences : tensor\n",
    "                Tensor of generated sequences\n",
    "            label_space_size : int\n",
    "                Size of label space\n",
    "        Returns\n",
    "        -------\n",
    "            X, lengths, y : list of tuples\n",
    "                Artificial ground truth dataset\n",
    "                    X dim : (seq len, batch size )\n",
    "                    lengths dim : (batch size)\n",
    "                    y dim : (batch size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        dataset = list()    # stores batch of data (X, lens, y)\n",
    "        \n",
    "        global_label_list = list()\n",
    "        \n",
    "        for sequence in sequences:\n",
    "            # Each 'token' in the sequence has a label mapping\n",
    "            label_list = list()\n",
    "            for token in sequence:\n",
    "                if token != self.pad_idx:   # don't give a label to any padding...\n",
    "                    label_list.append(random.randint(1, self.label_space_size-1))   # need to minus 1 as output loss function indexes from 0 to n_class - 1\n",
    "                else:\n",
    "                    label_list.append(self.pad_idx)\n",
    "            \n",
    "            global_label_list.append(torch.LongTensor(label_list))\n",
    "        \n",
    "        global_label_tensor = torch.stack(global_label_list)\n",
    "        dataset.append((sequences, lengths, global_label_tensor))   # stack list of labels into tensors\n",
    "        return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label space size of: 5\n",
      "Shapes - seq torch.Size([1, 10]) - lengths torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Test functionality\n",
    "tester = Tester()\n",
    "sequences, lengths = tester.build_sequences(batch_size=1, max_seq_len=10)\n",
    "dataset = tester.build_sequence_tags(sequences=sequences, lengths=lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X torch.Size([1, 10]) (torch.int64) - tensor([[70, 26,  7, 12, 18,  0,  0,  0,  0,  0]])\n",
      "y torch.Size([1, 10]) (torch.int64) - tensor([[3, 2, 3, 2, 1, 0, 0, 0, 0, 0]])\n",
      "lengths torch.Size([1]) (torch.int64) - tensor([5])\n"
     ]
    }
   ],
   "source": [
    "for X, length, y in dataset:\n",
    "    print(f'X {X.shape} ({X.dtype}) - {X}')\n",
    "    print(f'y {y.shape} ({y.dtype}) - {y}')\n",
    "    print(f'lengths {length.shape} ({length.dtype}) - {length}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [ ] Build data preprocessor\n",
    "- [ ] Build data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration for model building, training, evaluation process. This will be converted into yaml.\n",
    "config = {'': ''}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models\n",
    "<i>Model architectures</i><br>\n",
    "<b>SVAE</b> - RNN<br>\n",
    "<b>Discriminator</b> - FC NN<br>\n",
    "<b>Task Learner</b> - RNN<br>\n",
    "- [ ] SVAE\n",
    "- [ ] Discriminator\n",
    "- [ ] Task Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVAE\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVAE(nn.Module):\n",
    "    \"\"\" Sentence Variational Autoencoder (Bowman et al. 2016)\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        super(SVAE, self).__init__()\n",
    "        \n",
    "        # TODO: fix dodgy vocab_size issue... this will be cleared up when utils implemented properly\n",
    "        \n",
    "        self.tensor = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.Tensor\n",
    "        \n",
    "        self.max_sequence_length = 40    # arg\n",
    "        self.pad_idx = 0\n",
    "        self.eos_idx = vocab_size + 1\n",
    "        self.sos_idx = vocab_size + 2\n",
    "        self.unk_idx = vocab_size + 3\n",
    "        \n",
    "        self.vocab_size = vocab_size + 4\n",
    "        \n",
    "        self.z_dim = 8\n",
    "        \n",
    "        self.rnn_type = 'gru'\n",
    "        self.bidirectional = False\n",
    "        self.num_layers = 1\n",
    "        self.hidden_size = 128\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, embedding_size)\n",
    "        self.word_dropout_rate = 0.1\n",
    "        self.embedding_dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # set rnn type\n",
    "        if self.rnn_type == 'gru':\n",
    "            rnn = nn.GRU\n",
    "        else:\n",
    "            raise ValueError()\n",
    "        \n",
    "        # init encoder-decoder RNNs (models are identical)\n",
    "        self.encoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "        self.decoder_rnn = rnn(embedding_size,\n",
    "                               self.hidden_size, \n",
    "                               num_layers=self.num_layers,\n",
    "                               bidirectional=self.bidirectional,\n",
    "                               batch_first=True)\n",
    "\n",
    "        self.hidden_factor = (2 if self.bidirectional else 1) * self.num_layers\n",
    "        \n",
    "        # Initialisation of FC layers\n",
    "        # These go from encoder to latent (z) space\n",
    "        self.hidden2mean = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.hidden2logv = nn.Linear(self.hidden_size * self.hidden_factor, self.z_dim)\n",
    "        self.z2hidden = nn.Linear(self.z_dim, self.hidden_size * self.hidden_factor)\n",
    "        self.outputs2vocab = nn.Linear(self.hidden_size * (2 if self.bidirectional else 1), self.vocab_size)\n",
    "        \n",
    "        # init partial loss function\n",
    "        self.NLL = nn.NLLLoss(ignore_index=self.pad_idx, reduction='sum')   # TODO: REVIEW args\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequence, length):\n",
    "        \"\"\" Forward pass through VAE \"\"\"\n",
    "        \n",
    "        batch_size = input_sequence.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(length, descending=True)   # trick for packed padding\n",
    "        input_sequence = input_sequence[sorted_idx]\n",
    "        \n",
    "        # ENCODER\n",
    "        input_embedding = self.embedding(input_sequence)\n",
    "#         print(input_embedding.shape)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        _, hidden = self._encode(packed_input)\n",
    "        \n",
    "        if self.bidirectional or 1 < self.num_layers:\n",
    "            # flatten hidden state\n",
    "            hidden = hidden.view(batch_size, self.hidden_size * self.hidden_factor)\n",
    "        else:\n",
    "            # .squeeze() -> Returns a tensor with all the dimensions of input of size 1 removed.\n",
    "            print(f'hidden shape before squeeze {hidden.shape}')\n",
    "#             hidden = hidden.squeeze()   # doesn't work? gives wrong dimension down stream...\n",
    "            pass\n",
    "            print(f'hidden shape after squeeze {hidden.shape}')\n",
    "\n",
    "        \n",
    "        # Reparameterisation trick!\n",
    "        z, mean, logv, std = self.reparameterise(hidden, batch_size)\n",
    "        \n",
    "        # DECODER\n",
    "        if 0 < self.word_dropout_rate:\n",
    "            prob = torch.rand(input_sequence.size())\n",
    "\n",
    "            if torch.cuda.is_available():\n",
    "                prob = prob.cuda()\n",
    "\n",
    "            prob[(input_sequence.data - self.sos_idx) * (input_sequence.data - self.pad_idx) == 0] = 1\n",
    "\n",
    "            decoder_input_sequence = input_sequence.clone()\n",
    "            \n",
    "#             print(vocab_size)\n",
    "#             print(self.unk_idx)\n",
    "            decoder_input_sequence[prob < self.word_dropout_rate] = self.unk_idx\n",
    "\n",
    "#             print(decoder_input_sequence)\n",
    "            input_embedding = self.embedding(decoder_input_sequence)\n",
    "\n",
    "        input_embedding = self.embedding_dropout(input_embedding)\n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embedding, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        \n",
    "        outputs, _ = self._decode(packed_input, hidden)\n",
    "        \n",
    "        # process outputs\n",
    "        # Process outputs\n",
    "        # Unpack padded sequence\n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(outputs, batch_first=True)[0]\n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        _, reversed_idx = torch.sort(sorted_idx)\n",
    "        padded_outputs = padded_outputs[reversed_idx]\n",
    "        b, s, _ = padded_outputs.size()\n",
    "\n",
    "        # Project outputs to vocab\n",
    "        # e.g. project hidden state into label space...\n",
    "        logp = nn.functional.log_softmax(self.outputs2vocab(padded_outputs.view(-1, padded_outputs.size(2))), dim=-1)\n",
    "#         print(f'logp before view {logp.shape}\\n')\n",
    "#         print(f'b {b} s {s} no emb {self.embedding.num_embeddings}')\n",
    "        logp = logp.view(b, s, self.embedding.num_embeddings)\n",
    "\n",
    "        # logp - log posterior over label space; mean - tensor Gaussian mean, logv - tensor Gaussian variance, z - VAE latent space \n",
    "        return logp, mean, logv, z\n",
    "    \n",
    "    def to_var(self, x):\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.cuda()\n",
    "        return x\n",
    "    \n",
    "    def kl_anneal_function(self, anneal_function, step, k, x0):\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        if anneal_function == 'logistic':\n",
    "            return float(1/(1+np.exp(-k*(step-x0))))\n",
    "        elif anneal_function == 'linear':\n",
    "            return min(1, step/x0)\n",
    "        \n",
    "    def loss_fn(self, logp, target, mean, logv, anneal_function, step, k, x0):\n",
    "        \"\"\"\n",
    "        SVAE loss function\n",
    "        \n",
    "        NLL - Negative Log Likelihood loss between predicted tags and ground truth\n",
    "        KL_Loss - Evidence Lower Bound (ELBO)? - TODO: validate interpretation\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        # insert assertion\n",
    "        \n",
    "        # ALL THE CODE BELOW NEEDS REVIEW and piece wise implemented\n",
    "        # Cut-off unnecessary padding from target and flatten\n",
    "        target = target[:, :torch.max(length).item()].contiguous().view(-1)\n",
    "        logp = logp.view(-1, logp.size(2))\n",
    "        \n",
    "        # Negative log likelihood\n",
    "        NLL_loss = self.NLL(logp, target)\n",
    "        \n",
    "        # KL Divergence (or ELBO? TODO: determine which)\n",
    "        KL_loss = -0.5 * torch.sum(1 + logv - mean.pow(2) - logv.exp())\n",
    "        KL_weight = self.kl_anneal_function(anneal_function, step, k, x0)\n",
    "        \n",
    "        return NLL_loss, KL_loss, KL_weight\n",
    "    \n",
    "    def reparameterise(self, hidden, batch_size):\n",
    "        \"\"\" Implement reparameterisation trick (Kingma and Welling 2014) \"\"\"\n",
    "        \n",
    "        mean = self.hidden2mean(hidden)\n",
    "        logv = self.hidden2logv(hidden)\n",
    "        std = torch.exp(0.5 * logv) \n",
    "        \n",
    "        z = self.to_var(torch.randn([batch_size, self.z_dim]))\n",
    "        return z * std + mean, mean, logv, std\n",
    "    \n",
    "    def _encode(self, x):\n",
    "        \"\"\" x - pack padded sequence \"\"\"\n",
    "        return self.encoder_rnn(x)\n",
    "    \n",
    "    def _decode(self, x, hidden):\n",
    "        \"\"\" x - pack padded sequence\n",
    "            hidden - latent tensor\"\"\"\n",
    "        return self.decoder_rnn(x, hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "vocab_size = 100\n",
    "hidden_size = 128\n",
    "svae = SVAE(vocab_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using label space size of: 5\n",
      "Shapes - seq torch.Size([10, 40]) - lengths torch.Size([10])\n",
      "hidden shape before squeeze torch.Size([1, 10, 128])\n",
      "hidden shape after squeeze torch.Size([1, 10, 128])\n",
      "(tensor([[[-4.5983, -4.4916, -4.3991,  ..., -4.5816, -4.7353, -4.3919],\n",
      "         [-4.6237, -4.5505, -4.5878,  ..., -4.7848, -4.7002, -4.6800],\n",
      "         [-4.5375, -4.6115, -4.8641,  ..., -5.0021, -5.1341, -4.5905],\n",
      "         ...,\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759],\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759],\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759]],\n",
      "\n",
      "        [[-4.4114, -4.4459, -4.8041,  ..., -4.6162, -4.9344, -4.9172],\n",
      "         [-4.5462, -4.7396, -4.5475,  ..., -4.7223, -4.8348, -4.5941],\n",
      "         [-4.5065, -4.7213, -4.5215,  ..., -4.5295, -5.0201, -4.8757],\n",
      "         ...,\n",
      "         [-4.9214, -4.6634, -4.6963,  ..., -4.5376, -4.7926, -4.6830],\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759],\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759]],\n",
      "\n",
      "        [[-4.6202, -4.5794, -4.6338,  ..., -4.1072, -4.8159, -4.6494],\n",
      "         [-4.5309, -4.6225, -4.9548,  ..., -4.0814, -5.0472, -4.4299],\n",
      "         [-4.4253, -4.8249, -4.6238,  ..., -4.3125, -4.6824, -4.7736],\n",
      "         ...,\n",
      "         [-4.5398, -4.4799, -4.5119,  ..., -4.6019, -4.9236, -4.3926],\n",
      "         [-4.7975, -4.4283, -4.7193,  ..., -4.6480, -4.9605, -4.4394],\n",
      "         [-4.9442, -4.8566, -4.7588,  ..., -4.7418, -4.9551, -4.4126]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-4.6071, -4.5007, -4.8564,  ..., -4.5308, -4.4109, -4.5325],\n",
      "         [-4.8637, -4.4559, -4.8349,  ..., -4.4287, -4.9786, -4.4372],\n",
      "         [-4.6224, -4.2184, -4.5607,  ..., -4.6640, -4.9161, -4.4551],\n",
      "         ...,\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759],\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759],\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759]],\n",
      "\n",
      "        [[-4.8766, -4.6369, -4.6658,  ..., -4.2958, -4.3411, -4.8329],\n",
      "         [-4.6874, -4.5896, -4.9357,  ..., -4.1995, -4.6253, -4.7732],\n",
      "         [-4.4907, -4.6953, -4.8896,  ..., -4.3360, -4.5455, -4.9160],\n",
      "         ...,\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759],\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759],\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759]],\n",
      "\n",
      "        [[-4.4275, -4.3475, -4.7033,  ..., -4.6174, -4.9553, -4.4822],\n",
      "         [-4.5189, -4.4096, -4.8002,  ..., -4.4311, -5.0115, -4.3029],\n",
      "         [-4.6371, -4.4030, -4.9146,  ..., -4.8240, -4.8690, -4.6369],\n",
      "         ...,\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759],\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759],\n",
      "         [-4.6752, -4.6746, -4.5652,  ..., -4.5551, -4.6831, -4.5759]]],\n",
      "       grad_fn=<ViewBackward>), tensor([[[-0.3388,  0.0955, -0.1645,  0.0474,  0.0407,  0.1573, -0.1478,\n",
      "          -0.0282],\n",
      "         [ 0.2257,  0.0974,  0.0925,  0.0807,  0.1718, -0.4486,  0.0415,\n",
      "           0.1480],\n",
      "         [ 0.0607,  0.2441,  0.2053, -0.1666, -0.2098, -0.0308, -0.1122,\n",
      "           0.0257],\n",
      "         [ 0.2445, -0.0031,  0.0028,  0.0822, -0.0173,  0.0747, -0.0047,\n",
      "           0.1862],\n",
      "         [ 0.0547, -0.0649, -0.0797,  0.0186,  0.2020, -0.0978,  0.0787,\n",
      "           0.3198],\n",
      "         [-0.1017,  0.0678,  0.1029, -0.0313,  0.1583,  0.1732, -0.0008,\n",
      "           0.1301],\n",
      "         [-0.0577, -0.1040, -0.1546, -0.1510, -0.0608,  0.2370,  0.0096,\n",
      "          -0.1509],\n",
      "         [-0.1354,  0.3692, -0.0625, -0.1893,  0.1442,  0.1208, -0.0406,\n",
      "           0.1653],\n",
      "         [ 0.1696,  0.0934, -0.1009,  0.1412,  0.0821, -0.0213, -0.1258,\n",
      "           0.2239],\n",
      "         [ 0.0221, -0.0584, -0.0944,  0.1199,  0.2624, -0.0345,  0.1302,\n",
      "           0.2105]]], grad_fn=<AddBackward0>), tensor([[[-0.3286,  0.0304,  0.1752,  0.2928,  0.0464, -0.0641, -0.0819,\n",
      "           0.2076],\n",
      "         [-0.3144,  0.0449,  0.0272, -0.0601,  0.2740, -0.0639,  0.0129,\n",
      "           0.3366],\n",
      "         [ 0.0698, -0.1672,  0.0933,  0.2209,  0.2449, -0.1179, -0.1507,\n",
      "           0.1331],\n",
      "         [ 0.0143,  0.0736,  0.0657,  0.0395,  0.0448,  0.0271, -0.0033,\n",
      "          -0.1151],\n",
      "         [-0.1423, -0.2085,  0.0447, -0.2041, -0.1534, -0.1517,  0.0793,\n",
      "           0.1424],\n",
      "         [-0.2903, -0.0913, -0.5442,  0.2073, -0.0217, -0.2236,  0.0548,\n",
      "           0.0229],\n",
      "         [ 0.0195, -0.1732,  0.0589,  0.1183,  0.0615,  0.0232, -0.0684,\n",
      "           0.1593],\n",
      "         [-0.0635, -0.3105, -0.0358, -0.0541, -0.0704, -0.3196,  0.0450,\n",
      "           0.1080],\n",
      "         [-0.1066, -0.2438,  0.0890,  0.0984,  0.1765, -0.0969,  0.1590,\n",
      "           0.1170],\n",
      "         [-0.1382, -0.0526,  0.1657, -0.0167, -0.1487, -0.0249,  0.0034,\n",
      "           0.1814]]], grad_fn=<AddBackward0>), tensor([[[-1.5391,  0.2579, -3.0791, -1.8105,  0.3104, -0.3138,  0.3263,\n",
      "           0.0066],\n",
      "         [ 1.9534,  0.1524,  1.9207, -0.2655,  0.5060, -2.1501, -0.1777,\n",
      "           0.5286],\n",
      "         [-1.4049,  0.9131,  2.0310, -1.1577,  0.5428, -0.0283,  0.4430,\n",
      "           0.6628],\n",
      "         [ 0.4142,  1.3872,  0.4238,  0.0284,  0.0186, -0.6836, -0.1329,\n",
      "           1.9843],\n",
      "         [ 0.2105,  0.1954,  0.4705, -0.6880,  0.1395,  0.2652,  1.4145,\n",
      "          -0.1581],\n",
      "         [ 1.0068, -1.3363,  0.3766,  0.7445, -0.4106,  0.0125,  1.4853,\n",
      "          -0.3602],\n",
      "         [ 0.1894, -0.8632, -0.7864,  0.8464,  0.8453,  1.6983, -0.9977,\n",
      "          -0.7729],\n",
      "         [ 0.1811,  0.3140,  0.5792,  0.1611,  1.0139, -1.0288,  0.0833,\n",
      "           0.6739],\n",
      "         [-0.6075,  1.4097,  0.1625, -2.3237, -0.8551,  1.4040, -0.5175,\n",
      "           2.0917],\n",
      "         [ 0.8285, -0.0336, -0.1681,  0.0113,  1.2251,  0.9827,  0.5292,\n",
      "           0.5196]]], grad_fn=<AddBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Test forward pass of SVAE\n",
    "svae_ff_seqs, svae_ff_lengths = Tester().build_sequences(batch_size=10, max_seq_len=40)\n",
    "\n",
    "# Pass sequences and lengths into SVAE forward method\n",
    "print(svae.forward(svae_ff_seqs, svae_ff_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: GRU(128, 128, batch_first=True)\n",
      "Decoder: GRU(128, 128, batch_first=True)\n",
      "hidden2mean: Linear(in_features=128, out_features=8, bias=True)\n",
      "hidden2logv: Linear(in_features=128, out_features=8, bias=True)\n",
      "z2hidden: Linear(in_features=8, out_features=128, bias=True)\n",
      "outputs2vocab: Linear(in_features=128, out_features=104, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# Reviewing components\n",
    "print(f'Encoder: {svae.encoder_rnn}')\n",
    "print(f'Decoder: {svae.decoder_rnn}')\n",
    "print(f'hidden2mean: {svae.hidden2mean}')\n",
    "print(f'hidden2logv: {svae.hidden2logv}')\n",
    "print(f'z2hidden: {svae.z2hidden}')\n",
    "print(f'outputs2vocab: {svae.outputs2vocab}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Learner\n",
    "Using PyTorch tutorial implementation (https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html) for rapid development (will implement SoTA in the future)<br><br>\n",
    "To do:\n",
    " - [ ] Make batch based to suit other models rather than training on singletons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskLearner(nn.Module):\n",
    "    \"\"\" Task learner for NER \"\"\"\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(TaskLearner, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)    # expects input (batch, seq, feature)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, input_sequences, lengths):\n",
    "        \"\"\"\n",
    "        Pack padded sequence and sorted lengths used to optimise forward pass through RNN\n",
    "        \n",
    "        \n",
    "        input_sequence : tensor\n",
    "            (batch, seq, feature)\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = input_sequences.size(0)\n",
    "        sorted_lengths, sorted_idx = torch.sort(lengths, descending=True)\n",
    "        input_sequences = input_sequences[sorted_idx]\n",
    "        input_embeddings = self.word_embeddings(input_sequences)\n",
    "        \n",
    "        packed_input = rnn_utils.pack_padded_sequence(input_embeddings, sorted_lengths.data.tolist(), batch_first=True)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(packed_input)\n",
    "        \n",
    "        # Unpack padded sequence\n",
    "        padded_outputs = rnn_utils.pad_packed_sequence(lstm_out, batch_first=True)[0]\n",
    "        padded_outputs = padded_outputs.contiguous()\n",
    "        _, reversed_idx = torch.sort(sorted_idx)\n",
    "        padded_outputs = padded_outputs[reversed_idx]\n",
    "        b, s, _ = padded_outputs.size()\n",
    "        \n",
    "        # project into label space\n",
    "        tag_space = self.hidden2tag(padded_outputs.view(-1, padded_outputs.size(2)))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes - seq torch.Size([32, 10]) - lengths torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "# Testing functionality\n",
    "tasklearner = TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=104, tagset_size=4)\n",
    "# Generating data for testing\n",
    "seqs, lens = tester.build_sequences(batch_size=32, max_seq_len=10)   # Tester initialised at start of notebook\n",
    "dataset = tester.build_sequence_tags(sequences=seqs, lengths=lens)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Testing forward pass functionality\n",
    "for X, lens, y in dataset:\n",
    "#     print(X, lens, y)\n",
    "    forward_out = tasklearner.forward(input_sequences=X, lengths=lens)\n",
    "    print(f'Forward pass output shape: {forward_out.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 23, 24, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 45, 46, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 71, 72, 73, 74, 75, 76, 77, 80, 81, 82, 83, 84, 85, 87, 88, 89, 90, 91, 93, 94, 96, 97, 99]\n"
     ]
    }
   ],
   "source": [
    "# Generate vocab off of generated sequences\n",
    "# TODO: Add to utility functions / tester class\n",
    "vocab = list()\n",
    "for seq in seqs:\n",
    "      vocab.extend(seq.tolist())\n",
    "vocab = list(set(vocab))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training routine\n",
    "# vocab_size is max int in vocab ints + 1 as 0 is included...\n",
    "# tagset size is wrong...\n",
    "model = TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=max(vocab)+1, tagset_size=6)   # tagset size = special characters + number of class types (0 to n_class - 1)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with torch.no_grad():\n",
    "    print(seqs.shape)\n",
    "    # forward pass to get output scores\n",
    "    tag_scores = model(seqs, lens)\n",
    "    print(tag_scores.shape)\n",
    "#     print(tag_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "Shapes | Seq: torch.Size([32, 10]) Lengths: torch.Size([32]) Tags: torch.Size([32, 10])\n",
      "NLL Loss: 1.80\n",
      "\n",
      "Epoch: 1\n",
      "Shapes | Seq: torch.Size([32, 10]) Lengths: torch.Size([32]) Tags: torch.Size([32, 10])\n",
      "NLL Loss: 1.78\n",
      "\n",
      "Epoch: 2\n",
      "Shapes | Seq: torch.Size([32, 10]) Lengths: torch.Size([32]) Tags: torch.Size([32, 10])\n",
      "NLL Loss: 1.77\n",
      "\n",
      "Epoch: 3\n",
      "Shapes | Seq: torch.Size([32, 10]) Lengths: torch.Size([32]) Tags: torch.Size([32, 10])\n",
      "NLL Loss: 1.76\n",
      "\n",
      "Epoch: 4\n",
      "Shapes | Seq: torch.Size([32, 10]) Lengths: torch.Size([32]) Tags: torch.Size([32, 10])\n",
      "NLL Loss: 1.75\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "    for batch_seqs, batch_lens, batch_tags in dataset:\n",
    "        print(f'\\nEpoch: {epoch}')\n",
    "        print(f'Shapes | Seq: {batch_seqs.shape} Lengths: {batch_lens.shape} Tags: {batch_tags.shape}')\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Get max length of longest sequence in batch so it can be used to filter tags\n",
    "        sorted_lengths, sorted_idx = torch.sort(batch_lens, descending=True)   # longest seq at index 0\n",
    "        longest_seq = sorted_lengths[0].data.numpy()\n",
    "        longest_seq_len = longest_seq[longest_seq != 0][0]   # remove padding (TODO: change to pad_idx in the future)\n",
    "        \n",
    "        # Get predictions from model\n",
    "        tag_scores = model(batch_seqs, batch_lens)\n",
    "        \n",
    "        # Strip off as much padding as possible similar to (variable length sequences via pack padded methods)\n",
    "        batch_tags = torch.stack([tags[:longest_seq_len] for tags in batch_tags])\n",
    "        batch_tags = batch_tags.view(-1)\n",
    "        \n",
    "        # Calculate loss and backpropigate error through model\n",
    "        loss = loss_function(tag_scores, batch_tags)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f'NLL Loss: {loss.data.detach():0.2f}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# need to fix to be batch based rather than singleton\n",
    "with torch.no_grad():\n",
    "    batch_seqs, batch_lens, _ = dataset[0]\n",
    "    tag_scores = model(batch_seqs, batch_lens)\n",
    "\n",
    "    print(tag_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discriminator\n",
    "To do:\n",
    " - [ ] Implement..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" Adversary architecture for discriminator module \n",
    "    \n",
    "    Tensor needs to be FloatTensor; sequences and tag sets are LongTensor\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, z_dim=8):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.z_dim = z_dim    # latent space dimension (will be the same shape as the encoder output from VAE)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "                                nn.Linear(z_dim, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128, 128),\n",
    "                                nn.ReLU(True),\n",
    "                                nn.Linear(128,1),\n",
    "                                nn.Sigmoid()\n",
    "                                )\n",
    "        # Exe\n",
    "        self.weight_init()\n",
    "        \n",
    "    def weight_init(self):\n",
    "        \"\"\" Weight initialisation\n",
    "        \n",
    "        Using Xavier uniform initialisation rather than Kaiming (I think that is more focused for CV? TODO: investigate)\n",
    "        See: https://pytorch.org/cppdocs/api/function_namespacetorch_1_1nn_1_1init_1ace282f75916a862c9678343dfd4d5ffe.html\n",
    "        \"\"\"\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                if type(m) == nn.Linear:\n",
    "                    torch.nn.init.xavier_uniform_(m.weight)\n",
    "                    m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\" Forward pass through discriminator\n",
    "        \n",
    "        Arguments\n",
    "        --------\n",
    "            z : tensor\n",
    "                Tensor derived from SVAE latent space\n",
    "        \"\"\"\n",
    "        return self.net(z.type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: tensor([17, 20,  1, 26, 65, 53,  2, 76])\n",
      "Output: tensor([5.9446e-11], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Testing functionality\n",
    "z_dim = 8\n",
    "discriminator = Discriminator(z_dim=z_dim)\n",
    "\n",
    "# Pass random integer tensor through forward pass of discriminator\n",
    "rand_tensor = torch.randint(low=0, high=100,size=(z_dim,))\n",
    "print(f'Input: {rand_tensor}')\n",
    "print(f'Output: {discriminator.forward(rand_tensor)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(23.5460, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Testing training functionality for discriminator\n",
    "dsc_bce_loss = nn.BCELoss()\n",
    "dsc_optim = optim.Adam(discriminator.parameters(), lr=0.001)\n",
    "\n",
    "discriminator.train()\n",
    "\n",
    "preds = discriminator(rand_tensor)\n",
    "real_labels = torch.ones(preds.size(0))\n",
    "loss = dsc_bce_loss(preds, real_labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler Routine\n",
    "Active learning based sample selection for task learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code copied from VAAL and modified for sequence data\n",
    "class Sampler:\n",
    "    \"\"\" Adversary sampler \"\"\"\n",
    "    def __init__(self, budget):\n",
    "        self.budget = budget\n",
    "        \n",
    "    def sample(self, vae, discriminator, data, cuda):\n",
    "        \"\"\" Selective sampling algorithm\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "            vae : torch model\n",
    "                VAE model\n",
    "            discriminator : torch model\n",
    "                discriminator model\n",
    "            data : tensor\n",
    "                Image data\n",
    "            cuda : boolean\n",
    "                GPU flag\n",
    "        Returns\n",
    "        -------\n",
    "            querry_pool_indices: int, list\n",
    "                List of indices corresponding to sorted (top-K) samples to be sampled from\n",
    "        \"\"\"\n",
    "        all_preds = []\n",
    "        all_indices = []\n",
    "\n",
    "        for images, _, indices in data:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, _, mu, _ = vae(images)\n",
    "                preds = discriminator(mu)\n",
    "\n",
    "            preds = preds.cpu().data\n",
    "            all_preds.extend(preds)\n",
    "            all_indices.extend(indices)\n",
    "\n",
    "        all_preds = torch.stack(all_preds)\n",
    "        all_preds = all_preds.view(-1)\n",
    "        # need to multiply by -1 to be able to use torch.topk \n",
    "        all_preds *= -1\n",
    "\n",
    "        # select the points which the discriminator things are the most likely to be unlabeled\n",
    "        _, querry_indices = torch.topk(all_preds, int(self.budget))\n",
    "        querry_pool_indices = np.asarray(all_indices)[querry_indices]\n",
    "\n",
    "        return querry_pool_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing functionality\n",
    "sampler = Sampler(budget=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Routine\n",
    "<i> Pseudo code</i>\n",
    "\n",
    "```python\n",
    "for epoch in max_epochs:\n",
    "        train(task learner)\n",
    "            get preds\n",
    "            calc loss\n",
    "            zero grads\n",
    "            backpropigate loss\n",
    "            update model parameters\n",
    "        for step in max_steps:\n",
    "            train(SVAE)\n",
    "        for step in max_steps:\n",
    "            train(discriminator)\n",
    "```\n",
    "To do:\n",
    " - [ ] Implement training cycle for task learn\n",
    " - [ ] Implement training cycle for VAE\n",
    " - [ ] Implement training cycle for discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Solver(Tester):\n",
    "    def __init__(self, task_learner, vae, discriminator):\n",
    "        Tester.__init__(self)   # need tester class to access properties such as label_set_size\n",
    "        \n",
    "        # params (TODO: move to config)\n",
    "        self.epochs = 2\n",
    "        self.svae_steps = 2\n",
    "        self.discriminator_steps = 2 \n",
    "        \n",
    "        # Get models\n",
    "        # init else where?\n",
    "        self.task_learner =  TaskLearner(embedding_dim=128, hidden_dim=128, vocab_size=max(vocab)+1, tagset_size=self.label_space_size)\n",
    "        self.svae = SVAE(vocab_size=max(vocab)+1, embedding_size=128)\n",
    "        self.discriminator = Discriminator(z_dim=10)   # z_dim will be the latent output of the vae encoder... currently just the seq length\n",
    "        \n",
    "        # Loss functions\n",
    "        self.nll_loss = nn.NLLLoss()   # used in: TL\n",
    "        self.bce_loss = nn.BCELoss()   # used in: Discriminator\n",
    "        self.xxx_loss = 0    # used in: SVAE\n",
    "\n",
    "        # optimiser\n",
    "        self.optim_lr = 0.1   # don't want a global lr...\n",
    "        self.tl_optim = optim.SGD(model.parameters(), lr=self.optim_lr)   # todo: revisit learning rate\n",
    "        self.svae_optim = None\n",
    "        self.disc_optim = optim.Adam(self.discriminator.parameters(), lr=0.01)   # todo: revisit learning rate\n",
    "        \n",
    "        # Will need to implement this in the future, need to build generator function rather than rely on Torch implementationss?\n",
    "        self.data_loader = 'generator'\n",
    "        self.seqs, self.lens = tester.build_sequences(batch_size=2, max_seq_len=10)   # Tester initialised at start of notebook\n",
    "        self.dataset = tester.build_sequence_tags(sequences=self.seqs, lengths=self.lens)\n",
    "        \n",
    "        # Will need to implement this in the future, inherit the Sampler class?\n",
    "        self.sampler = 'sampler'\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\" Performs model training \"\"\"\n",
    "        \n",
    "        # turn on .train() mode\n",
    "        self.svae.train()\n",
    "        self.task_learner.train()\n",
    "        self.discriminator.train()\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "#             print(f'\\nEpoch: {epoch}')\n",
    "            \n",
    "            # these will be batches in the future, not single sequence/tag sets...\n",
    "            # build generator to wrap around sequence/tag sets and then just call next() on them at each epoch to get a batch...\n",
    "            # need to build that in the future to partition based on the unlabelled/labelled splits for AL experiments\n",
    "            for batch_seqs, batch_lens, batch_tags in self.dataset:\n",
    "                print(f'Dataset shapes\\nSeqs:{batch_seqs.shape}\\tLens:{batch_lens.shape}\\tTags:{batch_tags.shape}')\n",
    "            \n",
    "                # Train Task Learner\n",
    "                self.tl_train_step(batch_seqs, batch_lens, batch_tags)\n",
    "            \n",
    "                # Train VAE\n",
    "                for step in range(self.svae_steps):\n",
    "                    print(f'SVAE Step: {step}')\n",
    "                    self.svae_train_step(batch_seqs, batch_lens, batch_tags)\n",
    "\n",
    "                # Train Discriminator\n",
    "                for step in range(self.discriminator_steps):\n",
    "                    print(f'Discriminator Step: {step}')\n",
    "\n",
    "                    self.disc_train_step(batch_seqs)\n",
    "            clear_output(wait=True)\n",
    "    \n",
    "    def trim_padded_tags(self, batch_lengths: Tensor, batch_tags: Tensor) -> Tensor:\n",
    "        \"\"\" Takes a batch of sequences and tags and trims similar to pack padded sequence method \"\"\"\n",
    "        \n",
    "        # Get max length of longest sequence in batch so it can be used to filter tags\n",
    "        sorted_lengths, _ = torch.sort(batch_lengths, descending=True)   # longest seq at index 0\n",
    "        longest_seq = sorted_lengths[0].data.numpy()\n",
    "        longest_seq_len = longest_seq[longest_seq != self.pad_idx][0]   # remove padding\n",
    "        \n",
    "        # Strip off as much padding as possible similar to (variable length sequences via pack padded methods)\n",
    "        batch_tags = torch.stack([tags[:longest_seq_len] for tags in batch_tags])\n",
    "    \n",
    "        return batch_tags\n",
    "    \n",
    "    def tl_train_step(self, batch_seqs, batch_lengths, batch_tags):\n",
    "        \"\"\"\n",
    "        Perform training step of task learner \n",
    "        \n",
    "        Tensor dtype is LongTensor\n",
    "        \"\"\"\n",
    "#         print(seq.dtype)\n",
    "#         assert seq.dtype == 'torch.int64'\n",
    "#         assert tags.dtype == 'torch.int64'\n",
    "        \n",
    "        # zero grad as they accumulate in PyTorch\n",
    "        self.task_learner.zero_grad()\n",
    "        \n",
    "        preds = self.task_learner(batch_seqs, batch_lengths)\n",
    "        \n",
    "        batch_tags = self.trim_padded_tags(batch_lengths, batch_tags).view(-1)\n",
    "        \n",
    "        print(batch_seqs, batch_lengths, batch_tags)\n",
    "        \n",
    "        tl_loss = self.nll_loss(preds, batch_tags)\n",
    "        tl_loss.backward()\n",
    "        self.tl_optim.step()\n",
    "        \n",
    "        print(f'Task learning loss: {loss.data:0.4f}')\n",
    "    \n",
    "    def svae_train_step(self, batch_seqs, batch_lengths, batch_tags):\n",
    "        \"\"\"\n",
    "        Perform training step of sentence variational autoencoder\n",
    "        \n",
    "        Tensor type is LongTensor\n",
    "        \"\"\"\n",
    "        # Forward pass\n",
    "        logp, mean, logv, z = self.svae(batch_seqs, batch_lengths)\n",
    "        \n",
    "        # Loss calculation\n",
    "#         NLL_loss, KL_loss, KL_weight = loss_fn(logp, batch_tags, batch_lengths, mean, logv, anneal_function, step, k, x0)\n",
    "\n",
    "        print(f'logp: {logv} mean: {mean} logv: {logv} z: {z}')\n",
    "        \n",
    "    \n",
    "    def disc_train_step(self, seqs):\n",
    "        \"\"\" Perform training step of adversarial discriminatory \n",
    "        \n",
    "        Note: The loss function will be an aggregate across labelled and unlabelled samples (currently just labelled)\n",
    "        \n",
    "        Tensor dtype is FloatTensor for discriminator\n",
    "        \"\"\"\n",
    "        \n",
    "#         assert seq.dtype == 'torch.float64'\n",
    "        \n",
    "#         print(seq)\n",
    "#         print(seq.dtype)\n",
    "        \n",
    "        preds = self.discriminator(seqs)\n",
    "#         print(preds)\n",
    "        real_labels = torch.ones(preds.size(0))\n",
    "        disc_loss = self.bce_loss(preds, real_labels)\n",
    "        disc_loss.backward()\n",
    "        self.disc_optim.step()\n",
    "        \n",
    "        print(f'Discriminator loss: {loss.data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes\n",
      "Seqs:torch.Size([2, 10])\tLens:torch.Size([2])\tTags:torch.Size([2, 10])\n",
      "tensor([[14, 27, 42, 49, 16, 58, 86,  0,  0,  0],\n",
      "        [ 3, 86, 34,  5, 56, 26, 98, 49,  0,  0]]) tensor([7, 8]) tensor([3, 4, 3, 3, 3, 3, 3, 0, 3, 2, 3, 1, 3, 3, 2, 2])\n",
      "Task learning loss: 23.5460\n",
      "SVAE Step: 0\n",
      "hidden shape before squeeze torch.Size([1, 2, 128])\n",
      "hidden shape after squeeze torch.Size([1, 2, 128])\n",
      "logp: tensor([[[-0.0424,  0.0393,  0.1499, -0.2662,  0.0157, -0.0516, -0.1770,\n",
      "           0.1174],\n",
      "         [ 0.0196,  0.2950, -0.0080, -0.0985, -0.0549, -0.0729,  0.0613,\n",
      "          -0.0513]]], grad_fn=<AddBackward0>) mean: tensor([[[-0.2073, -0.0854, -0.0096,  0.0317,  0.0788,  0.0017,  0.0800,\n",
      "           0.1587],\n",
      "         [-0.1718,  0.0958, -0.2266, -0.1947, -0.1373,  0.0193,  0.0474,\n",
      "          -0.1937]]], grad_fn=<AddBackward0>) logv: tensor([[[-0.0424,  0.0393,  0.1499, -0.2662,  0.0157, -0.0516, -0.1770,\n",
      "           0.1174],\n",
      "         [ 0.0196,  0.2950, -0.0080, -0.0985, -0.0549, -0.0729,  0.0613,\n",
      "          -0.0513]]], grad_fn=<AddBackward0>) z: tensor([[[-1.4692, -1.9779,  1.2395,  1.2086,  1.0398,  1.4670,  0.2686,\n",
      "           0.4746],\n",
      "         [-0.5197, -0.3490, -1.0091, -2.1199,  0.8921, -0.0317, -1.0449,\n",
      "           0.0936]]], grad_fn=<AddBackward0>)\n",
      "SVAE Step: 1\n",
      "hidden shape before squeeze torch.Size([1, 2, 128])\n",
      "hidden shape after squeeze torch.Size([1, 2, 128])\n",
      "logp: tensor([[[-0.0424,  0.0393,  0.1499, -0.2662,  0.0157, -0.0516, -0.1770,\n",
      "           0.1174],\n",
      "         [ 0.0196,  0.2950, -0.0080, -0.0985, -0.0549, -0.0729,  0.0613,\n",
      "          -0.0513]]], grad_fn=<AddBackward0>) mean: tensor([[[-0.2073, -0.0854, -0.0096,  0.0317,  0.0788,  0.0017,  0.0800,\n",
      "           0.1587],\n",
      "         [-0.1718,  0.0958, -0.2266, -0.1947, -0.1373,  0.0193,  0.0474,\n",
      "          -0.1937]]], grad_fn=<AddBackward0>) logv: tensor([[[-0.0424,  0.0393,  0.1499, -0.2662,  0.0157, -0.0516, -0.1770,\n",
      "           0.1174],\n",
      "         [ 0.0196,  0.2950, -0.0080, -0.0985, -0.0549, -0.0729,  0.0613,\n",
      "          -0.0513]]], grad_fn=<AddBackward0>) z: tensor([[[-0.4290, -0.3309, -1.3658,  0.3089,  0.3204, -1.3438, -1.4169,\n",
      "          -1.3906],\n",
      "         [-0.8327,  1.7274, -0.8789,  1.1846,  0.6415,  1.8222, -1.2376,\n",
      "           0.9580]]], grad_fn=<AddBackward0>)\n",
      "Discriminator Step: 0\n",
      "Discriminator loss: 23.54595184326172\n",
      "Discriminator Step: 1\n",
      "Discriminator loss: 23.54595184326172\n"
     ]
    }
   ],
   "source": [
    "# Test training \n",
    "slvr = Solver(task_learner=None, vae=None, discriminator=None)\n",
    "slvr.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Routine\n",
    "To do:\n",
    " - [ ] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling Routine\n",
    "To do:\n",
    " - [ ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
