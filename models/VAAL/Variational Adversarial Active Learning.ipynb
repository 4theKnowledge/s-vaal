{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook implementation of Variational Adversarial Active Learning\n",
    "Implementation: https://github.com/sinhasam/vaal <br>\n",
    "Paper: https://arxiv.org/abs/1904.00370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments (arguments.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--cuda', action='store_true', help='If training is to be done on a GPU')\n",
    "    parser.add_argument('--dataset', type=str, default='cifar10', help='Name of the dataset used.')\n",
    "    parser.add_argument('--batch_size', type=int, default=128, help='Batch size used for training and testing')\n",
    "    parser.add_argument('--train_epochs', type=int, default=100, help='Number of training epochs')\n",
    "    parser.add_argument('--latent_dim', type=int, default=32, help='The dimensionality of the VAE latent dimension')\n",
    "    parser.add_argument('--data_path', type=str, default='./data', help='Path to where the data is')\n",
    "    parser.add_argument('--beta', type=float, default=1, help='Hyperparameter for training. The parameter for VAE')\n",
    "    parser.add_argument('--num_adv_steps', type=int, default=1, help='Number of adversary steps taken for every task model step')\n",
    "    parser.add_argument('--num_vae_steps', type=int, default=2, help='Number of VAE steps taken for every task model step')\n",
    "    parser.add_argument('--adversary_param', type=float, default=1, help='Hyperparameter for training. lambda2 in the paper')\n",
    "    parser.add_argument('--out_path', type=str, default='./results', help='Path to where the output log will be')\n",
    "    parser.add_argument('--log_name', type=str, default='accuracies.log', help='Final performance of the models will be saved with this name')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if not os.path.exists(args.out_path):\n",
    "        os.mkdir(args.out_path)\n",
    "    \n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dataset (custom_dataset.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import numpy\n",
    "\n",
    "from utils import *\n",
    "\n",
    "def imagenet_transformer():\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def cifar10_transformer():\n",
    "    return torchvision.transforms.Compose([\n",
    "           torchvision.transforms.RandomHorizontalFlip(),\n",
    "           torchvision.transforms.ToTensor(),\n",
    "           transforms.Normalize(mean=[0.5, 0.5, 0.5,],\n",
    "                                std=[0.5, 0.5, 0.5]),\n",
    "       ])\n",
    "\n",
    "class CIFAR10(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.cifar10 = datasets.CIFAR10(root=path,\n",
    "                                        download=True,\n",
    "                                        train=True,\n",
    "                                        transform=cifar10_transformer())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, numpy.float64):\n",
    "            index = index.astype(numpy.int64)\n",
    "\n",
    "        data, target = self.cifar10[index]\n",
    "\n",
    "        return data, target, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifar10)\n",
    "\n",
    "\n",
    "class CIFAR100(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.cifar100 = datasets.CIFAR100(root=path,\n",
    "                                        download=True,\n",
    "                                        train=True,\n",
    "                                        transform=cifar10_transformer())\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, numpy.float64):\n",
    "            index = index.astype(numpy.int64)\n",
    "\n",
    "        data, target = self.cifar100[index]\n",
    "\n",
    "        # Your transformations here (or set it in CIFAR10)\n",
    "\n",
    "        return data, target, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifar100)\n",
    "\n",
    "\n",
    "class ImageNet(Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.imagenet = datasets.ImageFolder(root=path, transform=imagenet_transformer)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if isinstance(index, numpy.float64):\n",
    "            index = index.astype(numpy.int64)\n",
    "        data, target = self.imagenet[index]\n",
    "\n",
    "        return data, target, index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagenet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main (main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data.sampler  as sampler\n",
    "import torch.utils.data as data\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "\n",
    "from custom_datasets import *\n",
    "import model\n",
    "import vgg\n",
    "from solver import Solver\n",
    "from utils import *\n",
    "import arguments\n",
    "\n",
    "\n",
    "def cifar_transformer():\n",
    "    return transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5,],\n",
    "                                std=[0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "\n",
    "def main(args):\n",
    "    if args.dataset == 'cifar10':\n",
    "        test_dataloader = data.DataLoader(\n",
    "                datasets.CIFAR10(args.data_path, download=True, transform=cifar_transformer(), train=False),\n",
    "            batch_size=args.batch_size, drop_last=False)\n",
    "\n",
    "        train_dataset = CIFAR10(args.data_path)\n",
    "\n",
    "        args.num_images = 50000\n",
    "        args.num_val = 5000\n",
    "        args.budget = 2500\n",
    "        args.initial_budget = 5000\n",
    "        args.num_classes = 10\n",
    "    elif args.dataset == 'cifar100':\n",
    "        test_dataloader = data.DataLoader(\n",
    "                datasets.CIFAR100(args.data_path, download=True, transform=cifar_transformer(), train=False),\n",
    "             batch_size=args.batch_size, drop_last=False)\n",
    "\n",
    "        train_dataset = CIFAR100(args.data_path)\n",
    "\n",
    "        args.num_val = 5000\n",
    "        args.num_images = 50000\n",
    "        args.budget = 2500\n",
    "        args.initial_budget = 5000\n",
    "        args.num_classes = 100\n",
    "\n",
    "    elif args.dataset == 'imagenet':\n",
    "        test_dataloader = data.DataLoader(\n",
    "                datasets.ImageFolder(args.data_path, transform=imagenet_transformer()),\n",
    "            drop_last=False, batch_size=args.batch_size)\n",
    "\n",
    "        train_dataset = ImageNet(args.data_path)\n",
    "\n",
    "        args.num_val = 128120\n",
    "        args.num_images = 1281167\n",
    "        args.budget = 64060\n",
    "        args.initial_budget = 128120\n",
    "        args.num_classes = 1000\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    all_indices = set(np.arange(args.num_images))\n",
    "    val_indices = random.sample(all_indices, args.num_val)\n",
    "    all_indices = np.setdiff1d(list(all_indices), val_indices)\n",
    "\n",
    "    initial_indices = random.sample(list(all_indices), args.initial_budget)\n",
    "    sampler = data.sampler.SubsetRandomSampler(initial_indices)\n",
    "    val_sampler = data.sampler.SubsetRandomSampler(val_indices)\n",
    "\n",
    "    # dataset with labels available\n",
    "    querry_dataloader = data.DataLoader(train_dataset, sampler=sampler, \n",
    "            batch_size=args.batch_size, drop_last=True)\n",
    "    val_dataloader = data.DataLoader(train_dataset, sampler=val_sampler,\n",
    "            batch_size=args.batch_size, drop_last=False)\n",
    "            \n",
    "    args.cuda = args.cuda and torch.cuda.is_available()\n",
    "    solver = Solver(args, test_dataloader)\n",
    "\n",
    "    splits = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "\n",
    "    current_indices = list(initial_indices)\n",
    "\n",
    "    accuracies = []\n",
    "    \n",
    "    for split in splits:\n",
    "        # need to retrain all the models on the new images\n",
    "        # re initialize and retrain the models\n",
    "        task_model = vgg.vgg16_bn(num_classes=args.num_classes)\n",
    "        vae = model.VAE(args.latent_dim)\n",
    "        discriminator = model.Discriminator(args.latent_dim)\n",
    "\n",
    "        unlabeled_indices = np.setdiff1d(list(all_indices), current_indices)\n",
    "        unlabeled_sampler = data.sampler.SubsetRandomSampler(unlabeled_indices)\n",
    "        unlabeled_dataloader = data.DataLoader(train_dataset, \n",
    "                sampler=unlabeled_sampler, batch_size=args.batch_size, drop_last=False)\n",
    "\n",
    "        # train the models on the current data\n",
    "        acc, vae, discriminator = solver.train(querry_dataloader,\n",
    "                                               val_dataloader,\n",
    "                                               task_model, \n",
    "                                               vae, \n",
    "                                               discriminator,\n",
    "                                               unlabeled_dataloader)\n",
    "\n",
    "\n",
    "        print('Final accuracy with {}% of data is: {:.2f}'.format(int(split*100), acc))\n",
    "        accuracies.append(acc)\n",
    "\n",
    "        sampled_indices = solver.sample_for_labeling(vae, discriminator, unlabeled_dataloader)\n",
    "        current_indices = list(current_indices) + list(sampled_indices)\n",
    "        sampler = data.sampler.SubsetRandomSampler(current_indices)\n",
    "        querry_dataloader = data.DataLoader(train_dataset, sampler=sampler, \n",
    "                batch_size=args.batch_size, drop_last=True)\n",
    "\n",
    "    torch.save(accuracies, os.path.join(args.out_path, args.log_name))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = arguments.get_args()\n",
    "    main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model (model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class View(nn.Module):\n",
    "    def __init__(self, size):\n",
    "        super(View, self).__init__()\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        return tensor.view(self.size)\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"Encoder-Decoder architecture for both WAE-MMD and WAE-GAN.\"\"\"\n",
    "    def __init__(self, z_dim=32, nc=3):\n",
    "        super(VAE, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = nc\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(nc, 128, 4, 2, 1, bias=False),              # B,  128, 32, 32\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),             # B,  256, 16, 16\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),             # B,  512,  8,  8\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 1024, 4, 2, 1, bias=False),            # B, 1024,  4,  4\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            View((-1, 1024*2*2)),                                 # B, 1024*4*4\n",
    "        )\n",
    "\n",
    "        self.fc_mu = nn.Linear(1024*2*2, z_dim)                            # B, z_dim\n",
    "        self.fc_logvar = nn.Linear(1024*2*2, z_dim)                            # B, z_dim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(z_dim, 1024*4*4),                           # B, 1024*8*8\n",
    "            View((-1, 1024, 4, 4)),                               # B, 1024,  8,  8\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),   # B,  512, 16, 16\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),    # B,  256, 32, 32\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),    # B,  128, 64, 64\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, nc, 1),                       # B,   nc, 64, 64\n",
    "        )\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            try:\n",
    "                for m in self._modules[block]:\n",
    "                    kaiming_init(m)\n",
    "            except:\n",
    "                kaiming_init(block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self._encode(x)\n",
    "        mu, logvar = self.fc_mu(z), self.fc_logvar(z)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_recon = self._decode(z)\n",
    "\n",
    "        return x_recon, z, mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        stds = (0.5 * logvar).exp()\n",
    "        epsilon = torch.randn(*mu.size())\n",
    "        if mu.is_cuda:\n",
    "            stds, epsilon = stds.cuda(), epsilon.cuda()\n",
    "        latents = epsilon * stds + mu\n",
    "        return latents\n",
    "\n",
    "    def _encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "    def _decode(self, z):\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"Adversary architecture(Discriminator) for WAE-GAN.\"\"\"\n",
    "    def __init__(self, z_dim=10):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.z_dim = z_dim\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(z_dim, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.weight_init()\n",
    "\n",
    "    def weight_init(self):\n",
    "        for block in self._modules:\n",
    "            for m in self._modules[block]:\n",
    "                kaiming_init(m)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "def kaiming_init(m):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        init.kaiming_normal(m.weight)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
    "        m.weight.data.fill_(1)\n",
    "        if m.bias is not None:\n",
    "            m.bias.data.fill_(0)\n",
    "\n",
    "\n",
    "def normal_init(m, mean, std):\n",
    "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "        m.weight.data.normal_(mean, std)\n",
    "        if m.bias.data is not None:\n",
    "            m.bias.data.zero_()\n",
    "    elif isinstance(m, (nn.BatchNorm2d, nn.BatchNorm1d)):\n",
    "        m.weight.data.fill_(1)\n",
    "        if m.bias.data is not None:\n",
    "            m.bias.data.zero_()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampler (sampler.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class AdversarySampler:\n",
    "    def __init__(self, budget):\n",
    "        self.budget = budget\n",
    "\n",
    "\n",
    "    def sample(self, vae, discriminator, data, cuda):\n",
    "        all_preds = []\n",
    "        all_indices = []\n",
    "\n",
    "        for images, _, indices in data:\n",
    "            if cuda:\n",
    "                images = images.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                _, _, mu, _ = vae(images)\n",
    "                preds = discriminator(mu)\n",
    "\n",
    "            preds = preds.cpu().data\n",
    "            all_preds.extend(preds)\n",
    "            all_indices.extend(indices)\n",
    "\n",
    "        all_preds = torch.stack(all_preds)\n",
    "        all_preds = all_preds.view(-1)\n",
    "        # need to multiply by -1 to be able to use torch.topk \n",
    "        all_preds *= -1\n",
    "\n",
    "        # select the points which the discriminator things are the most likely to be unlabeled\n",
    "        _, querry_indices = torch.topk(all_preds, int(self.budget))\n",
    "        querry_pool_indices = np.asarray(all_indices)[querry_indices]\n",
    "\n",
    "        return querry_pool_indices\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver (solver.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import sampler\n",
    "import copy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Solver:\n",
    "    def __init__(self, args, test_dataloader):\n",
    "        self.args = args\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.sampler = sampler.AdversarySampler(self.args.budget)\n",
    "\n",
    "\n",
    "    def read_data(self, dataloader, labels=True):\n",
    "        if labels:\n",
    "            while True:\n",
    "                for img, label, _ in dataloader:\n",
    "                    yield img, label\n",
    "        else:\n",
    "            while True:\n",
    "                for img, _, _ in dataloader:\n",
    "                    yield img\n",
    "\n",
    "\n",
    "    def train(self, querry_dataloader, val_dataloader, task_model, vae, discriminator, unlabeled_dataloader):\n",
    "        self.args.train_iterations = (self.args.num_images * self.args.train_epochs) // self.args.batch_size\n",
    "        lr_change = self.args.train_iterations // 4\n",
    "        labeled_data = self.read_data(querry_dataloader)\n",
    "        unlabeled_data = self.read_data(unlabeled_dataloader, labels=False)\n",
    "\n",
    "        optim_vae = optim.Adam(vae.parameters(), lr=5e-4)\n",
    "        optim_task_model = optim.SGD(task_model.parameters(), lr=0.01, weight_decay=5e-4, momentum=0.9)\n",
    "        optim_discriminator = optim.Adam(discriminator.parameters(), lr=5e-4)\n",
    "\n",
    "\n",
    "        vae.train()\n",
    "        discriminator.train()\n",
    "        task_model.train()\n",
    "\n",
    "        if self.args.cuda:\n",
    "            vae = vae.cuda()\n",
    "            discriminator = discriminator.cuda()\n",
    "            task_model = task_model.cuda()\n",
    "        \n",
    "        best_acc = 0\n",
    "        for iter_count in range(self.args.train_iterations):\n",
    "            if iter_count is not 0 and iter_count % lr_change == 0:\n",
    "                for param in optim_task_model.param_groups:\n",
    "                    param['lr'] = param['lr'] / 10\n",
    "            labeled_imgs, labels = next(labeled_data)\n",
    "            unlabeled_imgs = next(unlabeled_data)\n",
    "\n",
    "            if self.args.cuda:\n",
    "                labeled_imgs = labeled_imgs.cuda()\n",
    "                unlabeled_imgs = unlabeled_imgs.cuda()\n",
    "                labels = labels.cuda()\n",
    "\n",
    "            # task_model step\n",
    "            preds = task_model(labeled_imgs)\n",
    "            task_loss = self.ce_loss(preds, labels)\n",
    "            optim_task_model.zero_grad()\n",
    "            task_loss.backward()\n",
    "            optim_task_model.step()\n",
    "\n",
    "            # VAE step\n",
    "            for count in range(self.args.num_vae_steps):\n",
    "                recon, z, mu, logvar = vae(labeled_imgs)\n",
    "                unsup_loss = self.vae_loss(labeled_imgs, recon, mu, logvar, self.args.beta)\n",
    "                unlab_recon, unlab_z, unlab_mu, unlab_logvar = vae(unlabeled_imgs)\n",
    "                transductive_loss = self.vae_loss(unlabeled_imgs, \n",
    "                        unlab_recon, unlab_mu, unlab_logvar, self.args.beta)\n",
    "            \n",
    "                labeled_preds = discriminator(mu)\n",
    "                unlabeled_preds = discriminator(unlab_mu)\n",
    "                \n",
    "                lab_real_preds = torch.ones(labeled_imgs.size(0))\n",
    "                unlab_real_preds = torch.ones(unlabeled_imgs.size(0))\n",
    "                    \n",
    "                if self.args.cuda:\n",
    "                    lab_real_preds = lab_real_preds.cuda()\n",
    "                    unlab_real_preds = unlab_real_preds.cuda()\n",
    "\n",
    "                dsc_loss = self.bce_loss(labeled_preds, lab_real_preds) + \\\n",
    "                        self.bce_loss(unlabeled_preds, unlab_real_preds)\n",
    "                total_vae_loss = unsup_loss + transductive_loss + self.args.adversary_param * dsc_loss\n",
    "                optim_vae.zero_grad()\n",
    "                total_vae_loss.backward()\n",
    "                optim_vae.step()\n",
    "\n",
    "                # sample new batch if needed to train the adversarial network\n",
    "                if count < (self.args.num_vae_steps - 1):\n",
    "                    labeled_imgs, _ = next(labeled_data)\n",
    "                    unlabeled_imgs = next(unlabeled_data)\n",
    "\n",
    "                    if self.args.cuda:\n",
    "                        labeled_imgs = labeled_imgs.cuda()\n",
    "                        unlabeled_imgs = unlabeled_imgs.cuda()\n",
    "                        labels = labels.cuda()\n",
    "\n",
    "            # Discriminator step\n",
    "            for count in range(self.args.num_adv_steps):\n",
    "                with torch.no_grad():\n",
    "                    _, _, mu, _ = vae(labeled_imgs)\n",
    "                    _, _, unlab_mu, _ = vae(unlabeled_imgs)\n",
    "                \n",
    "                labeled_preds = discriminator(mu)\n",
    "                unlabeled_preds = discriminator(unlab_mu)\n",
    "                \n",
    "                lab_real_preds = torch.ones(labeled_imgs.size(0))\n",
    "                unlab_fake_preds = torch.zeros(unlabeled_imgs.size(0))\n",
    "\n",
    "                if self.args.cuda:\n",
    "                    lab_real_preds = lab_real_preds.cuda()\n",
    "                    unlab_fake_preds = unlab_fake_preds.cuda()\n",
    "                \n",
    "                dsc_loss = self.bce_loss(labeled_preds, lab_real_preds) + \\\n",
    "                        self.bce_loss(unlabeled_preds, unlab_fake_preds)\n",
    "\n",
    "                optim_discriminator.zero_grad()\n",
    "                dsc_loss.backward()\n",
    "                optim_discriminator.step()\n",
    "\n",
    "                # sample new batch if needed to train the adversarial network\n",
    "                if count < (self.args.num_adv_steps - 1):\n",
    "                    labeled_imgs, _ = next(labeled_data)\n",
    "                    unlabeled_imgs = next(unlabeled_data)\n",
    "\n",
    "                    if self.args.cuda:\n",
    "                        labeled_imgs = labeled_imgs.cuda()\n",
    "                        unlabeled_imgs = unlabeled_imgs.cuda()\n",
    "                        labels = labels.cuda()\n",
    "\n",
    "                \n",
    "\n",
    "            if iter_count % 100 == 0:\n",
    "                print('Current training iteration: {}'.format(iter_count))\n",
    "                print('Current task model loss: {:.4f}'.format(task_loss.item()))\n",
    "                print('Current vae model loss: {:.4f}'.format(total_vae_loss.item()))\n",
    "                print('Current discriminator model loss: {:.4f}'.format(dsc_loss.item()))\n",
    "\n",
    "            if iter_count % 1000 == 0:\n",
    "                acc = self.validate(task_model, val_dataloader)\n",
    "                if acc > best_acc:\n",
    "                    best_acc = acc\n",
    "                    best_model = copy.deepcopy(task_model)\n",
    "                \n",
    "                print('current step: {} acc: {}'.format(iter_count, acc))\n",
    "                print('best acc: ', best_acc)\n",
    "\n",
    "\n",
    "        if self.args.cuda:\n",
    "            best_model = best_model.cuda()\n",
    "\n",
    "        final_accuracy = self.test(best_model)\n",
    "        return final_accuracy, vae, discriminator\n",
    "\n",
    "\n",
    "    def sample_for_labeling(self, vae, discriminator, unlabeled_dataloader):\n",
    "        querry_indices = self.sampler.sample(vae, \n",
    "                                             discriminator, \n",
    "                                             unlabeled_dataloader, \n",
    "                                             self.args.cuda)\n",
    "\n",
    "        return querry_indices\n",
    "                \n",
    "\n",
    "    def validate(self, task_model, loader):\n",
    "        task_model.eval()\n",
    "        total, correct = 0, 0\n",
    "        for imgs, labels, _ in loader:\n",
    "            if self.args.cuda:\n",
    "                imgs = imgs.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds = task_model(imgs)\n",
    "\n",
    "            preds = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "            correct += accuracy_score(labels, preds, normalize=False)\n",
    "            total += imgs.size(0)\n",
    "        return correct / total * 100\n",
    "\n",
    "    def test(self, task_model):\n",
    "        task_model.eval()\n",
    "        total, correct = 0, 0\n",
    "        for imgs, labels in self.test_dataloader:\n",
    "            if self.args.cuda:\n",
    "                imgs = imgs.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                preds = task_model(imgs)\n",
    "\n",
    "            preds = torch.argmax(preds, dim=1).cpu().numpy()\n",
    "            correct += accuracy_score(labels, preds, normalize=False)\n",
    "            total += imgs.size(0)\n",
    "        return correct / total * 100\n",
    "\n",
    "\n",
    "    def vae_loss(self, x, recon, mu, logvar, beta):\n",
    "        MSE = self.mse_loss(recon, x)\n",
    "        KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        KLD = KLD * beta\n",
    "        return MSE + KLD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility (utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "def imagenet_transformer():\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def cifar_transformer():\n",
    "    return torchvision.transforms.Compose([\n",
    "           torchvision.transforms.RandomHorizontalFlip(),\n",
    "           torchvision.transforms.ToTensor(),\n",
    "       ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG (vgg.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "__all__ = [\n",
    "    'VGG', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn',\n",
    "    'vgg19_bn', 'vgg19',\n",
    "]\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    'vgg11': 'https://download.pytorch.org/models/vgg11-bbd30ac9.pth',\n",
    "    'vgg13': 'https://download.pytorch.org/models/vgg13-c768596a.pth',\n",
    "    'vgg16': 'https://download.pytorch.org/models/vgg16-397923af.pth',\n",
    "    'vgg19': 'https://download.pytorch.org/models/vgg19-dcbb9e9d.pth',\n",
    "    'vgg11_bn': 'https://download.pytorch.org/models/vgg11_bn-6002323d.pth',\n",
    "    'vgg13_bn': 'https://download.pytorch.org/models/vgg13_bn-abd245e5.pth',\n",
    "    'vgg16_bn': 'https://download.pytorch.org/models/vgg16_bn-6c64b313.pth',\n",
    "    'vgg19_bn': 'https://download.pytorch.org/models/vgg19_bn-c79401a0.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class VGG(nn.Module):\n",
    "\n",
    "    def __init__(self, features, num_classes=1000, init_weights=True):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = features\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 7 * 7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "def make_layers(cfg, batch_norm=False):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "cfgs = {\n",
    "    'A': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'B': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'D': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'E': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "\n",
    "def _vgg(arch, cfg, batch_norm, pretrained, progress, **kwargs):\n",
    "    if pretrained:\n",
    "        kwargs['init_weights'] = False\n",
    "    model = VGG(make_layers(cfgs[cfg], batch_norm=batch_norm), **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def vgg11(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 11-layer model (configuration \"A\") from\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>'_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg11', 'A', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg11_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 11-layer model (configuration \"A\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>'_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg11_bn', 'A', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 13-layer model (configuration \"B\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>'_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg13', 'B', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg13_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 13-layer model (configuration \"B\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>'_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg13_bn', 'B', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 16-layer model (configuration \"D\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>'_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg16', 'D', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg16_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 16-layer model (configuration \"D\") with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>'_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg16_bn', 'D', True, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 19-layer model (configuration \"E\")\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>'_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg19', 'E', False, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "def vgg19_bn(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"VGG 19-layer model (configuration 'E') with batch normalization\n",
    "    `\"Very Deep Convolutional Networks For Large-Scale Image Recognition\" <https://arxiv.org/pdf/1409.1556.pdf>'_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _vgg('vgg19_bn', 'E', True, pretrained, progress, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
