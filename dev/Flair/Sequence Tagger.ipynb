{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of the art NER using Flair Sequence Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import groupby\n",
    "import itertools\n",
    "import random\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "1. Join train, dev, test into single dataset for k-fold cross-validation\n",
    "2. Create splits with given sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each partition of data with `k` samples, we split the dataset into three sets with the following distribution: `80/10/10` for the train, dev, and test set, respectively. Therefore, for each `k` there will be varying sizes of all three sets of data, however this makes more intuitive sense than only partitioning the test set and keeping the dev and test sets the same size as if they were at `k_n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.getcwd() + '/resources/tasks/conll_03/full data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name: Number of documents\n",
    "# Includes all the docstarts... these are removed by Flair\n",
    "splits = {'train': {'len': 14987, 'samples': []}, 'dev': {'len': 3466, 'samples': []}, 'test': {'len': 3684, 'samples': []}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for split_name in splits:\n",
    "    print(f'Processing: {split_name}')\n",
    "    \n",
    "    with open(data_path + split_name + '.txt', 'r') as fr:\n",
    "        lines = fr.readlines()\n",
    "    \n",
    "    # Groups documents based on position of new line separators (lines with no tokens)\n",
    "    docs = [list(group) for k, group in groupby(lines, lambda x: len(x) == 1) if not k]\n",
    "    \n",
    "    print(f'Dataset size (docs): {len(docs)}')\n",
    "    \n",
    "    all_data.append(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = 0.01\n",
    "dev_split = 1#train_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Currently using oversized dev/test sets relative to small sample sizes at the beginning of training splits (e.g. 3k for test/dev and %*train_len)\n",
    "all_data = np.array(list(itertools.chain(*all_data)))\n",
    "dataset_size = len(all_data)\n",
    "all_indices = set(np.arange(dataset_size))\n",
    "\n",
    "for split_name in splits:\n",
    "    if split_name == 'test':\n",
    "        # Use entire 3k of test samples\n",
    "        sampled_indices = np.array(random.sample(list(all_indices), k=splits[split_name]['len']))\n",
    "        \n",
    "    if split_name == 'dev':\n",
    "        # Split dev set with the same proportion of it's original dataset ~24% of the training split\n",
    "        sampled_indices = np.array(random.sample(list(all_indices), k=math.ceil(splits[split_name]['len']*dev_split)))\n",
    "        \n",
    "    if split_name == 'train':\n",
    "        sampled_indices = np.array(random.sample(list(all_indices), k=math.ceil(splits[split_name]['len']*train_split)))\n",
    "    splits[split_name]['samples'] = sampled_indices\n",
    "    # Remove sampled indices from dataset\n",
    "    all_indices = np.setdiff1d(list(all_indices), sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Size of datasets: Train {len(splits[\"train\"][\"samples\"])} Dev {len(splits[\"dev\"][\"samples\"])} Test {len(splits[\"test\"][\"samples\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train Sample\\n{all_data[splits[\"train\"][\"samples\"][0]]}\\nDev Sample\\n{all_data[splits[\"dev\"][\"samples\"][0]]}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# check if sharing data\n",
    "print(set(dev_indices).intersection(set(test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save splits back into directory for Corpus reader to find\n",
    "for split_name in splits:\n",
    "    print(split_name)\n",
    "    split_data_str = \"\\n\".join([\"\".join(doc) for doc in all_data[splits[split_name]['samples']]])\n",
    "    with open(os.getcwd() + '/resources/tasks/conll_03/' + split_name + '.txt', 'w') as fw:\n",
    "        fw.write(split_data_str)\n",
    "        fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flair, torch\n",
    "flair.device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import CONLL_03\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, PooledFlairEmbeddings\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get the corpus\n",
    "corpus: Corpus = CONLL_03(base_path='resources/tasks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "\n",
    "    # GloVe embeddings\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # contextual string embeddings, forward\n",
    "    PooledFlairEmbeddings('news-forward', pooling='min'),\n",
    "\n",
    "    # contextual string embeddings, backward\n",
    "    PooledFlairEmbeddings('news-backward', pooling='min'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize sequence tagger\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize trainer\n",
    "from flair.trainers import ModelTrainer\n",
    "from datetime import datetime\n",
    "\n",
    "trainer: ModelTrainer = ModelTrainer(tagger, corpus)\n",
    "\n",
    "train_w_dev = False\n",
    "\n",
    "\n",
    "#        return {\n",
    "#             \"test_score\": final_score,\n",
    "#             \"dev_score_history\": dev_score_history,\n",
    "#             \"train_loss_history\": train_loss_history,\n",
    "#             \"dev_loss_history\": dev_loss_history,\n",
    "#         }\n",
    "save_path = f'resources/taggers/example-ner-{train_split*100:0.0f} ' + 'twd0' if train_w_dev else 'twd1'\n",
    "trainer.train(save_path,\n",
    "              train_with_dev=train_w_dev,  \n",
    "              max_epochs=50)   # 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
